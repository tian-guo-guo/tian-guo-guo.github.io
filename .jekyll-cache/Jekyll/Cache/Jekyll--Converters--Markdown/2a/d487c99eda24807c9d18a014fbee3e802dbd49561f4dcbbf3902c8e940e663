I"¥<h2 id="transfer-learning-for-low-resource-neural-machine-translation-barret">Transfer Learning for Low-Resource Neural Machine Translation Barret</h2>

<h3 id="abstract">Abstract</h3>
<p>The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that signifi- cantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an av- erage of 5.6 BLEU on four low-resource lan- guage pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource ma- chine translation close to a strong syntax based machine translation (SBMT) system, exceed- ing its performance on one language pair. Ad- ditionally, using the transfer learning model for re-scoring, we can improve the SBMT sys- tem by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.</p>

<p>ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘(NMT)çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å·²è¢«è¯æ˜åœ¨å¤§æ•°æ®åœºæ™¯ä¸­æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å¯¹äºä½èµ„æºè¯­è¨€çš„æ•ˆæœè¦å·®å¾—å¤šã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå®ƒèƒ½æ˜¾è‘—æé«˜ä½èµ„æºè¯­è¨€çš„BLEUåˆ†æ•°ã€‚ æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯é¦–å…ˆè®­ç»ƒä¸€ä¸ªé«˜èµ„æºè¯­è¨€å¯¹ï¼ˆçˆ¶æ¨¡å‹ï¼‰ï¼Œç„¶åå°†ä¸€äº›å­¦ä¹ åˆ°çš„å‚æ•°ä¼ é€’ç»™ä½èµ„æºè¯­è¨€å¯¹ï¼ˆå­æ¨¡å‹ï¼‰æ¥åˆå§‹åŒ–å’Œçº¦æŸè®­ç»ƒã€‚ åˆ©ç”¨æˆ‘ä»¬çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬å¯¹å››ä¸ªä½èµ„æºè¯­è¨€å¯¹çš„åŸºçº¿NMTæ¨¡å‹è¿›è¡Œäº†5.6BLEUçš„æ”¹è¿›ã€‚ é›†æˆå’ŒæœªçŸ¥è¯æ›¿æ¢å¢åŠ äº†2ä¸ªBLEUï¼Œä½¿å¾—ä½èµ„æºæœºå™¨ç¿»è¯‘çš„NMTæ€§èƒ½æ¥è¿‘äºä¸€ä¸ªå¼ºçš„åŸºäºè¯­æ³•çš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿ(SBMT)ï¼Œè¶…è¿‡äº†å®ƒåœ¨ä¸€ä¸ªè¯­è¨€å¯¹ä¸Šçš„æ€§èƒ½ã€‚ å¦å¤–ï¼Œåˆ©ç”¨è¿ç§»å­¦ä¹ æ¨¡å‹å¯¹SBMTç³»ç»Ÿè¿›è¡Œé‡æ–°è¯„åˆ†ï¼Œå¹³å‡æé«˜äº†1.3bleuï¼Œæ”¹å–„äº†ä½èµ„æºæœºå™¨ç¿»è¯‘çš„ç°çŠ¶ã€‚</p>

<h3 id="1-introduction">1 Introduction</h3>
<p>Neural machine translation (NMT) (Sutskever et al., 2014) is a promising paradigm for extracting translation knowledge from parallel text. NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs such as Englishâ€“French. However, neural methods are data-hungry and learn poorly from low-count events. This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce. Table 1 shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) (Galley et al., 2004; Galley et al., 2006) strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feed- input techniques from Luong et al. (2015a).</p>

<p>ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰ï¼ˆSutskever et al.ï¼Œ2014ï¼‰æ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„ä»å¹³è¡Œæ–‡æœ¬ä¸­æå–ç¿»è¯‘çŸ¥è¯†çš„èŒƒå¼ã€‚ NMTç³»ç»Ÿåœ¨è‹±æ³•ç­‰è¯­è¨€å¯¹çš„å¤§æ•°æ®è®­ç»ƒæ¡ä»¶ä¸‹å–å¾—äº†æœ‰ç«äº‰åŠ›çš„å‡†ç¡®ç‡ã€‚ ç„¶è€Œï¼Œç¥ç»æ–¹æ³•å¯¹æ•°æ®çš„éœ€æ±‚å¾ˆå¤§ï¼Œä»ä½è®¡æ•°äº‹ä»¶ä¸­å­¦ä¹ çš„èƒ½åŠ›ä¹Ÿå¾ˆå·®ã€‚ è¿™ç§è¡Œä¸ºä½¿å¾—Vanilla NMTåœ¨å¹¶è¡Œæ•°æ®ç¨€ç¼ºçš„ä½èµ„æºè¯­è¨€ä¸­æ˜¯ä¸€ä¸ªç³Ÿç³•çš„é€‰æ‹©ã€‚ è¡¨1æ˜¾ç¤ºï¼Œå¯¹äº4ç§ä½èµ„æºè¯­è¨€ï¼Œæ ‡å‡†çš„String-to-Treeç»Ÿè®¡MTç³»ç»Ÿ(SBMT)ï¼ˆGalleyç­‰äººï¼Œ2004ï¼›Galleyç­‰äººï¼Œ2006ï¼‰çš„æ€§èƒ½å¼ºäºNMTï¼Œå³ä½¿NMTä½¿ç”¨Luongç­‰äººçš„æœ€æ–°çš„æœ¬åœ°æ³¨æ„åŠ æè¦è¾“å…¥æŠ€æœ¯ã€‚ (2015a)ã€‚
<img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table1.png" alt="190910-Table1.png" /></p>

<p>In this paper, we describe a method for substantially improving NMT results on these languages. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model. We re- port NMT improvements from transfer learning of 5.6 BLEU on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in all four language pairs, and exceeds SBMT performance in one of them. Furthermore, we show that NMT is an exceptional re-scorer of â€˜traditionalâ€™ MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a re-scoring model.</p>

<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§åœ¨è¿™äº›è¯­è¨€ä¸Šå®è´¨ä¸Šæ”¹è¿›NMTç»“æœçš„æ–¹æ³•ã€‚ æˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯é¦–å…ˆè®­ç»ƒä¸€ä¸ªé«˜èµ„æºè¯­è¨€å¯¹ï¼Œç„¶åä½¿ç”¨å¾—åˆ°çš„è®­ç»ƒç½‘ç»œï¼ˆçˆ¶æ¨¡å‹ï¼‰åˆå§‹åŒ–å’Œçº¦æŸæˆ‘ä»¬çš„ä½èµ„æºè¯­è¨€å¯¹ï¼ˆå­æ¨¡å‹ï¼‰çš„è®­ç»ƒã€‚ æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å›ºå®šçˆ¶æ¨¡å‹çš„æŸäº›å‚æ•°å¹¶è®©å…¶é¦€çš„å‚æ•°ç”±å­æ¨¡å‹å¾®è°ƒæ¥ä¼˜åŒ–æˆ‘ä»¬çš„ç»“æœã€‚ æˆ‘ä»¬ä»å¹³å‡5.6BLEUçš„è¿ç§»å­¦ä¹ ä¸­æŠ¥å‘Šäº†NMTçš„æ”¹è¿›ï¼Œå¹¶åˆ†æäº†è¯¥æ–¹æ³•çš„å·¥ä½œåŸç†ã€‚ æœ€ç»ˆçš„NMTç³»ç»Ÿåœ¨æ‰€æœ‰å››ç§è¯­è¨€å¯¹ä¸­éƒ½æ¥è¿‘äºå¼ºå¤§çš„SBMTåŸºçº¿ï¼Œå…¶ä¸­ä¸€ç§è¯­è¨€å¯¹çš„æ€§èƒ½è¶…è¿‡äº†SBMTã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†NMTæ˜¯â€œä¼ ç»Ÿâ€MTè¾“å‡ºçš„ä¸€ä¸ªç‰¹æ®Šçš„é‡å¾—åˆ†è€…ï¼› å³ä½¿NMTæœ¬èº«æ¯”SBMTå·®ï¼Œä½œä¸ºä¸€ä¸ªé‡æ–°è¯„åˆ†æ¨¡å‹ï¼Œå®ƒä¹Ÿèƒ½å¤ŸæŒç»­æ”¹è¿›SBMTç³»ç»Ÿçš„è¾“å‡ºã€‚</p>

<p>We provide a brief description of our NMT model in Section 2. Section 3 gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek, and Urdu into English with the help of a Frenchâ€“English parent model are presented in Section 4. Section 5 explores alternatives to our model to enhance under- standing. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and that using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine- tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models, showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section 6.</p>

<p>æˆ‘ä»¬åœ¨ç¬¬2èŠ‚ä¸­ç®€è¦æè¿°äº†æˆ‘ä»¬çš„NMTæ¨¡å‹ã€‚ ç¬¬ä¸‰èŠ‚ä»‹ç»äº†è¿ç§»å­¦ä¹ çš„èƒŒæ™¯çŸ¥è¯†ï¼Œå¹¶è¯´æ˜äº†å¦‚ä½•åˆ©ç”¨è¿ç§»å­¦ä¹ æ¥æé«˜æœºå™¨ç¿»è¯‘çš„æ€§èƒ½ã€‚ ç¬¬å››èŠ‚ä»‹ç»äº†æˆ‘ä»¬åœ¨æ³•è‹±çˆ¶æ¯æ¨¡å¼çš„å¸®åŠ©ä¸‹ï¼Œå°†è±ªè¨è¯­ã€åœŸè€³å…¶è¯­ã€ä¹Œå…¹åˆ«å…‹è¯­å’Œä¹Œå°”éƒ½è¯­ç¿»è¯‘æˆè‹±è¯­çš„ä¸»è¦å®éªŒã€‚ ç¬¬5èŠ‚æ¢è®¨äº†æˆ‘ä»¬çš„æ¨¡å¼çš„æ›¿ä»£åŠæ³•ï¼Œä»¥æé«˜äººä»¬çš„è®¤è¯†ã€‚ æˆ‘ä»¬å‘ç°ï¼Œçˆ¶æ¯è¯­è¨€å¯¹çš„é€‰æ‹©ä¼šå½±å“å­¦ä¹ æˆç»©ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªä½¿ç”¨äººå·¥è¯­è¨€çš„è¿ç§»æˆç»©çš„ç»éªŒä¸Šç•Œã€‚ æˆ‘ä»¬ç”¨çº¯è‹±è¯­è¯­è¨€æ¨¡å‹ã€å¤åˆ¶æ¨¡å‹å’Œå•è¯æ’åºæ¨¡å‹è¿›è¡Œäº†å®éªŒï¼Œä»¥è¡¨æ˜æˆ‘ä»¬ä¼ é€’çš„ä¿¡æ¯ä¸ä»…ä»…æ˜¯å•ä¸€è¯­è¨€ä¿¡æ¯ï¼Œä½¿ç”¨ä¸€ä¸ªç»è¿‡åŒè¯­è¯­æ–™åº“è®­ç»ƒçš„ç¿»è¯‘æ¨¡å‹ä½œä¸ºçˆ¶è¯­è¨€æ˜¯éå¸¸å¿…è¦çš„ã€‚ æˆ‘ä»¬å±•ç¤ºäº†åœ¨è½¬ç§»æœŸé—´å†»ç»“ã€å¾®è°ƒå’Œæ›´æ™ºèƒ½åœ°åˆå§‹åŒ–åŸºäºæ³¨æ„çš„NMTç³»ç»Ÿçš„ä¸åŒç»„ä»¶çš„æ•ˆæœã€‚ æˆ‘ä»¬æ¯”è¾ƒäº†è½¬ç§»æ¨¡å‹å’Œæ— è½¬ç§»æ¨¡å‹çš„å­¦ä¹ æ›²çº¿ï¼Œç»“æœè¡¨æ˜è½¬ç§»æ¨¡å‹è§£å†³çš„æ˜¯ä¸€ä¸ªè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæœç´¢é—®é¢˜ã€‚ æˆ‘ä»¬åœ¨ç¬¬6èŠ‚ä¸­æ€»ç»“äº†æˆ‘ä»¬çš„è´¡çŒ®ã€‚</p>

<h3 id="2-nmt-background">2 NMT Background</h3>
<p>In the neural encoder-decoder framework for MT (Neco and Forcada, 1997; CastaËœno and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a), we use a recurrent neural network (encoder) to convert a source sen- tence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector to a target sentence. In this paper, we use a two-layer encoder-decoder system (Figure 1) with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). The models were trained to optimize maximum likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). Additionally, we use an attention mechanism that allows the target decoder to look back at the source encoder, specifically the local attention model from Luong et al. (2015a). In our model we also use the feed-input input connection from Luong et al. (2015a) where at each timestep on the decoder we feed in the top layerâ€™s hidden state into the lowest layer of the next timestep.</p>

<p>åœ¨ç”¨äºMTçš„ç¥ç»ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ˆNecoå’ŒForcadaï¼Œ1997ï¼›Casta~NOå’ŒCasacubertaï¼Œ1997ï¼›Sutskeverç­‰äººï¼Œ2014ï¼›Bahdanauç­‰äººï¼Œ2014ï¼›Luongç­‰äººï¼Œ2015a)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨é€’å½’ç¥ç»ç½‘ç»œï¼ˆç¼–ç å™¨ï¼‰å°†æºæ„Ÿæµ‹è½¬æ¢ä¸ºå¯†é›†çš„å›ºå®šé•¿åº¦å‘é‡ã€‚ ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å¦ä¸€ä¸ªé€’å½’ç½‘ç»œï¼ˆè§£ç å™¨ï¼‰å°†è¯¥å‘é‡è½¬æ¢ä¸ºç›®æ ‡è¯­å¥ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå…·æœ‰é•¿çŸ­æ—¶è®°å¿†(LSTM)å•å…ƒçš„ä¸¤å±‚ç¼–ç å™¨-è§£ç å™¨ç³»ç»Ÿï¼ˆå›¾1ï¼‰ï¼ˆHochreiterå’ŒSchmidhuberï¼Œ1997ï¼‰ã€‚ è¿™äº›æ¨¡å‹è¢«è®­ç»ƒæˆé€šè¿‡æ—¶é—´åå‘ä¼ æ’­ä¼˜åŒ–æœ€å¤§ä¼¼ç„¶ï¼ˆé€šè¿‡SoftMaxå±‚ï¼‰ï¼ˆWerbosï¼Œ1990ï¼‰ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§å…³æ³¨æœºåˆ¶ï¼Œå…è®¸ç›®æ ‡è§£ç å™¨å›é¡¾æºç¼–ç å™¨ï¼Œç‰¹åˆ«æ˜¯Luongç­‰äººçš„å±€éƒ¨å…³æ³¨æ¨¡å‹ã€‚ (2015a)ã€‚ åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†Luongç­‰äººçš„feed-inputè¾“å…¥è¿æ¥ã€‚ (2015a)å…¶ä¸­åœ¨è§£ç å™¨ä¸Šçš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬å°†é¡¶å±‚çš„éšè—çŠ¶æ€é¦ˆé€åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ä½å±‚ã€‚</p>

<h3 id="3-transfer-learning">3 Transfer Learning</h3>
<p>Transfer learning uses knowledge from a learned task to improve the performance on a related task, typically reducing the amount of required training data (Torrey and Shavlik, 2009; Pan and Yang, 2010). In natural language processing, transfer learning methods have been successfully applied to speech recognition, document classification and sentiment analysis (Wang and Zheng, 2015). Deep learning models discover multiple levels of representation, some of which may be useful across tasks, which makes them particularly suited to transfer learning (Bengio, 2012). For example, CiresÂ¸an et al. (2012) use a convolutional neural network to recognize handwritten characters and show positive effects of transfer between models for Latin and Chinese characters. Ours is the first study to apply transfer learning to neural machine translation.</p>

<p>è½¬ç§»å­¦ä¹ åˆ©ç”¨ä»å­¦ä¹ ä»»åŠ¡ä¸­è·å¾—çš„çŸ¥è¯†æ¥æé«˜ç›¸å…³ä»»åŠ¡çš„ç»©æ•ˆï¼Œé€šå¸¸å‡å°‘æ‰€éœ€çš„åŸ¹è®­æ•°æ®é‡ï¼ˆTorreyå’ŒShavlikï¼Œ2009å¹´ï¼›Panå’ŒYangï¼Œ2010å¹´ï¼‰ã€‚ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œè¿ç§»å­¦ä¹ æ–¹æ³•å·²ç»æˆåŠŸåœ°åº”ç”¨äºè¯­éŸ³è¯†åˆ«ã€æ–‡æ¡£åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æï¼ˆWangå’ŒZhengï¼Œ2015ï¼‰ã€‚ æ·±åº¦å­¦ä¹ æ¨¡å‹å‘ç°äº†å¤šä¸ªè¡¨å¾å±‚æ¬¡ï¼Œå…¶ä¸­ä¸€äº›å¯èƒ½åœ¨ä»»åŠ¡ä¹‹é—´æœ‰ç”¨ï¼Œè¿™ä½¿å¾—å®ƒä»¬ç‰¹åˆ«é€‚åˆäºè¿ç§»å­¦ä¹ ï¼ˆBengioï¼Œ2012ï¼‰ã€‚ ä¾‹å¦‚ï¼ŒCiresç­‰äººã€‚ ï¼ˆ2012ï¼‰ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¯†åˆ«æ‰‹å†™ä½“å­—ç¬¦ï¼Œå¹¶æ˜¾ç¤ºæ‹‰ä¸æ–‡å’Œä¸­æ–‡å­—ç¬¦æ¨¡å‹ä¹‹é—´è½¬ç§»çš„ç§¯ææ•ˆæœã€‚ æˆ‘ä»¬çš„ç ”ç©¶æ˜¯ç¬¬ä¸€ä¸ªå°†è¿ç§»å­¦ä¹ åº”ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘çš„ç ”ç©¶ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-figure1.png" alt="190910-fugure1.png" /></p>

<p>There has also been work on using data from multiple language pairs in NMT to improve performance. Recently, Dong et al. (2015) showed that sharing a source encoder for one language helps performance when using different target decoders for different languages. In that paper the authors showed that using this framework improves performance for low-resource languages by incorporating a mix of low-resource and high-resource languages. Firat et al. (2016) used a similar approach, employ- ing a separate encoder for each source language, a separate decoder for each target language, and a shared attention mechanism across all languages. They then trained these components jointly across multiple different language pairs to show improve- ments in a lower-resource setting.</p>

<p>åœ¨NMTä¸­ï¼Œè¿˜ç ”ç©¶äº†å¦‚ä½•ä½¿ç”¨æ¥è‡ªå¤šä¸ªè¯­è¨€å¯¹çš„æ•°æ®æ¥æé«˜æ€§èƒ½ã€‚ æœ€è¿‘ï¼ŒDongç­‰äººã€‚ ï¼ˆ2015ï¼‰è¡¨æ˜ï¼Œå½“å¯¹ä¸åŒè¯­è¨€ä½¿ç”¨ä¸åŒçš„ç›®æ ‡è§£ç å™¨æ—¶ï¼Œå…±äº«ä¸€ç§è¯­è¨€çš„æºç¼–ç å™¨æœ‰åŠ©äºæé«˜æ€§èƒ½ã€‚ åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä½œè€…å±•ç¤ºäº†ä½¿ç”¨è¯¥æ¡†æ¶é€šè¿‡åˆå¹¶ä½èµ„æºå’Œé«˜èµ„æºè¯­è¨€çš„æ··åˆæ¥æé«˜ä½èµ„æºè¯­è¨€çš„æ€§èƒ½ã€‚ Firatç­‰äºº ï¼ˆ2016ï¼‰ä½¿ç”¨äº†ç±»ä¼¼çš„æ–¹æ³•ï¼Œä¸ºæ¯ç§æºè¯­è¨€ä½¿ç”¨å•ç‹¬çš„ç¼–ç å™¨ï¼Œä¸ºæ¯ç§ç›®æ ‡è¯­è¨€ä½¿ç”¨å•ç‹¬çš„è§£ç å™¨ï¼Œä»¥åŠè·¨æ‰€æœ‰è¯­è¨€çš„å…±äº«æ³¨æ„æœºåˆ¶ã€‚ ç„¶åï¼Œä»–ä»¬åœ¨å¤šä¸ªä¸åŒçš„è¯­è¨€å¯¹ä¸­è”åˆåŸ¹è®­è¿™äº›ç»„ä»¶ï¼Œä»¥åœ¨èµ„æºè¾ƒå°‘çš„ç¯å¢ƒä¸­æ˜¾ç¤ºæ”¹è¿›ã€‚</p>

<p>There are a few key differences between our work and theirs. One is that we are working with truly small amounts of training data. Dong et al. (2015) used a training corpus of about 8m English words for the low-resource experiments, and Firat et al. (2016) used from 2m to 4m words, while we have at most 1.8m words, and as few as 0.2m. Additionally, the aforementioned previous work used the same domain for both low-resource and high-resource languages, while in our case the datasets come from vastly different domains, which makes the task much harder and more realistic. Our approach only requires using one additional high-resource language, while the other papers used many. Our approach also allows for easy training of new low- resource languages, while Dong et al. (2015) and Fi- rat et al. (2016) do not specify how a new language should be added to their pipeline once the models are trained. Finally, Dong et al. (2015) observe an aver- age BLEU gain on their low-resource experiments of +1.16, and Firat et al. (2016) obtain BLEU gains of +1.8, while we see a +5.6 BLEU gain.</p>

<p>æˆ‘ä»¬çš„å·¥ä½œå’Œä»–ä»¬çš„å·¥ä½œæœ‰å‡ ä¸ªä¸»è¦çš„ä¸åŒä¹‹å¤„ã€‚ å…¶ä¸€ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†çœŸæ­£å°‘é‡çš„è®­ç»ƒæ•°æ®ã€‚ Dongç­‰äººã€‚ ï¼ˆ2015ï¼‰ä½¿ç”¨äº†å¤§çº¦800ä¸‡ä¸ªè‹±è¯­å•è¯çš„è®­ç»ƒè¯­æ–™åº“è¿›è¡Œä½èµ„æºå®éªŒï¼Œä»¥åŠFiratç­‰äººã€‚ ï¼ˆ2016å¹´ï¼‰ä½¿ç”¨äº†200ä¸‡è‡³400ä¸‡å­—ï¼Œè€Œæˆ‘ä»¬æœ€å¤šåªæœ‰180ä¸‡å­—ï¼Œæœ€å°‘åªæœ‰20ä¸‡å­—ã€‚ æ­¤å¤–ï¼Œå‰é¢æåˆ°çš„å·¥ä½œå¯¹ä½èµ„æºå’Œé«˜èµ„æºè¯­è¨€ä½¿ç”¨ç›¸åŒçš„åŸŸï¼Œè€Œåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæ•°æ®é›†æ¥è‡ªæˆªç„¶ä¸åŒçš„åŸŸï¼Œè¿™ä½¿å¾—ä»»åŠ¡å˜å¾—æ›´åŠ å›°éš¾å’Œç°å®ã€‚ æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä½¿ç”¨ä¸€ç§é¢å¤–çš„é«˜èµ„æºè¯­è¨€ï¼Œè€Œå…¶ä»–çš„æ–‡ä»¶ä½¿ç”¨äº†å¾ˆå¤šã€‚ æˆ‘ä»¬çš„æ–¹æ³•è¿˜å…è®¸å®¹æ˜“åœ°è®­ç»ƒæ–°çš„ä½èµ„æºè¯­è¨€ï¼Œè€ŒDongç­‰äººã€‚ ï¼ˆ2015å¹´ï¼‰å’ŒFi-Ratç­‰äººã€‚ ï¼ˆ2016ï¼‰ä¸å…·ä½“è¯´æ˜ä¸€æ—¦åŸ¹è®­äº†æ¨¡å‹ï¼Œåº”å¦‚ä½•å°†æ–°è¯­è¨€æ·»åŠ åˆ°ä»–ä»¬çš„ç®¡é“ä¸­ã€‚ æœ€åï¼ŒDongç­‰äººã€‚ ï¼ˆ2015å¹´ï¼‰åœ¨ä½èµ„æºå®éªŒä¸­è§‚å¯Ÿåˆ°å¹³å‡BLEUå¢ç›Šä¸º+1.16ï¼ŒFiratç­‰äººã€‚ ï¼ˆ2016ï¼‰è·å¾—+1.8çš„BLEUå¢ç›Šï¼Œè€Œæˆ‘ä»¬çœ‹åˆ°+5.6çš„BLEUå¢ç›Šã€‚</p>

<p>The transfer learning approach we use is simple and effective. We first train an NMT model on a large corpus of parallel data (e.g., Frenchâ€“English). We call this the parent model. Next, we initialize an NMT model with the already-trained parent model. This new model is then trained on a very small par- allel corpus (e.g., Uzbekâ€“English). We call this the child model. Rather than starting from a random position, the child model is initialized with the weights from the parent model.</p>

<p>æˆ‘ä»¬ä½¿ç”¨çš„è¿ç§»å­¦ä¹ æ–¹æ³•ç®€å•æœ‰æ•ˆã€‚ æˆ‘ä»¬é¦–å…ˆåœ¨å¤§é‡çš„å¹³è¡Œæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæ³•è¯­-è‹±è¯­ï¼‰ä¸Šè®­ç»ƒä¸€ä¸ªNMTæ¨¡å‹ã€‚ æˆ‘ä»¬ç§°ä¹‹ä¸ºçˆ¶æ¨¡å‹ã€‚ æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç”¨å·²ç»è®­ç»ƒå¥½çš„çˆ¶æ¨¡å‹åˆå§‹åŒ–ä¸€ä¸ªNMTæ¨¡å‹ã€‚ ç„¶åï¼Œåœ¨ä¸€ä¸ªéå¸¸å°çš„å¹¶è¡Œè¯­æ–™åº“ï¼ˆå¦‚ä¹Œå…¹åˆ«å…‹è¯­-è‹±è¯­ï¼‰ä¸Šè®­ç»ƒè¿™ä¸ªæ–°æ¨¡å‹ã€‚ æˆ‘ä»¬æŠŠè¿™å«åšå„¿ç«¥æ¨¡å‹ã€‚ å­æ¨¡å‹ä¸æ˜¯ä»éšæœºä½ç½®å¼€å§‹ï¼Œè€Œæ˜¯ç”¨çˆ¶æ¨¡å‹çš„æƒé‡åˆå§‹åŒ–ã€‚</p>

<p>A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child model train- ing. In the Frenchâ€“English to Uzbekâ€“English ex- ample, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The parameters of the English embeddings are then frozen, while the Uzbek em- beddingsâ€™ parameters are allowed to be modified, i.e. fine-tuned, during training of the child model. Freezing certain transferred parameters and fine tun- ing others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameter space. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above.</p>

<p>è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç†ç”±æ˜¯ï¼Œåœ¨æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡å‹ä¸Šçš„å¼ºå…ˆéªŒåˆ†å¸ƒã€‚ åœ¨å¤§é‡åŒè¯­æ•°æ®ä¸Šè®­ç»ƒçš„çˆ¶æ¨¡å‹å¯ä»¥çœ‹ä½œé”šç‚¹ï¼Œæ˜¯æ¨¡å‹ç©ºé—´ä¸­å…ˆéªŒåˆ†å¸ƒçš„å³°å€¼ã€‚ å½“æˆ‘ä»¬è®­ç»ƒç”¨çˆ¶æ¨¡å‹åˆå§‹åŒ–çš„å­æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¿®æ­£äº†å¯èƒ½åœ¨ä»»åŠ¡ä¹‹é—´æœ‰ç”¨çš„å‚æ•°ï¼Œä»¥ä¾¿å®ƒä»¬åœ¨å­æ¨¡å‹è®­ç»ƒæœŸé—´ä¸ä¼šè¢«æ›´æ”¹ã€‚ åœ¨æ³•è¯­-è‹±è¯­åˆ°ä¹Œå…¹åˆ«å…‹è¯­-è‹±è¯­çš„ä¾‹å­ä¸­ï¼Œç”±äºåˆå§‹åŒ–ï¼Œä»çˆ¶æ¨¡å‹å¤åˆ¶è‹±è¯­å•è¯åµŒå…¥ï¼Œä½†æ˜¯ä¹Œå…¹åˆ«å…‹è¯­å•è¯æœ€åˆæ˜ å°„åˆ°éšæœºæ³•è¯­åµŒå…¥ã€‚ ç„¶åå†»ç»“è‹±æ–‡åµŒå…¥çš„å‚æ•°ï¼Œè€Œå…è®¸åœ¨è®­ç»ƒå­æ¨¡å‹æœŸé—´ä¿®æ”¹ä¹Œå…¹åˆ«å…‹åµŒå…¥çš„å‚æ•°ï¼Œå³å¾®è°ƒã€‚ å†»ç»“æŸäº›ä¼ é€’çš„å‚æ•°å’Œå¾®è°ƒå…¶ä»–å‚æ•°å¯ä»¥è¢«è®¤ä¸ºæ˜¯å¯¹åº”ç”¨äºä¸€äº›å‚æ•°ç©ºé—´çš„ç´§å…ˆéªŒæˆ–å¼ºæ­£åˆ™åŒ–çš„ç¡¬è¿‘ä¼¼ã€‚ æˆ‘ä»¬è¿˜å¯¹æ™®é€šçš„L2æ­£åˆ™åŒ–è¿›è¡Œäº†å®éªŒï¼Œä½†å‘ç°å®ƒä¸ä¸Šé¢æè¿°çš„å‚æ•°å†»ç»“ç›¸æ¯”æ²¡æœ‰æ˜æ˜¾çš„æ”¹å–„ã€‚</p>

<p>Our method results in large BLEU increases for a variety of low resource languages. In one of the four language pairs our NMT system using transfer beats a strong SBMT baseline. Not only do these transfer models do well on their own, they also give large gains when used for re-scoring n-best lists (n = 1000) from the SBMT system. Section 4 de- tails these results.</p>

<p>æˆ‘ä»¬çš„æ–¹æ³•å¯¼è‡´å„ç§ä½èµ„æºè¯­è¨€çš„BLEUå¤§å¹…å¢åŠ ã€‚ åœ¨å››ç§è¯­è¨€å¯¹ä¸­çš„ä¸€ç§è¯­è¨€å¯¹ä¸­ï¼Œæˆ‘ä»¬çš„NMTç³»ç»Ÿä½¿ç”¨çš„æ˜¯è¿ç§»ï¼Œè€Œä¸æ˜¯å¼ºå¤§çš„SBMTåŸºçº¿ã€‚ è¿™äº›è½¬ç§»æ¨¡å‹ä¸ä»…æœ¬èº«è¡¨ç°è‰¯å¥½ï¼Œè€Œä¸”åœ¨ç”¨äºä»SBMTç³»ç»Ÿä¸­é‡æ–°è¯„åˆ†n-æœ€ä½³åˆ—è¡¨(n=1000)æ—¶ä¹Ÿä¼šå¸¦æ¥å¾ˆå¤§çš„æ”¶ç›Šã€‚ ç¬¬å››èŠ‚å¯¹è¿™äº›ç»“æœè¿›è¡Œäº†åˆ†æã€‚</p>

<h3 id="4-experiments">4 Experiments</h3>
<p>To evaluate how well our transfer method works we apply it to a variety of low-resource languages, both stand-alone and for re-scoring a strong SBMT base- line. We report large BLEU increases across the board with our transfer method.</p>

<p>ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„è¿ç§»æ–¹æ³•çš„å·¥ä½œæƒ…å†µï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºå„ç§ä½èµ„æºè¯­è¨€ï¼ŒåŒ…æ‹¬ç‹¬ç«‹è¯­è¨€å’Œç”¨äºé‡æ–°è¯„åˆ†å¼ºå¤§çš„SBMTåŸºçº¿çš„è¯­è¨€ã€‚ æˆ‘ä»¬æŠ¥å‘Šä½¿ç”¨æˆ‘ä»¬çš„è½¬ç§»æ–¹æ³•å…¨é¢å¤§å¹…å¢åŠ BLEUã€‚</p>

<p>For all of our experiments with low-resource languages we use French as the parent source language and for child source languages we use Hausa, Turkish, Uzbek, and Urdu. The target language is al- ways English. Table 1 shows parallel training data set sizes for the child languages, where the language with the most data has only 1.8m English tokens. For comparison, our parent Frenchâ€“English model uses a training set with 300 million English tokens and achieves 26 BLEU on the development set. Table 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems when our transfer method is not used.</p>

<p>å¯¹äºæ‰€æœ‰ä½èµ„æºè¯­è¨€çš„å®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨æ³•è¯­ä½œä¸ºçˆ¶æºè¯­è¨€ï¼Œå¯¹äºå­æºè¯­è¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨è±ªè¨è¯­ã€åœŸè€³å…¶è¯­ã€ä¹Œå…¹åˆ«å…‹è¯­å’Œä¹Œå°”éƒ½è¯­ã€‚ ç›®çš„è¯­æ˜¯é€šç”¨è‹±è¯­ã€‚ è¡¨1æ˜¾ç¤ºäº†å­è¯­è¨€çš„å¹¶è¡Œè®­ç»ƒæ•°æ®é›†å¤§å°ï¼Œå…¶ä¸­æ•°æ®æœ€å¤šçš„è¯­è¨€åªæœ‰180ä¸‡ä¸ªè‹±è¯­æ ‡è®°ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ¯å…¬å¸æ³•æ–‡-è‹±æ–‡æ¨¡å¼ä½¿ç”¨äº†ä¸€å¥—æœ‰3äº¿ä¸ªè‹±æ–‡æ ‡è®°çš„åŸ¹è®­é›†ï¼Œå¹¶åœ¨å¼€å‘é›†ä¸Šå®ç°äº†26ä¸ªBLEUã€‚ è¡¨1è¿˜æ˜¾ç¤ºäº†SBMTç³»ç»Ÿå¾—åˆ†ä»¥åŠä¸ä½¿ç”¨ä¼ è¾“çš„NMTåŸºçº¿ã€‚ SBMTç³»ç»Ÿå’ŒNMTç³»ç»Ÿåœ¨ä¸é‡‡ç”¨æˆ‘ä»¬çš„ä¼ è¾“æ–¹æ³•æ—¶å­˜åœ¨å¾ˆå¤§çš„å·®è·ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table1.png" alt="190910-Table1.png" /></p>

<p>The SBMT system used in this paper is a string-to-tree statistical machine translation system (Galley et al., 2006; Galley et al., 2004). In this system there are two count-based 5-gram language models. One is trained on the English side of the WMT 2015 Englishâ€“French dataset and the other is trained on the English side of the low-resource bi- text. Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (Chiang et al., 2009).</p>

<p>æœ¬æ–‡ä½¿ç”¨çš„SBMTç³»ç»Ÿæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ°æ ‘çš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼ˆGalleyç­‰äººï¼Œ2006ï¼›Galleyç­‰äººï¼Œ2004ï¼‰ã€‚ åœ¨è¯¥ç³»ç»Ÿä¸­ï¼Œæœ‰ä¸¤ä¸ªåŸºäºè®¡æ•°çš„5-å…‹è¯­è¨€æ¨¡å‹ã€‚ ä¸€ä¸ªåœ¨2015å¹´WMTè‹±æ³•æ•°æ®é›†çš„è‹±æ–‡éƒ¨åˆ†æ¥å—åŸ¹è®­ï¼Œå¦ä¸€ä¸ªåœ¨èµ„æºä¸è¶³çš„åŒè¯­æ–‡æœ¬çš„è‹±æ–‡éƒ¨åˆ†æ¥å—åŸ¹è®­ã€‚ æ­¤å¤–ï¼ŒSBMTæ¨¡å‹ä½¿ç”¨æ•°åƒä¸ªç¨€ç–å‡ºç°çš„è¯æ±‡åŒ–å¥æ³•ç‰¹å¾ï¼ˆChiang et al.ï¼Œ2009ï¼‰ã€‚</p>

<p>For our NMT system, we use development sets for Hausa, Turkish, Uzbek, and Urdu to tune the learning rate, parameter initialization range, dropout rate, and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K, and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5, as in Zaremba et al. (2014). The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parent mod- els is 0.5 with a decay rate of 0.9 when the de- velopment perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm of all param- eters is greater than 5. The initial parameter range is [-0.08, +0.08]. We also initialize our forget-gate biases to 1 as specified by JÂ´ozefowicz et al. (2015) and Gers et al. (2000). For decoding we use a beam search of width 12.</p>

<p>å¯¹äºæˆ‘ä»¬çš„NMTç³»ç»Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨Hausaã€Turkishã€Uzbekå’ŒUrduçš„å¼€å‘é›†æ¥è°ƒæ•´æ‰€æœ‰å®éªŒçš„å­¦ä¹ é€Ÿç‡ã€å‚æ•°åˆå§‹åŒ–èŒƒå›´ã€è¾å­¦ç‡å’Œéšè—çŠ¶æ€å¤§å°ã€‚ å¯¹äºè®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨å°æ‰¹å¤§å°128ï¼Œéšè—çŠ¶æ€å¤§å°1000ï¼Œç›®æ ‡è¯æ±‡å¤§å°15Kï¼Œæºè¯æ±‡å¤§å°30Kã€‚ å¦‚Zarembaç­‰äººæ‰€è¿°ï¼Œå„¿ç«¥æ¨¡å‹çš„è¾å­¦ç‡ä¸º0.5ã€‚ ï¼ˆ2014å¹´ï¼‰ã€‚ å…¬å…±çˆ¶æ¨¡å‹ä»¥0.2çš„è¾å­¦ç‡è¿›è¡Œè®­ç»ƒã€‚ åœ¨å‘å±•å›°æƒ‘æ²¡æœ‰æ”¹å–„çš„æƒ…å†µä¸‹ï¼Œå„¿ç«¥å’Œå®¶é•¿æ¨¡å¼çš„å­¦ä¹ ç‡ä¸º0.5ï¼Œè¡°å‡ç‡ä¸º0.9ã€‚ è¿™äº›å„¿ç«¥æ¨¡ç‰¹éƒ½ç»è¿‡äº†100å¹´çš„è®­ç»ƒã€‚ å½“æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦èŒƒæ•°å¤§äº5æ—¶ï¼Œæˆ‘ä»¬é‡æ–°ç¼©æ”¾æ¢¯åº¦ã€‚ åˆå§‹å‚æ•°èŒƒå›´ä¸º[-0.08ï¼Œ+0.08]ã€‚ æˆ‘ä»¬è¿˜å°†é—å¿˜æ …åç½®åˆå§‹åŒ–ä¸º1ï¼Œå¦‚JÃ³zefowiczç­‰äººæ‰€è¿°ã€‚ ï¼ˆ2015å¹´ï¼‰å’ŒGersç­‰äººã€‚ ï¼ˆ2000å¹´ï¼‰ã€‚ å¯¹äºè§£ç ï¼Œæˆ‘ä»¬ä½¿ç”¨å®½åº¦ä¸º12çš„æ³¢æŸæœç´¢ã€‚</p>

<h3 id="41-transfer-results">4.1 Transfer Results</h3>
<p>The results for our transfer learning method applied to the four languages above are in Table 2. The parent models were trained on the WMT 2015 (Bojar et al., 2015) Frenchâ€“English corpus for 5 epochs. Our baseline NMT systems (â€˜NMTâ€™ row) all receive a large BLEU improvement when using the transfer method (the â€˜Xferâ€™ row) with an average BLEU improvement of 5.6. Additionally, when we use un- known word replacement from Luong et al. (2015b) and ensemble together 8 models (the â€˜Finalâ€™ row) we further improve upon our BLEU scores, bringing the average BLEU improvement to 7.5. Overall our method allows the NMT system to reach competitive scores and outperform the SBMT system in one of the four language pairs.</p>

<p>æˆ‘ä»¬çš„è¿ç§»å­¦ä¹ æ–¹æ³•åº”ç”¨äºä»¥ä¸Šå››ç§è¯­è¨€çš„ç»“æœè§è¡¨2ã€‚ æ¯æ¨¡å‹åœ¨2015å¹´WMTï¼ˆBojarç­‰äººï¼Œ2015å¹´ï¼‰æ³•è¯­-è‹±è¯­è¯­æ–™åº“ä¸Šè¿›è¡Œäº†5ä¸ªæ—¶æœŸçš„è®­ç»ƒã€‚ æˆ‘ä»¬çš„åŸºçº¿NMTç³»ç»Ÿï¼ˆâ€œNMTâ€è¡Œï¼‰åœ¨ä½¿ç”¨ä¼ è¾“æ–¹æ³•ï¼ˆâ€œXFERâ€è¡Œï¼‰æ—¶éƒ½å¾—åˆ°äº†å¾ˆå¤§çš„BLEUæ”¹è¿›ï¼Œå¹³å‡BLEUæ”¹è¿›ä¸º5.6ã€‚ å¦å¤–ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªLuongç­‰äººçš„æœªçŸ¥è¯æ›¿æ¢æ—¶ã€‚ (2015b)å’Œé›†æˆ8ä¸ªæ¨¡å‹ï¼ˆâ€œæœ€ç»ˆâ€è¡Œï¼‰ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ”¹å–„äº†æˆ‘ä»¬çš„BLEUå¾—åˆ†ï¼Œä½¿å¹³å‡BLEUæ”¹å–„åˆ°7.5ã€‚ æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸NMTç³»ç»Ÿåœ¨å››ç§è¯­è¨€å¯¹ä¸­çš„ä¸€ç§ä¸­è¾¾åˆ°ç«äº‰æ€§çš„åˆ†æ•°å¹¶ä¸”ä¼˜äºSBMTç³»ç»Ÿã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table2.png" alt="190910-Table2.png" /></p>

<h3 id="42-re-scoring-results">4.2 Re-scoring Results</h3>
<p>We also use the NMT model with transfer learn- ing as a feature when re-scoring output n-best lists (n = 1000) from the SBMT system. Table 3 shows the results of re-scoring. We compare re-scoring with transfer NMT to re-scoring with baseline (i.e. non-transfer) NMT and to re-scoring with a neural language model. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015; Williams et al., 2015). Additionally, it is trained us- ing dropout with a dropout probability of 0.2 as suggested by Zaremba et al. (2014). Re-scoring with the transfer NMT model yields an improvement of 1.1â€“ 1.6 BLEU points above the strong SBMT system; we find that transfer NMT is a better re-scoring feature than baseline NMT or neural language models.</p>

<p>åœ¨å¯¹SBMTç³»ç»Ÿè¾“å‡ºçš„N-Beståˆ—è¡¨(n=1000)è¿›è¡Œé‡æ–°è¯„åˆ†æ—¶ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†ä»¥è¿ç§»å­¦ä¹ ä¸ºç‰¹å¾çš„NMTæ¨¡å‹ã€‚ è¡¨3æ˜¾ç¤ºäº†é‡æ–°è¯„åˆ†çš„ç»“æœã€‚ æˆ‘ä»¬æ¯”è¾ƒäº†é‡è¯„åˆ†ä¸è¿ç§»NMTï¼Œé‡è¯„åˆ†ä¸åŸºçº¿ï¼ˆå³éè¿ç§»ï¼‰NMTå’Œé‡è¯„åˆ†ä¸ç¥ç»è¯­è¨€æ¨¡å‹ã€‚ è¯¥ç¥ç»è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªå…·æœ‰2å±‚1000ä¸ªéšæ€çš„LSTMç¥ç»ç½‘ç»œã€‚ å®ƒçš„ç›®æ ‡è¯æ±‡é‡ä¸º100Kï¼Œä½¿ç”¨å™ªå£°å¯¹æ¯”ä¼°è®¡è¿›è¡Œè®­ç»ƒï¼ˆMNIHå’ŒTEHï¼Œ2012å¹´ï¼›Vaswaniç­‰äººï¼Œ2013å¹´ï¼›Baltescuå’ŒBlunsomï¼Œ2015å¹´ï¼›Williamsç­‰äººï¼Œ2015å¹´ï¼‰ã€‚ æ­¤å¤–ï¼Œè¿˜æŒ‰ç…§Zarembaç­‰äººçš„å»ºè®®ï¼Œå¯¹è¾å­¦è€…è¿›è¡Œäº†è¾å­¦æ¦‚ç‡ä¸º0.2çš„åŸ¹è®­ã€‚ ï¼ˆ2014å¹´ï¼‰ã€‚ ç”¨è½¬ç§»NMTæ¨¡å‹è¿›è¡Œé‡æ–°è¯„åˆ†ï¼Œæ¯”å¼ºSBMTç³»ç»Ÿæé«˜äº†1.1-1.6ä¸ªBLEUç‚¹ï¼› æˆ‘ä»¬å‘ç°ï¼Œè¿ç§»NMTæ¯”åŸºçº¿NMTæˆ–ç¥ç»è¯­è¨€æ¨¡å‹å…·æœ‰æ›´å¥½çš„å†è¯„åˆ†ç‰¹å¾ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table3.png" alt="190910-Table3.png" /></p>

<p>In the next section, we describe a number of additional experiments designed to help us understand the contribution of the various components of our transfer model.</p>

<p>åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€äº›é¢å¤–çš„å®éªŒï¼Œè¿™äº›å®éªŒæ—¨åœ¨å¸®åŠ©æˆ‘ä»¬ç†è§£ä¼ è¾“æ¨¡å‹çš„å„ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚</p>

<h3 id="5-analysis">5 Analysis</h3>
<p>We analyze the effects of using different parent models, regularizing different parts of the child model, and trying different regularization techniques.</p>

<p>æˆ‘ä»¬åˆ†æäº†ä½¿ç”¨ä¸åŒçš„çˆ¶æ¨¡å‹ã€å¯¹å­æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†è¿›è¡Œæ­£åˆ™åŒ–ä»¥åŠå°è¯•ä¸åŒçš„æ­£åˆ™åŒ–æŠ€æœ¯çš„æ•ˆæœã€‚</p>

<h3 id="51-different-parent-languages">5.1 Different Parent Languages</h3>
<p>In the above experiments we use Frenchâ€“English as the parent language pair. Here, we experiment with different parent languages. In this set of experiments we use Spanishâ€“English as the child language pair. A description of the data used in this section is presented in Table 4.</p>

<p>åœ¨ä¸Šè¿°å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ³•è¯­å’Œè‹±è¯­ä½œä¸ºæ¯ä½“è¯­è¨€å¯¹ã€‚ åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨ä¸åŒçš„çˆ¶è¯­è¨€è¿›è¡Œå®éªŒã€‚ åœ¨è¿™ç»„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¥¿ç­ç‰™è¯­å’Œè‹±è¯­ä½œä¸ºå­è¯­è¨€å¯¹ã€‚ è¡¨4è¯´æ˜äº†æœ¬èŠ‚ä½¿ç”¨çš„æ•°æ®ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table4.png" alt="190910-Table4.png" /></p>

<p>Our experimental results are shown in Table 5, where we use French and German as parent languages. If we just train a model with no transfer on a small Spanishâ€“English training set we get a BLEU score of 16.4. When using our transfer method we get Spanishâ€“English BLEU scores of 31.0 and 29.8 via French and German parent languages, respectively. As expected, French is a better parent than German for Spanish, which could be the result of the parent language being more similar to the child language. We suspect using closely-related parent language pairs would improve overall quality.</p>

<p>æˆ‘ä»¬çš„å®éªŒç»“æœå¦‚è¡¨5æ‰€ç¤ºï¼Œå…¶ä¸­æˆ‘ä»¬ä½¿ç”¨æ³•è¯­å’Œå¾·è¯­ä½œä¸ºçˆ¶è¯­è¨€ã€‚ å¦‚æœæˆ‘ä»¬åªæ˜¯åœ¨ä¸€ä¸ªå°çš„è¥¿ç­ç‰™-è‹±è¯­è®­ç»ƒé›†ä¸Šè®­ç»ƒä¸€ä¸ªæ²¡æœ‰è½¬ä¼šçš„æ¨¡ç‰¹ï¼Œæˆ‘ä»¬å¾—åˆ°äº†16.4çš„BLEUåˆ†æ•°ã€‚ å½“ä½¿ç”¨æˆ‘ä»¬çš„è¿ç§»æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡æ³•è¯­å’Œå¾·è¯­çš„çˆ¶è¯­è¨€åˆ†åˆ«å¾—åˆ°31.0å’Œ29.8çš„è¥¿ç­ç‰™è¯­-è‹±è¯­BLEUåˆ†æ•°ã€‚ æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ³•è¯­æ¯”å¾·è¯­æ›´é€‚åˆè¥¿ç­ç‰™è¯­ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºçˆ¶è¯­è¨€ä¸å­è¯­è¨€æ›´ç›¸ä¼¼ã€‚ æˆ‘ä»¬æ€€ç–‘ä½¿ç”¨å¯†åˆ‡ç›¸å…³çš„çˆ¶è¯­è¨€å¯¹ä¼šæé«˜æ•´ä½“è´¨é‡ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table5.png" alt="190910-Table5.png" /></p>

<h3 id="52-effects-of-having-similar-parent-language">5.2 Effects of having Similar Parent Language</h3>
<p>Next, we look at a best-case scenario in which the parent language is as similar as possible to the child language.</p>

<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸ªæœ€å¥½çš„æƒ…å†µï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œçˆ¶è¯­è¨€ä¸å­è¯­è¨€å°½å¯èƒ½ç›¸ä¼¼ã€‚</p>

<p>Here we devise a synthetic child language (called French?) which is exactly like French, except its vocabulary is shuffled randomly. (e.g., â€œinternationaleâ€ is now â€œpomme,â€ etc). This language, which looks unintelligible to human eyes, nevertheless has the same distributional and relational properties as actual French, i.e. the word that, prior to vocabulary reassignment, was â€˜roiâ€™ (king) is likely to share distributional characteristics, and hence embedding similarity, to the word that, prior to reassignment, was â€˜reineâ€™ (queen). French should be the ideal par- ent model for French?.</p>

<p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆæˆçš„å„¿ç«¥è¯­è¨€ï¼ˆç§°ä¸ºæ³•è¯­ï¼Ÿï¼‰ è¿™å’Œæ³•è¯­ä¸€æ¨¡ä¸€æ ·ï¼Œåªä¸è¿‡å®ƒçš„è¯æ±‡æ˜¯éšæœºæ’åˆ—çš„ã€‚ ï¼ˆä¾‹å¦‚ï¼Œâ€œinternationaleâ€ç°åœ¨æ˜¯â€œpommeâ€ç­‰ï¼‰ã€‚ è¿™ä¸€è¯­è¨€åœ¨äººçœ¼çœ‹æ¥éš¾ä»¥ç†è§£ï¼Œä½†å´å…·æœ‰ä¸å®é™…æ³•è¯­ç›¸åŒçš„åˆ†å¸ƒå’Œå…³ç³»å±æ€§ï¼Œå³åœ¨è¯æ±‡é‡æ–°åˆ†é…ä¹‹å‰ä¸ºâ€œroiâ€ï¼ˆå›½ç‹ï¼‰çš„å•è¯å¯èƒ½å…·æœ‰åˆ†å¸ƒç‰¹å¾ï¼Œå› æ­¤åµŒå…¥äº†ä¸é‡æ–°åˆ†é…ä¹‹å‰ä¸ºâ€œreineâ€ï¼ˆçš‡åï¼‰çš„å•è¯çš„ç›¸ä¼¼æ€§ã€‚ æ³•è¯­åº”è¯¥æ˜¯æ³•è¯­çš„ç†æƒ³å…¸èŒƒã€‚</p>

<p>The results of this experiment are shown in Table 6. We get a 4.3 BLEU improvement with an unrelated parent (i.e. Frenchâ€“parent and Uzbekâ€“ child), but we get a 6.7 BLEU improvement with a â€˜closely relatedâ€™ parent (i.e. Frenchâ€“parent and French?â€“child). We conclude that the choice of par- ent model can have a strong impact on transfer mod- els, and choosing better parents for our low-resource languages (if data for such parents can be obtained) could improve the final results.</p>

<p>å®éªŒç»“æœè§è¡¨6ã€‚ æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª4.3 BLEUæ”¹è¿›ä¸ä¸€ä¸ªæ— å…³çš„çˆ¶æ¯ï¼ˆå³æ³•å›½çˆ¶æ¯å’Œä¹Œå…¹åˆ«å…‹å­©å­ï¼‰ï¼Œä½†æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª6.7 BLEUæ”¹è¿›ä¸ä¸€ä¸ªâ€˜å¯†åˆ‡ç›¸å…³â€™çš„çˆ¶æ¯ï¼ˆå³æ³•å›½çˆ¶æ¯å’Œæ³•å›½å­©å­ï¼‰ã€‚ æˆ‘ä»¬çš„ç»“è®ºæ˜¯ï¼ŒParentæ¨¡å‹çš„é€‰æ‹©å¯¹è¿ç§»æ¨¡å¼æœ‰å¾ˆå¤§çš„å½±å“ï¼Œä¸ºæˆ‘ä»¬çš„ä½èµ„æºè¯­è¨€é€‰æ‹©æ›´å¥½çš„çˆ¶è¯­è¨€ï¼ˆå¦‚æœå¯ä»¥è·å¾—è¿™æ ·çš„çˆ¶è¯­è¨€çš„æ•°æ®ï¼‰å¯ä»¥æ”¹å–„æœ€ç»ˆçš„ç»“æœã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table6.png" alt="190910-Table6.png" /></p>

<h3 id="53-ablation-analysis">5.3 Ablation Analysis</h3>
<p>In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to determine the scenario that yields optimal performance. Figure 2 shows a diagram of the components of a sequence- to-sequence model. Table 7 shows the effects of al- lowing various components of the child NMT model to be trained. We find that the optimal setting for transferring from Frenchâ€“English to Uzbekâ€“English in terms of BLEU performance is to allow all of the components of the child model to be trained except for the input and output target embeddings.</p>

<p>åœ¨æ‰€æœ‰ä¸Šè¿°å®éªŒä¸­ï¼Œåªæœ‰ç›®æ ‡è¾“å…¥å’Œè¾“å‡ºåµŒå…¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å›ºå®šçš„ã€‚ åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æå½“æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†å›ºå®šæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œä»¥ç¡®å®šäº§ç”Ÿæœ€ä½³æ€§èƒ½çš„åœºæ™¯ã€‚ å›¾2æ˜¾ç¤ºäº†åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ç»„ä»¶å›¾ã€‚ è¡¨7æ˜¾ç¤ºäº†è¦è®­ç»ƒçš„å„¿ç«¥NMTæ¨¡å‹çš„å„ä¸ªç»„æˆéƒ¨åˆ†çš„æ•ˆæœã€‚ æˆ‘ä»¬å‘ç°ï¼Œä»BLEUæ€§èƒ½çš„è§’åº¦æ¥çœ‹ï¼Œä»æ³•è¯­-è‹±è¯­è½¬æ¢åˆ°ä¹Œå…¹åˆ«å…‹è¯­-è‹±è¯­çš„æœ€ä½³è®¾ç½®æ˜¯å…è®¸è®­ç»ƒé™¤è¾“å…¥å’Œè¾“å‡ºç›®æ ‡åµŒå…¥ä¹‹å¤–çš„å­æ¨¡å‹çš„æ‰€æœ‰ç»„ä»¶ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-figure2.png" alt="190910-figure2.png" /></p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table7.png" alt="190910-Table7.png" /></p>

<p>Even though we use this setting for our main experiments, the optimum setting is likely to be language- and corpus-dependent. For Turkish, experiments show that freezing attention parameters as well gives slightly better results. For parent-child models with closely related languages we expect freezing, or strongly regularizing, more components of the model to give better results.</p>

<p>å³ä½¿æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªè®¾ç½®ä¸ºæˆ‘ä»¬çš„ä¸»è¦å®éªŒï¼Œæœ€ä¼˜çš„è®¾ç½®å¯èƒ½æ˜¯ä¾èµ–äºè¯­è¨€å’Œè¯­æ–™åº“ã€‚ å¯¹äºåœŸè€³å…¶è¯­ï¼Œå®éªŒè¡¨æ˜ï¼Œå†»ç»“æ³¨æ„å‚æ•°ä¹Ÿä¼šç»™å‡ºç¨å¥½çš„ç»“æœã€‚ å¯¹äºå…·æœ‰å¯†åˆ‡ç›¸å…³è¯­è¨€çš„çˆ¶å­æ¨¡å‹ï¼Œæˆ‘ä»¬å¸Œæœ›å†»ç»“æˆ–å¼ºæ­£åˆ™åŒ–æ¨¡å‹çš„æ›´å¤šç»„ä»¶ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚</p>

<h3 id="54-learning-curve">5.4 Learning Curve</h3>
<p>In Figure 3 we plot learning curves for both a transfer and a non-transfer model on training and development sets. We see that the final training set perplexities for both the transfer and non-transfer model are very similar, but the development set perplexity for the transfer model is much better.</p>

<p>åœ¨å›¾3ä¸­ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†åŸ¹è®­å’Œå¼€å‘é›†ä¸Šçš„è¿ç§»å’Œéè¿ç§»æ¨¡å‹çš„å­¦ä¹ æ›²çº¿ã€‚ æˆ‘ä»¬å‘ç°ï¼Œè¿ç§»æ¨¡å‹å’Œéè¿ç§»æ¨¡å‹çš„æœ€ç»ˆè®­ç»ƒé›†å›°æƒ‘éå¸¸ç›¸ä¼¼ï¼Œä½†è¿ç§»æ¨¡å‹çš„å‘å±•é›†å›°æƒ‘è¦å¥½å¾—å¤šã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-figure3.png" alt="190910-figure3.png" /></p>

<h3 id="54-learning-curve-1">5.4 Learning Curve</h3>
<p>In Figure 3 we plot learning curves for both a transfer and a non-transfer model on training and development sets. We see that the final training set perplexities for both the transfer and non-transfer model are very similar, but the development set perplexity for the transfer model is much better.</p>

<p>åœ¨å›¾3ä¸­ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†åŸ¹è®­å’Œå¼€å‘é›†ä¸Šçš„è¿ç§»å’Œéè¿ç§»æ¨¡å‹çš„å­¦ä¹ æ›²çº¿ã€‚ æˆ‘ä»¬å‘ç°ï¼Œè¿ç§»æ¨¡å‹å’Œéè¿ç§»æ¨¡å‹çš„æœ€ç»ˆè®­ç»ƒé›†å›°æƒ‘éå¸¸ç›¸ä¼¼ï¼Œä½†è¿ç§»æ¨¡å‹çš„å‘å±•é›†å›°æƒ‘è¦å¥½å¾—å¤šã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-figure3.png" alt="190910-figure3.png" /></p>

<p>The fact that the two models start from and converge to very different points, yet have similar train- ing set performances, indicates that our architecture and training algorithm are able to reach a good minimum of the training objective regardless of the initialization. However, the training objective seems to have a large basin of models with similar performance and not all of them generalize well to the development set. The transfer model, starting with and staying close to a point known to perform well on a related task, is guided to a final point in the weight space that generalizes to the development set much better.</p>

<p>è¿™ä¸¤ç§æ¨¡å‹ä»ä¸åŒçš„ç‚¹å¼€å§‹ï¼Œæ”¶æ•›åˆ°ä¸åŒçš„ç‚¹ï¼Œä½†è®­ç»ƒé›†æ€§èƒ½ç›¸ä¼¼ï¼Œè¡¨æ˜æ— è®ºåˆå§‹åŒ–ä¸å¦ï¼Œæˆ‘ä»¬çš„ç»“æ„å’Œè®­ç»ƒç®—æ³•éƒ½èƒ½å¾ˆå¥½åœ°è¾¾åˆ°è®­ç»ƒç›®æ ‡çš„æœ€å°å€¼ã€‚ ç„¶è€Œï¼ŒåŸ¹è®­ç›®æ ‡ä¼¼ä¹æœ‰ä¸€å¤§ç›†æ€§èƒ½ç›¸ä¼¼çš„æ¨¡å‹ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„æ¨¡å‹éƒ½èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°å¼€å‘é›†ã€‚ è½¬ç§»æ¨¡å‹ä»ä¸€ä¸ªå·²çŸ¥åœ¨ç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„ç‚¹å¼€å§‹å¹¶ä¿æŒåœ¨è¯¥ç‚¹é™„è¿‘ï¼Œè¢«å¼•å¯¼åˆ°æƒé‡ç©ºé—´ä¸­çš„æœ€åä¸€ç‚¹ï¼Œè¯¥ç‚¹æ›´å¥½åœ°æ¨å¹¿åˆ°å¼€å‘é›†ã€‚</p>

<h3 id="55-dictionary-initialization">5.5 Dictionary Initialization</h3>
<p>Using the transfer method, we always initialize input language embeddings for the child model with randomly-assigned embeddings from the par- ent (which has a different input language). A smarter method might be to initialize child embeddings with similar parent embeddings, where similarity is mea- sured by word-to-word t-table probabilities. To get these probabilities we compose Uzbekâ€“English and Englishâ€“French t-tables obtained from the Berkeley Aligner (Liang et al., 2006). We see from Figure 4 that this dictionary-based assignment results in faster improvement in the early part of the train- ing. However the final performance is similar to our standard model, indicating that the training is able to untangle the dictionary permutation introduced by randomly-assigned embeddings.</p>

<p>ä½¿ç”¨ä¼ è¾“æ–¹æ³•ï¼Œæˆ‘ä»¬æ€»æ˜¯ä½¿ç”¨æ¥è‡ªè§’è‰²ï¼ˆå…·æœ‰ä¸åŒè¾“å…¥è¯­è¨€ï¼‰çš„éšæœºåˆ†é…çš„åµŒå…¥æ¥åˆå§‹åŒ–å­æ¨¡å‹çš„è¾“å…¥è¯­è¨€åµŒå…¥ã€‚ ä¸€ç§æ›´èªæ˜çš„æ–¹æ³•å¯èƒ½æ˜¯ç”¨ç±»ä¼¼çš„çˆ¶åµŒå…¥æ¥åˆå§‹åŒ–å­åµŒå…¥ï¼Œå…¶ä¸­ç›¸ä¼¼æ€§ç”±å­—åˆ°å­—çš„tè¡¨æ¦‚ç‡æ¥åº¦é‡ã€‚ ä¸ºäº†å¾—åˆ°è¿™äº›æ¦‚ç‡ï¼Œæˆ‘ä»¬ç¼–å†™äº†ä»Berkeley Alignerè·å¾—çš„ä¹Œå…¹åˆ«å…‹è¯­-è‹±è¯­å’Œè‹±è¯­-æ³•è¯­Tè¡¨ï¼ˆLiangç­‰äººï¼Œ2006å¹´ï¼‰ã€‚ ä»å›¾4ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™ç§åŸºäºå­—å…¸çš„èµ‹å€¼åœ¨è®­ç»ƒçš„æ—©æœŸä¼šå¯¼è‡´æ›´å¿«çš„æ”¹è¿›ã€‚ ç„¶è€Œï¼Œæœ€ç»ˆçš„æ€§èƒ½ä¸æˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹ç›¸ä¼¼ï¼Œè¿™è¡¨æ˜è®­ç»ƒèƒ½å¤Ÿè§£å¼€ç”±éšæœºåˆ†é…çš„åµŒå…¥æ‰€å¼•å…¥çš„å­—å…¸æ’åˆ—ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-figure4.png" alt="190910-figure4.png" /></p>

<h3 id="56-different-parent-models">5.6 Different Parent Models</h3>
<p>In the above experiments, we use a parent model trained on a large Frenchâ€“English corpus. One might hypothesize that our gains come from exploiting the English half of the corpus as an additional language model resource. Therefore, we explore transfer learning for the child model with parent models that only use the English side of the Frenchâ€“ English corpus. We consider the following parent models in our ablative transfer learning scenarios:</p>

<p>åœ¨ä¸Šè¿°å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåœ¨å¤§å‹æ³•è¯­è‹±è¯­è¯­æ–™åº“ä¸Šè®­ç»ƒçš„çˆ¶æ¨¡å‹ã€‚ æœ‰äººå¯èƒ½ä¼šå‡è®¾ï¼Œæˆ‘ä»¬çš„æ”¶è·æ¥è‡ªäºåˆ©ç”¨è¯­æ–™åº“çš„è‹±è¯­éƒ¨åˆ†ä½œä¸ºé¢å¤–çš„è¯­è¨€æ¨¡å‹èµ„æºã€‚ å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨åªä½¿ç”¨è‹±æ³•è¯­æ–™åº“ä¸­è‹±è¯­éƒ¨åˆ†çš„çˆ¶æ¯æ¨¡å¼æ¥æ¢ç´¢å„¿ç«¥æ¨¡å¼çš„è¿ç§»å­¦ä¹ ã€‚ åœ¨æ¶ˆèæ€§è¿ç§»å­¦ä¹ æƒ…å¢ƒä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä»¥ä¸‹çˆ¶æ¯æ¨¡å‹:</p>

<p>â€¢ A true translation model (Frenchâ€“English Par- ent) <br />
â€¢ A word-for-word English copying model (Englishâ€“English Parent)<br />
â€¢ A model that unpermutes scrambled English (EngPermâ€“English Parent)<br />
â€¢ (The parameters of) an RNN language model (LM Xfer)<br />
â€¢ çœŸæ­£çš„ç¿»è¯‘æ¨¡å¼ï¼ˆæ³•è‹±éƒ¨åˆ†ï¼‰ <br />
â€¢ é€å­—è‹±æ–‡å¤åˆ¶æ¨¡å¼ï¼ˆè‹±æ–‡-è‹±æ–‡å®¶é•¿ï¼‰ <br />
â€¢ ä¸ç½®ä¹±è‹±è¯­çš„æ¨¡å¼ï¼ˆengperm-english parentï¼‰ <br />
â€¢ï¼ˆçš„å‚æ•°ï¼‰RNNè¯­è¨€æ¨¡å‹ï¼ˆLM xFERï¼‰</p>

<p>The results, in Table 8, show that transfer learning does not simply import an English language model, but makes use of translation parameters learned from the parentâ€™s large bilingual text.</p>

<p>è¡¨8ä¸­çš„ç»“æœè¡¨æ˜ï¼Œè¿ç§»å­¦ä¹ ä¸æ˜¯ç®€å•åœ°å¯¼å…¥è‹±è¯­è¯­è¨€æ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨ä»çˆ¶æ¯çš„å¤§å‹åŒè¯­æ–‡æœ¬ä¸­å­¦ä¹ åˆ°çš„ç¿»è¯‘å‚æ•°ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@1.0/assets/img/blog/190910-Table8.png" alt="190910-Table8.png" /></p>

<h3 id="6-conclusion">6 Conclusion</h3>
<p>Overall, our transfer method improves NMT scores on low-resource languages by a large margin and al- lows our transfer NMT system to come close to the performance of a very strong SBMT system, even exceeding its performance on Hausaâ€“English. In addition, we consistently and significantly improve state-of-the-art SBMT systems on low-resource languages when the transfer NMT system is used for re- scoring. Our experiments suggest that there is still room for improvement in selecting parent languages that are more similar to child languages, provided data for such parents can be found.</p>

<p>æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„è¿ç§»æ–¹æ³•å¤§å¤§æé«˜äº†ä½èµ„æºè¯­è¨€çš„NMTæˆç»©ï¼Œä½¿æˆ‘ä»¬çš„è¿ç§»NMTç³»ç»Ÿæ¥è¿‘äºä¸€ä¸ªéå¸¸å¼ºå¤§çš„SBMTç³»ç»Ÿçš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†Hausa-Englishç³»ç»Ÿçš„æ€§èƒ½ã€‚ æ­¤å¤–ï¼Œåœ¨ä½¿ç”¨è¿ç§»NMTç³»ç»Ÿè¿›è¡Œé‡æ–°è¯„åˆ†æ—¶ï¼Œæˆ‘ä»¬ä¸€è‡´ä¸”æ˜¾è‘—åœ°æ”¹è¿›äº†ä½èµ„æºè¯­è¨€ä¸Šçš„æœ€æ–°SBMTç³»ç»Ÿã€‚ æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨é€‰æ‹©æ›´ç±»ä¼¼äºå­è¯­è¨€çš„çˆ¶è¯­è¨€æ–¹é¢ä»æœ‰æ”¹è¿›çš„ä½™åœ°ï¼Œå‰ææ˜¯å¯ä»¥æ‰¾åˆ°è¿™ç±»çˆ¶è¯­è¨€çš„æ•°æ®ã€‚</p>
:ET