I"¶<h1 id="scrapyè·å–ç½‘ç«™è¯æ±‡è¡¨">Scrapyè·å–ç½‘ç«™è¯æ±‡è¡¨</h1>

<h2 id="1scrapyçˆ¬è™«çš„åˆ›å»º">1ã€scrapyçˆ¬è™«çš„åˆ›å»º</h2>

<p>ã€€ã€€åœ¨vscodeçš„Terminalä¸­è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼š</p>

<p>ã€€ã€€ã€€ã€€åˆ›å»ºscrapyé¡¹ç›®ï¼šscrapy startproject bio</p>

<p>ã€€ã€€ã€€ã€€è¿›å…¥åˆ°é¡¹ç›®ç›®å½•ä¸­ï¼šcd bio</p>

<p>ã€€ã€€ã€€ã€€åˆ›å»ºä¸€ä¸ªæ–°çš„spiderï¼šscrapy genspider bio2 yingyucihui.scientrans.com</p>

<p>ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ Spider çš„åç§°ï¼Œ ç¬¬äºŒ ä¸ªå‚æ•°æ˜¯ç½‘ç«™åŸŸå ã€‚ æ‰§è¡Œå®Œæ¯•ä¹‹åï¼Œ spiders æ–‡ä»¶å¤¹ä¸­å¤šäº†ä¸€ä¸ª bio2.py ï¼Œ å®ƒå°±æ˜¯åˆšåˆšåˆ›å»ºçš„ Spider, å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># -*- coding: utf-8 -*-
#http://yingyucihui.scientrans.com/shengwucihui/index.html

import scrapy
from bio.items import BioItem
from scrapy import Request

class Bio2Spider(scrapy.Spider):
    name = 'bio2'
    allowed_domains = ['yingyucihui.scientrans.com']
    start_urls = ['http://yingyucihui.scientrans.com/shengwucihui/238_65_1.html']

    def parse(self, response):
        print(response.url)
        title = response.xpath('//dd/h2/text()').extract_first()
        words = response.xpath('//dd/text()').extract()

        for item in zip(title, words):
            yield {
                # 'title':item[:],
                'words':item[1],
            }

        next1 = 'http://yingyucihui.scientrans.com/shengwucihui/'
        next2 = response.xpath("//span/a[@class='content2']/@href").extract()
        next = next1 + next2[-1]
        # print(next2[-1])
        yield Request(next)

</code></pre></div></div>

<h2 id="2scrapyçˆ¬è™«ä»£ç ç¼–å†™">2ã€scrapyçˆ¬è™«ä»£ç ç¼–å†™</h2>

<h3 id="21itemsæ–‡ä»¶ç¼–å†™">2.1itemsæ–‡ä»¶ç¼–å†™</h3>

<p>ã€€ã€€åœ¨items.pyæ–‡ä»¶ä¸­å®šä¹‰è‡ªå·±è¦æŠ“å–çš„æ•°æ®ï¼Œæˆ‘ä»¬è¦çˆ¬å–å¤©å–„æ™ºèƒ½ç½‘ç«™çš„è¯¾ç¨‹ã€è¯¾ç¨‹é“¾æ¥å’Œå­¦ä¹ äººæ•°ï¼Œéœ€è¦è¿™ä¸‰è€…çš„æ•°æ®ï¼Œæ‰€ä»¥æ­¤æ—¶åˆ›å»ºitemçš„ä¸‰ä¸ªç±»ã€‚</p>
:ET