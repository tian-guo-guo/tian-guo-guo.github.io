I"r<h1 id="2020-overview-of-the-6th-workshop-on-asian-translation">2020 Overview of the 6th Workshop on Asian Translation</h1>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geswdiqwntj30q40jgdjh.jpg" alt="image-20200515095314474" /></p>

<h1 id="摘要">摘要</h1>

<p>​		本文介绍了第六届亚洲翻译研讨会(WAT2019)分享任务的结果，包括JA EN，JA ZH科学论文翻译子任务，JA EN，JA KO，JA EN专利翻译子任务，HI EN，MY EN，KM EN，TA EN混合域子任务，RU JA新闻评论翻译任务和EN HI多模态翻译任务。 对于WAT2019，25个团队参与了分担任务。 此外，我们亦接获10份研究报告，其中7 1份已获接纳。 向自动评估服务器提交了约400份翻译结果，并对选定的提交进行了人工评估。</p>

<h1 id="1-introduction">1 Introduction</h1>

<p>​		亚洲翻译工作坊(WAT)是一个以亚洲语言为重点的公开评估活动。 继前几届工作坊WAT2014-WAT2018（Nakazawa et al.，2014，2015，2016，2017，2018）的成功之后，WAT2019将机器翻译研究者和用户聚集在一起，尝试，评估，分享和讨论机器翻译的全新思路。 我们一直致力于在所有亚洲国家中实际使用机器翻译。</p>

<p>​		对于第六届WAT，我们在WAT2018的大部分子任务之外，采用了新的翻译子任务，有高棉英语和泰米尔英语混合域语料库，<a href="http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/">2</a>俄罗斯日语新闻评论语料库和英语印地语多模态语料库<a href="https://ufal.mff.cuni.cz/hindi-visual-genome/ wat-2019-multimodal-task">3</a>。</p>

<p>​		WAT是一个独特的亚洲语言翻译工作坊，具有以下特点:</p>

<p>​		•开放创新平台由于测试数据固定且开放，我们可以多年来对同一数据集上的翻译系统进行重复评估。 WAT随时接收提交的资料； 即没有翻译结果的提交期限W.R.T自动评价翻译质量。</p>

<p>​		•领域和语言对WAT是世界上第一个针对科学论文领域以及中日韩日语对的研讨会。 今后，我们将增加更多的亚洲语言如越南语，泰语等。</p>

<p>​		•评估方法评估既可以自动进行，也可以手动进行。 首先，使用BLEU，RIBES和AMFM三个度量标准对所有提交的翻译结果进行自动评估。 其中，选定的翻译结果采用两种人为评价:成对评价和初专干事充分性评价。</p>

<h1 id="2-datasets">2 Datasets</h1>

<h2 id="21-aspec">2.1 ASPEC</h2>

<p>​		ASPEC由日本科学技术厅(JST)与国家信息和通信技术研究所(NICT)合作建造。 该语料库由用于JA en子任务的日英科技论文摘要语料库（ASPEC-JE）和用于JA zh子任务的日汉科技论文摘录语料库（ASPEC-JC）组成。 每个语料库的统计数据如表1所示。</p>

<h3 id="211-aspec-je">2.1.1 ASPEC-JE</h3>

<p>​		ASPEC-JE的训练数据是由NICT从JST拥有的大约200万篇日文-英文科学论文摘要中构建的。 数据为可比语料库，使用Utiyama和Isahara(2007)的方法自动查找句子对应关系。 每个句子对都伴随着由该方法计算的相似度分数和表示科学领域的领域ID。 在ASPEC-JE的README文件中描述了字段ID和字段名称之间的对应关系，以及训练数据的频率和出现率。</p>

<p>​		从排除训练数据中句子的日英论文摘要中的平行句子中提取开发，开发-测试和测试数据。 每个数据集由400个文档组成，并以相同的速率包含每个字段中的句子。 文档对齐是自动进行的，只包括1对1对齐的文档。 因此，有可能恢复原始文件。 格式与训练数据相同，只是没有相似度得分。</p>

<h3 id="212-aspec-jc">2.1.2 ASPEC-JC</h3>

<p>​		ASPEC-JC是一个平行语料库，由JST的文献数据库和电子期刊网站J-STAGE中的日语科技论文组成，并经必要的学术协会许可翻译成中文。 摘要和段落单元从正文中选取，以便包含最高的总体词汇覆盖率。</p>

<p>​		开发，开发测试和测试数据是从整个语料库中包含单个段落的文档中随机抽取的。 每套包含400段（文献）。 没有跨培训，开发，开发测试和测试集共享相同数据的文档。</p>

<h2 id="22-jpc">2.2 JPC</h2>

<p>​		日本专利局(JPO)与NICT合作，为专利任务建立了JPO专利语料库(JPC)。 该语料库包括中日，韩日和英日专利著录，国际专利分类(IPC)部分为化学，电气，机械工程和物理。</p>

<p>​		在WAT2019上，专利任务有两个子任务:normal subtask和expression pattern subtask子任务。 这两个子任务对每个语言对都使用通用的训练，开发和开发-测试数据。 针对三种语言对的正常子任务使用四个不同特征的测试数据:</p>

<p>​		•test-N:以下三组的联合；</p>

<p>​		•test-N1:2011年至2013年期间公布的专利族专利文件；</p>

<p>​		•test-N2:2016年至2017年期间公布的专利族专利文件； 和</p>

<p>​		•test-N3:2016年至2017年间公布的专利文件，其中目标句子是通过翻译源句子手工创建的。</p>

<p>​		ZH JA对的expression pattern子任务使用test-EP数据。 test-EP数据由注释有表达模式类别的句子组成:发明标题(TIT)，摘要(ABS)，权利要求范围(CLM)或描述(DES)。 语料库统计如表2所示。 注意，训练，开发，开发-测试和测试-N1数据与WAT2017中使用的数据相同。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesxfb4drej30du06gq3r.jpg" alt="image-20200515102934210" /></p>

<p>Table 2: Statistics for JPC</p>

<h2 id="23-tddc">2.3 TDDC</h2>

<p>​		及时披露文件语料库(TDDC)是由日本交易所集团(JPX)建设的。 该语料库是从东京证券交易所(TSE)上市公司公布的以往PDF格式的日文和英文及时披露文件中人工对齐句子而成。 及时披露任务侧重于从及时披露文件中摘录的句子的日文到英文翻译，以避免会使投资者感到困惑的误译。</p>

<p>​		东京证交所是全球最大的资本市场之一，截至2018年底，已有超过3600家公司上市。 公司必须及时向公众披露重大信息，包括财务报表，公司行动和公司治理政策。 这些及时披露文件构成投资决策的重要依据，包含重要数字（如销售额，利润，重要日期）和专有名词（如人名，地名，公司，业务和产品）。 由于这类信息对投资者至关重要，因此应避免误译，译文应具有高质量。</p>

<p>​		语料库由日英句子对，文档散列，句子散列组成。 文档散列是文档ID的散列，文档ID是源文档的唯一标识符。 句子散列是文档ID和句子ID的散列，句子ID是每个源文档中句子的唯一标识符。</p>

<p>​		语料库被划分为训练数据，开发数据，开发测试数据和测试数据。 训练数据被拆分成来自不同时期的两（2）组数据。 第一个数据集是根据2016年1月1日至2017年12月31日披露的文件创建的，第二个数据集是根据2018年1月1日至2018年6月30日的文件创建的。 开发，开发-测试和测试数据集是从2018年1月1日至2018年6月30日披露的及时披露文件中提取的，不包括用于创建训练数据的文件。 这一时期的文档是随机选择的，句子是从每个随机选择的，离散的文档集中提取的，这样提取的来源就不会有偏差。 因此，用于训练，开发，开发-测试和测试数据的源文档集是相互独立的。 进一步地，开发，开发-测试和测试的每个数据集被进一步拆分成两（2）组数据:以日语句号(.:U+3002)结尾的句子被分类为‘文本’，它有各种句子，其他的被分类为‘项目’，它有许多重复和相似的表达。 每个语料库的统计数据如表3所示。</p>

<h2 id="24时事语料库">2.4时事语料库</h2>

<p>​		时事语料库是由时事出版社有限公司与NICT合作建设的。 语料库由时事社新闻文本组成，包括政治，经济，国家，商业，市场，体育等各类新闻。 语料库分为训练，发展，发展-测试和测试数据，测试数据由日英句对组成。 每个语料库的统计数据如表4所示。</p>

<p>​		使用（Utiyama and Isahara，2007）中的方法，以与ASPEC相同的方式识别每个数据中的句子对。</p>

<h2 id="25-iitb语料库">2.5 IITB语料库</h2>

<p>​		孟买英语-印地语语料库包含英语-印地语平行语料库以及从各种来源和语料库收集的印地语单语语料库。 这个语料库是孟买印度语言技术中心多年来开发的。 该语料库用于混合领域任务。 语料库的统计信息如表5所示。</p>

<p>2.6 ALT和UCSY语料库</p>

<p>​		WAT2019缅甸英语翻译任务的平行数据由ALT语料库和UCSY语料库两个语料库组成。</p>

<p>​		•ALT语料库是亚洲语言树库(ALT)项目（Riza et al.，2016）的一部分，由20000个新闻文章中的缅甸-英语平行句子组成。</p>

<p>​		•UCSY语料库（Yi Mon Shwe Sin and Khin Mar Soe，2018）由缅甸仰光计算机研究大学(UCSY)NLP实验室构建。 语料库由20万个缅甸英语平行句组成，来自不同领域，包括新闻文章和教科书。</p>

<p>​		ALT语料库已被人工切分成词（Ding et al.，2018，2019），UCSY语料库未被切分。 将缅甸数据标记为写入单元的脚本随数据一起发布。 缅甸翻译结果的自动评估是基于标记化的书写单位，而不考虑ALT数据中的分段词。 然而，参与者可以自己的方式使用ALT数据中的分段。</p>

<p>​		缅甸英语翻译任务的训练，发展和测试数据的详细构成见表6。 注意，这两个语料库都是从WAT2018中使用的数据中修改的。</p>

<p>2.7 ALT和ECCC语料库</p>

<p>​		WAT2019高棉语-英语翻译任务的并行数据由ALT语料库和ECCC语料库两个语料库组成。</p>

<p>​		•高棉语语料库是亚洲语言树库(ALT)项目（Riza等人，2016年）的一部分，由新闻文章中的两万个高棉语-英语平行句子组成。 		•柬埔寨法院特别法庭的高棉英语双语记录文件对摘录了10万个高棉英语平行句子，由柬埔寨国家邮政，电信和信通技术研究所收集。</p>

<p>​		ALT语料库已被人工切分成词（Ding et al.，2018），ECCC语料库未被切分。 将高棉语数据标记为书写单元的脚本随数据一起发布。 高棉语翻译结果的自动评估是基于标记化的书写单位，而不考虑ALT数据中的分段词。 然而，参与者可以自己的方式使用ALT数据中的分段。</p>

<p>​		高棉语-英语翻译任务的训练，发展和测试数据的详细构成见表7。</p>

<h2 id="28多模态任务语料库">2.8多模态任务语料库</h2>

<p>​		对于英语印地语多模态翻译任务，我们要求参与者使用印地语视觉基因组语料库（HVG，Parida et al.，2019a，b）。 语料库的统计数据如表8所示。 原始HVG中的一个“项目”由一个带有突出显示图像一部分的矩形区域的图像，这个区域的原始英文标题和印地语参考翻译组成。 根据跟踪（见下面的2.8.1)，这些项目组件中的一些可用作源，一些可用作参考或扮演竞争候选解决方案的角色。</p>

<p>​		HVG培训，D-Test和E-Test部分可提前供参与者使用。 参与者被明确指示不要以任何方式查阅E-Test，但严格地说，他们本可以使用参考译文（从评估的角度来看，这将意味着作弊）。 C-Test只针对任务本身提供:源端被分发到任务参与者，目标端只在输出提交截止日期后发布。</p>

<p>​		注意，原始的可视基因组遭受相当程度的噪声。 一些观察到的英语语法错误如图1所示。 我们也抓住这个机会，使用我们的人工评估来验证图片所给字幕的质量，见下面的8.4.1。</p>

<p>​		多模式任务包括三条轨道，如图1所示:</p>

<h3 id="281多模态任务轨迹">2.8.1多模态任务轨迹</h3>

<p>​		1.纯文本翻译（WAT官方表格中标注为“文本”）:要求参与者将简短的英文说明（文本）翻译成印地语。 不能使用任何视觉信息。 另一方面，允许额外的文本资源（但需要在相应的系统描述论文中指定）。</p>

<p>​		2.印地语字幕（标记为“Hi”）:要求参与者为输入图像中给定的矩形区域生成印地语字幕。</p>

<p>​		3.多模态翻译（标为“MM”）:给定一幅图像，其中的一个矩形区域以及矩形区域的英文说明，要求参与者将英文文本翻译成印地语。 文本信息和视觉信息都可以使用。</p>

<h2 id="29-entam语料库">2.9 EnTam语料库</h2>

<p>​		对于泰米尔语英语翻译任务，我们要求参与者使用公开的EnTam混合域语料库4（Ramasamy et al.，2012）。 该语料库包含训练，发展和测试句子，大部分来自新闻领域。 其他领域是圣经和电影。 语料库的统计数据如表9所示。</p>

<h2 id="210-jarunc语料库">2.10 JaRuNC语料库</h2>

<p>​		对于俄语日语任务，我们要求参与者使用JaRuNC语料库5（Imankulova et al.，2019），该语料库属于新闻评论领域。 此数据集是手动对齐和清理的，是三种语言的。 它也可以用来评价俄语英语翻译质量，但这超出了本年度子任务的范围。 领域内平行语料库统计见表10。 此外，我们鼓励学员使用来自不同来源的领域外平行语料库，如KFTT，6 JESC，7 TED，8 ASPEC，9 UN，10 Yandex 11和俄语英语新闻评论语料库。 12</p>

<h1 id="3-基线系统">3 基线系统</h1>

<p>​		Human evaluations对大多数WAT任务的评估是以对特定基线系统的翻译结果和每个参与者系统的翻译结果进行成对比较的方式进行的。 也就是说，特定的基线系统是人类评估的标准。 <strong>在WAT 2019上，我们采用了以注意力机制为基线系统的神经机器翻译(NMT)</strong>。</p>

<p>​		NMT基线系统由可公开获得的软件组成，建立这些系统和使用这些系统进行翻译的程序公布在WAT的网页上<a href="http://lotus.kuee.kyoto-u.ac.jp/WAT/">13</a>。我们亦设有SMT基线系统，以应付于二零一七年年底或之前开始的工作。 基准系统如表11，12和13所示。 SMT基线系统在WAT 2017年概述论文中进行了描述（Nakazawa等人，2017年）。 商业RBMT系统和在线翻译系统由组织者操作。 我们注意到这些RBMT公司和在线翻译公司并没有屈服于自己。 因为我们的目的不是比较商业RBMT系统或来自自己没有参与的公司的在线翻译系统，所以这些系统的系统ID在本文中是匿名的。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesxkq0xbkj313q0bejud.jpg" alt="image-20200515103446129" /></p>

<p>Table 11: Baseline Systems I</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesxmhmieuj314o07idhh.jpg" alt="image-20200515103627992" /></p>

<p>Table 12: Baseline Systems II</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesxmqvxtfj30si04q3z7.jpg" alt="image-20200515103643046" /></p>

<p>Table 13: Baseline Systems III</p>

<h2 id="31培训数据">3.1培训数据</h2>

<p>​		我们使用以下数据来训练NMT基线系统。</p>

<p>​		•除了ASPEC日语-英语任务外，每个任务的所有训练数据都用于训练。 对于ASPEC日英任务，我们只使用了train-1.txt，它由一百万个相似度分数很高的平行句子对组成。</p>

<p>​		•每项任务的所有开发数据都用于验证。</p>

<h2 id="32标记化">3.2标记化</h2>

<p>​		我们使用以下工具进行标记化。</p>

<h3 id="321适用于aspecjpctddcjijialtucsyeccc和iitb">3.2.1适用于ASPEC，JPC，TDDC，JIJI，ALT，UCSY，ECCC和IITB</h3>

<p>​		•Juman版本7.0 14用于日语分段。</p>

<p>​		<strong>•Stanford Word Segmenter版本201401-04 <a href="http://nlp.stanford.edu/software/segmenter. shtml">15（中文Penn Treebank(CTB)模型）</a>，用于中文切分。</strong></p>

<p>​		<strong>•The Moses toolkit for English and Indonesian tokenization.。</strong></p>

<p>​		•用于韩语分段的Mecab-ko 16。</p>

<p>​		•用于Indic语言分割的Indic NLP库17。</p>

<p>​		•用于缅甸语和高棉语分段的ALT语料库中包括的工具。</p>

<p>​		•subword-nmt for all languages.<a href="https://github.com/rsennrich/subword-nmt">18</a>。</p>

<p>​		当我们构建BPE代码时，我们合并了源和目标语句，并且我们使用100,000作为-s选项。 当subword-nmt应用BPE时，我们使用10作为词汇阈值。</p>

<h3 id="322-entam新闻评论">3.2.2 EnTam:新闻评论</h3>

<p>​		•摩西工具包，仅用于新闻评论数据的英文和俄文。</p>

<p>​		•用于日语分段的Mecab 19。</p>

<p>​		•EnTam语料库未被任何外部工具包标记。</p>

<p>​		•两个语料库都经过Tensor2Tensor内部的预处理/后处理，包括分词。</p>

<h3 id="323多模式任务">3.2.3多模式任务</h3>

<p>​		•印地语视觉基因组是未标记的，我们没有使用或推荐任何特定的外部标记。</p>

<p>​		•标准的OpenNMT-py子词切分用于基线系统的前/后处理，每个参与者使用他们想要的。</p>

<h2 id="33基准nmt方法">3.3基准NMT方法</h2>

<p>​		我们在大多数任务中都注意地使用了以下NMT。 我们将Transformer（Vaswani et al.，2017）(Tensor2Tensor))用于新闻评论和英语泰米尔语任务，将Transformer（OpenNMT-py）用于多模态任务。</p>

<h3 id="331-nmt-with-attention">3.3.1 NMT with Attention</h3>

<p>我们使用OpenNMT（Klein等人，2017）作为带注意力的NMT（系统ID:NMT）的基线NMT系统的实现。 我们使用了以下OpenNMT配置。</p>

<p>​		•encoder_type=brnn</p>

<p>​		•brnn_merge=concat</p>

<p>​		•src_seq_length=150</p>

<p>​		•tgt_seq_length=150</p>

<p>​		•src_vocab_size=100000</p>

<p>​		•tgt_vocab_size=100000</p>

<p>​		•src_words_min_frequency=1</p>

<p>​		•tgt_words_min_frequency=1</p>

<p>​		其他系统参数使用默认值。</p>

<h3 id="332-transformer-tensor2tensor">3.3.2 Transformer (Tensor2Tensor)</h3>

<p>​		对于新闻评论和英语泰米尔语任务，我们使用了Tensor2Tensor对变压器的20实现（Vaswani等人，2017），并对所有基线模型使用对应于“基础”模型的默认超参数设置。 新闻评论任务的基线是Imankulova等人所描述的多语言模型。 （2019），仅使用域内并行语料库进行训练。 我们使用（Johnson等人，2017）提出的令牌技巧来训练多语言模型。 至于英语泰米尔任务，我们用32,000个单独的子词词汇为每个翻译方向训练单独的基线模型。</p>

<h3 id="333-transformer-opennmt-py">3.3.3 Transformer (OpenNMT-py)</h3>

<p>​		对于多模态任务，我们使用了在OpenNMT-py（Klein等人，2017）中实现的转换器模型（Vaswani等人，2018），并使用了具有默认参数的多模态任务基线“基础”模型。 我们为源语言和目标语言共同生成了32K子词类型的词汇表。 该词汇表在编码器和解码器之间共享。</p>

<h1 id="4-automatic-evaluation">4 Automatic Evaluation</h1>

<h2 id="41-procedure-for-calculating-automatic-evaluation-score">4.1 Procedure for Calculating Automatic Evaluation Score</h2>

<p>​		我们通过三个指标评估翻译结果:BLEU（Papineni等人，2002），RIBES（Isozaki等人，2010）和AMFM（Banchs等人，2015）。 使用Moses toolkit（Koehn et al.，2007）中的Multi-Bleu.perl计算BLEU得分。 RIBES评分使用RIBES.py版本1.02.4计算。 使用WAT2019网页中列出的技术合作者创建的脚本计算出21个AMFM分数。 22每项任务的所有分数均使用相应的参考译文计算。</p>

<p>​		在计算自动评估分数之前，使用标记化/切分工具对翻译结果进行标记化或切分。 对于日语分割，我们使用了三种不同的工具:Juman版本7.0（Kurohashi等人，1994），KyTea 0.4.6（Neubig等人，2011）和MeCab 0.996(Kudo，2005），前者具有完整的SVM模型23，后者具有IPA dictionary 2.7.024。</p>

<p>​		进行中文分词，我们使用了两种不同的工具:KyTea 0.4.6和Stanford Word Segmenter（Tseng，2005）version 2014-06-16和中文Penn Treebank(CTB)和北京大学(PKU)模型25。</p>

<p>​		对于韩语分割，我们使用了MECAB-KO。 26对于缅甸和高棉语分段，我们使用myseg.py 27和kmseg.py 28。 对于英语和俄语标记，我们使用了Moses工具包中的Tokenizer.perl29。 对于印地语和泰米尔语标记，我们使用了印度NLP库。 30自动评估的详细程序载于WAT2019评估网页。 31</p>

<h2 id="42-automatic-evaluation-system">4.2 Automatic Evaluation System</h2>

<p>​		自动评价系统由参与者接收翻译结果，并对上传的结果自动给出评价分数The automatic evaluation system receives translation results by participants and automatically gives evaluation scores to the uploaded results.。 如图2所示，系统要求参与者为每次提交提供以下信息:</p>

<p>​		•人的评价:是否提交结果供人评价；Human Evaluation: whether or not they submit the results for human evaluation;</p>

<p>​		•公布评价结果:无论是否允许在WAT2019网页上公布自动评价分数。Publish the results of the evaluation: whether or not they permit to publish automatic evaluation scores on the WAT2019 web page.</p>

<p>​		•任务:提交结果的任务；Task: the task you submit the results for;</p>

<p>​		•使用的其他资源:是否使用了额外资源； 和Used Other Resources: whether or not they used additional resources; and</p>

<p>​		•方法:方法类型包括SMT，RBMT，SMT加RBMT，EBMT，NMT等。</p>

<p>​		通过WAT2019评价网页公开参与者允许发表的翻译结果的评价分数。 参与者还可以使用相同的web界面提交结果供人类评估。</p>

<p>​		即使在WAT2019年之后，这套自动评估系统仍将可用。 任何人都可以通过注册网页中描述的程序为系统注册帐户。 <a href="http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2019/registration/index.html">32</a></p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesynfi2vsj30qi0vmq91.jpg" alt="image-20200515111158361" /></p>

<h2 id="43-多模态任务附加自动评分">4.3 多模态任务附加自动评分</h2>

<p>​		对于多模态任务，在WAT评估服务器之外运行了几个额外的自动度量，即:BLEU（现在由Moses scorer 33计算），characTER（Wang et al.，2016），chrF3(Popovic，2015），TER（Snover et al.，2006），WER，PER和CDER（Leusch et al.，2006）。 除了chrF3和characTER，我们在评分前对候选人和参考运行摩西标记器34。 对于所有错误度量，即较好分数较低的度量，我们取1-x来反转分数，并在度量名称前加“n”来表示这一点。 通过这种修改，更高的分数总是表示更好的翻译结果。 此外，为了更好的可读性，我们将所有度量值乘以100。</p>

<h1 id="5human-evaluation">5Human Evaluation</h1>

<p>​		在WAT2019中，我们进行了三种人类评估:针对纯文本语言对的成对评估（5.1节）和初级专业人员适足性评估（5.2节），以及针对多模态任务的成对直接评估（5.3节）。</p>

<h2 id="51pairwise-evaluation">5.1Pairwise Evaluation</h2>

<p>​		我们对提交供人评估的参与者系统进行了成对评估。 <strong>由一家专业翻译公司对提交的译文进行评估，并通过与基线译文进行比较</strong>（见第3节），对提交的译文进行成对评分。</p>

<h3 id="511-sentence-selection-and-evaluation">5.1.1 Sentence Selection and Evaluation</h3>

<p>​		对于成对评价，我们从<strong>每个任务</strong>的<strong>测试集</strong>中<strong>随机抽取了400个句子</strong>。 我们在连续的子任务中使用了与去年相同的句子。 基线和提交的翻译以随机顺序与输入的源句子一起显示给注释者。 注释者被要求判断哪一种翻译更好，或者它们是否不相上下。</p>

<h3 id="512-voting">5.1.2 Voting</h3>

<p>​		为保证评价的质量，每句话由5名不同的批注员进行评价，最后决定取决于5个评语。 我们将每个判断j i(i=1，···，5）定义为:</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geswyypj9zj30cm02wq37.jpg" alt="image-20200515101351165" /></p>

<p>最终决策D定义如下，使用<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geswz3v419j303a00wglh.jpg" alt="image-20200515101359471" /></p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geswza7h8nj308803aq32.jpg" alt="image-20200515101410272" /></p>

<h3 id="513-pairwise-score-calculation">5.1.3 Pairwise Score Calculation</h3>

<p>​		假设W是相对于基线的胜数，L是输数，T是平局数。 成对得分可通过以下公式计算:</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geswzmhbsij309w022glm.jpg" alt="image-20200515101429468" /></p>

<p>​		从定义来看，成对得分的范围在-100到100之间。</p>

<h3 id="514-confidence-interval-estimation">5.1.4 Confidence Interval Estimation</h3>

<p>​		置信区间有几种估计方法。 我们选择使用自举重采样（Koehn，2004）来估计95%的置信区间。 程序如下:</p>

<p>​		1.从400个人类评价句子中随机抽取300个句子，计算所选句子的成对得分</p>

<p>​		2.重复上一步1000次，得到1000个成对得分</p>

<p>​		3.对1000个分数进行排序，并通过丢弃前25个分数和后25个分数来估计95%的置信区间</p>

<h2 id="52-jpo-adequacy-evaluation">5.2 JPO Adequacy Evaluation</h2>

<p>​		我们对每个子任务的前两名或前三名参与者的成对评估系统进行初专干事适足性评估35。 <strong>评价是由翻译专家根据初专干事充分性评价标准进行的，该标准最初由初专干事界定，用于评估翻译专利文件的质量The evaluation was carried out by translation experts based on the JPO adequacy evaluation criterion, which is originally defined by JPO to assess the quality of translated patent documents.。</strong></p>

<h3 id="521-sentence-selection-and-evaluation">5.2.1 Sentence Selection and Evaluation</h3>

<p>​		对于JPO adequacy evaluation，200个测试句子是从用于两两评价的400个测试句子中随机抽取的。 对于每个测试句子，将输入源句子，被试系统翻译和参考翻译展示给注释者。 为了保证评价的质量，每个句子都由两名批注员进行评价。 注意，除了WAT2019上的新子任务之外，所选的句子与之前的工作坊中使用的句子相同。</p>

<h3 id="522-evaluation-criterion">5.2.2 Evaluation Criterion</h3>

<p>​		表14显示初专干事适足标准从5到1。 评价是主观进行的。 “重要信息”表示技术因素及其相互关系。 还考虑各要素的重要程度进行评价。 各等级的百分比大致反映了源句意义的传递程度。 初级专业人员文件（日文）描述了详细的标准<a href="http://www.jpo.go.jp/shiryou/toushin/chousa/ tokkyohonyaku_hyouka.htm">36</a>。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geszg8kmpyj30e608cmyh.jpg" alt="image-20200515113939355" /></p>

<h2 id="53-多模态任务人工评估">5.3 多模态任务人工评估</h2>

<p>​		对多模态任务三个轨道的评估遵循直接评估（DA，Graham et al.，2016）技术，通过要求注释者给每个候选人分配从0到100的分数。 分数是使用滑块分配的，没有数字反馈，因此刻度是有效连续的。 经过一定数量的评分项目，每个注释者的预测稳定下来。</p>

<p>​		所收集的DA分数可以直接对每个系统和轨道进行平均（表示为“AVE”），或者首先对每个注释器进行标准化，然后进行平均（“AVE Z”）。 标准化消除了在分配的分数范围内个体差异的影响:分数被缩放，使得每个注释者的平均分数为0，标准差为1。</p>

<p>​		我们的评价不同于基本的DA在以下几个方面:（1）我们以双语方式进行评价，即我们要求注释者充分理解源英语以便能够评估印地语翻译的充分性；（2）我们要求注释者一次给两个不同的片段打分，而原始DA一次只显示一个候选片段。</p>

<p>​		双语评估的主要好处是评估时不需要参考资料。 取而代之的是，参考可以包含在其他候选项中，人工评估允许我们直接比较机器翻译和人工翻译的性能。</p>

<p>​		实验增加了双重判断（一次给两个考生打分）。 这样做的好处是节省了注释者的一些时间（他们不需要阅读源码或再次检查图片），也可以通过直接的成对比较来评估候选对象。 在WMT的历史上（Bojar等人，2016年），5路相对排名被使用了很多年。 如果有5个候选人，可能无法很精确地比较个人对。 对于单候选DA，成对比较不能作为系统排序的基础。 我们认为，一个屏幕上有两个候选人可能是一种很好的平衡。</p>

<p>​		为了充分的统计可靠性，判断应该是相互独立的。 在我们的双重评分中并不是这样，即使我们明确地要求人们对候选人进行相互独立的评分。 然而，即使在最初的方法中也不能保证完全的独立性，因为注释者会记住他们过去的判断。 今年，WMT甚至通过按自然顺序对给定文档中的所有片段进行评分，使DA具有可供注释者使用的文档上下文。 因此，在解释DA分数时，我们敢于假装判断的独立性。</p>

<p>​		图3，图4和图5显示了每个音轨注释的用户界面。</p>

<p>​		在“纯文本”评估中，一篇英文文本（来源）和两篇印地语译文（候选1和2）被展示给注释者。 在“多模态”评价中，注释者同时被显示图像和源英语文本。 第一个问题是验证源英语文本是否是所指示区域的良好标题。 对于两个翻译候选人，要求注释者独立指出意义保留到什么程度。 “印地语字幕”评价只显示图像和两个印地语候选。 请注释人员注意，这两个标题应独立处理，每一个标题都可以考虑该区域的一个非常不同的方面。</p>

<h1 id="6-参与者">6 参与者</h1>

<p>​		表15显示了WAT2019的参与者。 该表列出了来自不同国家的25个组织，包括日本，印度，缅甸，美国，韩国，中国，法国和瑞士。</p>

<p>​		提交了25个团队的约400个翻译成果进行自动评估，提交了8个团队的约30个翻译成果进行成对评估。 我们选取了约50个翻译结果进行JPO adequacy评估。 表16显示了每个小组在截止日期前提交结果的任务。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geszi346v7j30re0iidko.jpg" alt="image-20200515114126266" /></p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesziol0p6j30vc0ostcm.jpg" alt="image-20200515114200359" /></p>

<h1 id="7-评价结果">7 评价结果</h1>

<p>​		在这一节中，从几个角度报告了针对WAT2019的评估结果。 自动和人工评估的一些结果也可以在WAT2019网站上获得。 37</p>

<h2 id="71-official-evaluation-results">7.1 Official Evaluation Results</h2>

<p>​		图6，7，8和9显示了ASPEC子任务的官方评估结果，图10，11，12，13，14和15显示了JPC子任务的官方评估结果，图16和17显示了JIJI子任务的官方评估结果，图18和19显示了NCPD子任务的官方评估结果，图20和21显示了IITB子任务的官方评估结果，图22，23，24和25显示了ALT子任务的官方评估结果，图26和27显示了TDDC子任务的官方评估结果，图28和29显示了UFAL子任务的官方评估结果。 每个figure都包含JPO适足性评价结果和top系统的评价摘要。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geszkxi09wj30um0pgn24.jpg" alt="image-20200515114409327" /></p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geszle5cqmj30tu1227bj.jpg" alt="image-20200515114436451" /></p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1geszlnnog1j30to0bmq59.jpg" alt="image-20200515114452447" /></p>

<p>​		详细的自动评估结果见附录A.所选提交材料的详细JPO适足性评估结果见表17和18。 加权k（Cohen，1968）的权重定义为评估1-评估2/4。</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesznnkhddj30ne0q6wly.jpg" alt="image-20200515114647116" /></p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesznwtbxbj30n20rg10s.jpg" alt="image-20200515114702073" /></p>

<p>​		表20提供了多模态任务的自动得分以及WAT评估服务器BLEU得分。 对于每个测试集（E-Test和C-Test），在所有轨道（纯文本，字幕或多模态翻译）上的分数是可比较的，因为潜在的参考翻译集是相同的。 然而，字幕任务的得分将非常低，因为独立于英文源字幕生成的字幕很可能与参考译文不同。</p>

<p>​		对于多模式任务，表19显示了所有有效系统提交的手工评估分数。 如上所述，我们使用了reference转换，就好像它是竞争系统之一，请参见表中的“reference”行。 注释是完全匿名的，所以注释者没有机会知道他们是在给人类翻译打分还是给MT输出打分。</p>

<h2 id="72-statistical-significance-testing-of-pairwise-evaluation-between-submissions">7.2 Statistical Significance Testing of Pairwise Evaluation between Submissions</h2>

<p>​		表21显示了aspec-ja-en子任务的统计显著性测试结果，表22显示了JIJI子任务的统计显著性测试结果，表23显示了TDDC子任务的统计显著性测试结果。 在显著性水平上，行中系统优于列中系统，分别为p&lt;0.01，0.05和0.1。 测试也通过自举重采样完成，如下所示:</p>

<p>​		1.从400个成对评价句子中随机抽取300个句子，计算两个系统在所选句子上的成对得分</p>

<p>​		2.重复上一步1000次，统计胜(W)，负(L)和平局(T)的次数</p>

<p>​		3.计算<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gesx5pnvljj303001adfp.jpg" alt="image-20200515102020790" /></p>

<p>Inter-annotator Agreement</p>

<p>​		为了评估工作人员之间的协议的可靠性，我们计算了Fleiss的k值（Fleiss et al.，1971）。 结果如表24所示。 我们可以看到，x j平移的k值比j x平移的k值大。 这可能是因为这些语言对的工作人员大多数是日语，对自己母语的评价要比一般的其他语言容易得多。 印地语的k值相对较高。 这可能是因为印地语的整体翻译质量较低，评估人员很容易区分出较好的译文和较差的译文。</p>

<h1 id="8-findings">8 Findings</h1>

<p>​		在本节中，我们将展示一些翻译任务的发现。</p>

<h2 id="81-tddc">8.1 TDDC</h2>

<p>​		在自动评价和人工评价的结果中，每一个系统都正确地翻译了大部分句子。 根据人类对‘项目’和‘文本’子任务的评价，所有评价者将所有对中超过70%的人评为4或5。 这些高评级对大多由来自及时披露文件的典型术语和句子组成。 本任务侧重于数字的准确翻译，因此评价标准确认了在包含数字的典型句子中不存在误译，如货币单位和日期。</p>

<p>​		然而，在及时披露文件中使用的不常见句子往往会被误译。 例如，不常见的专有名词容易被省略或误译为其他意义的词；结构复杂，不常见的句子，一般是长句，在从句的依存关系上也会出现错误。</p>

<p>​		此外，一些系统将没有主语的句子翻译成主语不正确的句子。 日语句子经常省略主语和宾语，而英语中通常会包含主语和宾语。 例如，一句日语“27,000.”（公司的普通股，最多限制在27,000股），翻译成“（不相关的公司名称）最多27,000股的普通股”。</p>

<p>​		而且，还有一些不正确的修饰语或限定词。 在日文及时披露文件中，有许多日期前缀，如“（this）”“（this）”“（next）”“（last）”等。 有些系统翻译的句子中包含这些单词的年份不正确。 例如，一个包含“3”（本财年第三季度末）的日语句子被翻译成了“2016财年第三季度末”。</p>

<p>​		归纳起来，造成这些误译的原因有以下几点:</p>

<p>​		•系统很难翻译TDDC不包含的长句和专有名词。</p>

<p>​		•有些源句由于缺少主语和（或）宾语而不清楚，因此不适合英译。</p>

<p>​		•TDDC包含语义不平衡的对，系统可能会受到源对句子中任何一个的强烈影响。</p>

<p>​		另一方面，有些译文似乎适合于TDDC中随意省略冗余表达的句子，但评价者给它们打了低分，可能是因为它们不是直译。 这一结果暗示有必要创建另一个评估标准，该标准评估正确地向投资者传递信息的正确性。</p>

<h2 id="82-englishtamil-task">8.2 English↔Tamil Task</h2>

<p>​		我们观察到，大多数参与者使用迁移学习技术，如微调和混合微调来翻译泰米尔语英语，从而获得了相当高质量的翻译。 然而，英语泰米尔语翻译仍然很差，其主要原因是缺乏辅助平行语料库。 我们期望利用大型域内单语语料库进行反译能有助于缓解这一问题。 我们将为明年的任务提供这样的语料库。</p>

<h2 id="83news-commentary-task">8.3News Commentary Task</h2>

<p>​		我们只收到3份俄文和日文翻译的意见书，所有意见书都利用了Imankulova等人提出的多语种和多步骤微调。 （2019）的研究结果表明，对于具有非常小的领域内平行语料库的语言对，谨慎地选择语料库和稳健的训练可以显著地提高NMT的质量。 在明年的工作中，我们预期会有更多的意见书，参加者会利用更多更大的单语和双语语料库。</p>

<h2 id="84-multi-modal-task">8.4 Multi-Modal Task</h2>

<h2 id="841validation-of-source-english-captions">8.4.1Validation of Source English Captions</h2>

<p>​		在多模态轨道的人工评估中，我们的注释员同时看到图片和源文本（以及两个评分的候选）。 我们借此机会对原始HVG数据的质量进行了双重检查。 在给候选对象打分之前，我们要求注释人员确认源英文文本对于图像的指定区域来说是一个很好的标题。</p>

<p>​		表25中的结果表明，对于数量惊人的项目，我们没有收到任何答案。 这证实了即使是非匿名注释器也可以很容易地提供草率的评估。 有可能，这些遗漏的一部分可以归因于我们的注释界面，它在一个页面上显示所有项目，并且依赖于滚动。 下一次，我们将在每一页上只显示一个注释项，同时也考虑突出显示未回答的问题。 严格要求答案并不总是合适的，但是我们需要确保注释者意识到他们正在跳过一个问题。</p>

<p>​		幸运的是，错误的源字幕并不是经常发生的情况，占评估实例的1%或2%。</p>

<h3 id="842-relation-to-human-translation">8.4.2 Relation to Human Translation</h3>

<p>​		多模态任务的双语评估方式允许我们评估参考译文，就好像它们是另一个相互竞争的机器翻译系统。 因此，表19也列出了“参考资料”。</p>

<p>​		在赛道和测试集上（EV vs.CH），人类超过了MT候选人。 唯一的例外是IDIAP run 2956在E-Test的textonly翻译中获胜，但这在C-Test(CH)中没有得到确认。 匿名化系统683在多模态轨迹(MM)中E-Test上的得分也几乎达到了人类的表现。 这些都不是第一个MT表现与人类一样的案例，我们很高兴看到这一点，当目标是一种印度语言。</p>

<h3 id="843evaluating-captioning">8.4.3Evaluating Captioning</h3>

<p>​		虽然自动得分在不同任务之间是可比较的，但必须单独考虑仅印地语字幕（“Hi”）。 如果没有源句，人类和机器都很有可能想出变化很大的文字说明。 同一个形象可以用许多不同的方面来描述。 我们所有的自动度量标准通常都是基于相同的字符序列，单词或n-图的存在来比较候选字幕和参考字幕。 偏离参考的考生，无论实际素质如何，都将得到低分。</p>

<p>​		与其他子任务（“纯文本”和“多模态”翻译）相比，“印地语字幕”的自动评估分数非常非常低，如表20所示。 由于标题生成错误，甚至连人类注释者也无法为“印地语标题”条目提交的大部分片段给出任何分数。</p>

<h1 id="9-conclusion-and-future-perspective">9 Conclusion and Future Perspective</h1>

<p>​		本文总结了WAT2019的共享任务。 我们在全球范围内有25个参与者，通过对提交的意见进行分析和找出问题，收集了大量对改进现有机器翻译系统有用的意见。</p>

<p>​		在下一个WAT研讨会上，我们计划使用带有上下文的新数据集对一些翻译子任务进行文档级评估，并考虑如何在WAT中实现上下文感知机器翻译。 此外，我们还计划对译文进行外部评估。</p>
:ET