I"n<h1 id="2019-mass-masked-sequence-to-sequence-pre-training-for-language-generation">2019 MASS/ Masked Sequence to Sequence Pre-training for Language Generation</h1>

<h1 id="摘要">摘要</h1>

<p>​		通过将知识从资源丰富的预训练任务转移到低/零资源的下游任务，BERT（Devlin et al.，2018）等预训练和调优在语言理解方面取得了巨大成功。 受BERT算法成功的启发，我们提出了一种用于基于编解码器的语言生成的掩码序列到序列预训练(MASS)方法。 MASS采用编码器-解码器框架，在给定句子剩余部分的情况下重构一个句子片段:其编码器以一个带有随机掩蔽片段（几个连续令牌）的句子作为输入，其解码器尝试预测这个掩蔽片段。 通过这种方式，MASS可以联合训练编码器和解码器，以发展表示抽取和语言建模的能力。 通过进一步调整各种零/低资源语言生成任务，包括神经机器翻译，文本摘要和会话响应生成（3个任务，共8个数据集），MASS无需预训练或使用其他预训练方法，即可实现比基线更显著的改进。 特别是，我们在无监督的英法翻译中达到了最先进的准确率（BLEU评分为37.5），甚至超过了早期基于注意的监督模型（Bahdanau等人，2015b)1。</p>

<h1 id="1导言">1.导言</h1>

<p>​		当目标任务的训练数据资源较少或为零时，预训练和优化被广泛使用，而预训练有大量的数据（Girshick et al.，2014；Szegedy et al.，2015；Ouyang et al.，2015；Dai&amp;Le，2015；Howard&amp;Ruder，2018；Radford et al.，2018；Devlin et al.，2018）。 例如，在计算机视觉中，模型通常在大规模ImageNet数据集上进行预训练，然后在下游任务上进行调整，如目标检测（Szegedy et al.，2015；Ouyang et al.，2015）或图像分割（Girshick et al.，2014）。 最近，诸如ELMo（Peters et al.，2018），OpenAI GPT（Radford et al.，2018）和BERT（Devlin et al.，2018）等预训练方法在自然语言处理领域引起了广泛关注，并在情感分类（Socher et al.，2013），自然语言推理（Bowman et al.，2015），命名实体识别（Tjong Kim Sang&amp;De Meulder，2003）和团队问答（Rajpurkar et al.，2016）等多种语言理解任务中实现了最先进的精确度，这些任务通常具有有限的监督数据。 在上述的预训练方法中，BERT是最突出的一种，它通过掩蔽语言建模和下一句预测在大型单语语料库上对双向编码器表示进行预训练。</p>

<p>​		与语言理解不同的是，语言生成的目的是生成基于某些输入的自然语言句子，包括神经机器翻译(NMT)（Cho et al.，2014；Bahdanau et al.，2015a；Vaswani et al.，2017），文本摘要（Ayana et al.，2016；Suzuki&amp;Nagata，2017；Gehring et al.，2017）和会话反应生成（Shang et al.，2015；Vinyals&amp;Le，2015）。 语言生成任务通常是数据饥渴型的，其中许多任务在训练数据方面是低资源甚至零源的。 在这些自然语言生成任务上直接应用类似于BERT的预训练方法是不可行的，因为BERT是为语言理解而设计的，而语言理解通常仅由一个编码器或解码器来处理。 因此，如何设计语言生成任务（通常采用基于编解码器的序列到序列学习框架）的预训练方法具有很大的潜力和重要性。</p>

<p>​		在本文中，受BERT的启发，我们提出了一种新的预训练目标:用于语言生成的掩码序列到序列学习（MAsked Sequence to Sequence learning，MASS）。 MASS基于序列到序列的学习框架:它的编码器以一个带有屏蔽片段（几个连续的令牌）的句子作为输入，它的解码器以编码器表示为条件预测这个屏蔽片段。 与BERT或只对编码器或解码器进行预训练的语言模型不同，MASS经过精心设计，分两步对编码器和解码器进行联合预训练:1）通过预测在编码器端被掩蔽的句子片段，MASS可以迫使编码器理解未掩蔽的令牌的含义，以便在解码器端预测被掩蔽的令牌； 2）MASS通过掩蔽源侧未掩蔽的解码器输入令牌，可以迫使解码器更多地依赖源表示而不是目标侧先前的令牌进行下一个令牌预测，更好地促进编解码器之间的联合训练。</p>

<p>​		MASS只需预先训练一个模型，然后对各种下游任务进行调整。 我们使用transformer作为基本序列，在WMT单语语料库2上对模型进行排序和预训练，然后对三种不同的语言生成任务进行调整，包括NMT，文本摘要和会话响应生成。 考虑到下游的任务覆盖了像NMT这样的跨语言任务，我们对一个模型进行了多语言的预训练。 我们研究了所有三个任务的低资源设置，并且考虑了纯零资源设置的无监督NMT。 对于NMT，实验是在WMT14英语-法语，WMT16英语-德语和WMT16英语-罗马尼亚语数据集上进行的。 对于无监督NMT，我们直接在单语数据上调整预训练模型，并带有反向翻译损失（Lample et al.，2018），而不是像Lample et al.那样使用额外的去噪自动编码器损失。 （2018年）。 对于低资源的NMT，我们根据有限的双语数据调整我们的模型。 对于另外两个任务，我们分别在Gigaword语料库和Cornell Movie Dialog语料库上进行了实验，实验结果表明，我们的方法在这些任务上以及在零资源和低资源环境下都得到了改进，证明了我们的方法是有效的，适用于广泛的序列生成任务。</p>

<p>​		本文的主要贡献如下:1）提出了一种用于语言生成的掩码序列到序列预训练方法MASS（masked sequence to sequence pre-training）； 2）我们将MASS应用于各种语言生成任务，包括NMT，文本摘要和会话响应生成，并实现了显著的改进，证明了我们提出的方法的有效性。特别是，我们在两种语言对:英语-法语和英语-德语上实现了最先进的无监督NMT BLEU评分，并且在BLEU评分方面比以前的无监督NMT方法（Lample&amp;Conneau，2019）在英语-法语上超过4分，在法语-英语上超过1分，甚至超过了早期的基于注意的监督模型（Bahdanau等人，2015b)。</p>

<h1 id="2相关工作">2.相关工作</h1>

<p>在自然语言处理的序列到序列学习和预训练方面已经有了大量的工作。 在本节中，我们简要回顾了几种常用的方法。</p>

<h2 id="21-序列到序列学习">2.1. 序列到序列学习</h2>

<p>​		序列到序列学习（Cho等，2014； Bahdanau等人，2015a； Wu等人，2016年； Gehring等人，2017年； Vaswani et al.，2017）是人工智能领域的一项具有挑战性的任务，涵盖了多种语言生成应用，如NMT（Cho et al.，2014； Bahdanau等人，2015a； Wu等人，2016年； Gehring等人，2017年； Vaswani等人，2017年； Tan等人，2019年； Artetxe等人，2017年； Lample等人，2017年； 2018年； He等人，2018年； 哈桑等人，2018年； 宋等人，2018年； Shen et al.，2018），文本摘要（Ayana et al.，2016； 铃木&amp;永田，2017年； Gehring等，2017），问答（Yuan等，2017； Fedus et al.，2018）和会话反应生成（Shang et al.，2015； Vinyals&amp;Le，2015年）。</p>

<p>​		由于深度学习的推进，序列到序列学习近年来备受关注。 然而，许多语言生成任务（如NMT）缺乏成对数据，却有大量的非成对数据。 因此，针对未配对数据的预训练和针对小规模配对数据的优化将有助于完成这些任务，而这正是本工作的重点。</p>

<h2 id="22-nlp任务的预训练">2.2. NLP任务的预训练</h2>

<p>​		预训练已经广泛应用于NLP任务中，以学习更好的语言表征。 以前的工作主要集中在自然语言理解任务上，可以分为基于特征的方法和优化方法。 基于特征的方法主要利用前训练为下游任务提供语言表征和特征，包括词级表征（Brown et al.，1992；Ando&amp;Zhang，2005；Blitzer et al.，2006；Collobert&amp;Weston，2008；Mikolov et al.，2013；Pennington et al.，2014）和句子级表征（Kiros et al.，2015；Logeswaran&amp;Lee，2018；Le&amp;Mikolov，2014），以及来自NMT模型的语境敏感特征（McCann et al.，2017）和ELMo（Peters et al.，2018）。 微调方法主要是在语言建模目标上预先训练模型，然后利用监督数据在下游任务上对模型进行微调（Dai&amp;Le，2015；Howard&amp;Ruder，2018；Radford et al.，2018；Devlin et al.，2018）。 特别是Devlin等人。 （2018）提出了基于掩蔽语言建模和下一句预测的BERT，并在GLUE基准（Wang et al.，2018）和SQuAD（Rajpurkar et al.，2016）中对多种语言理解任务实现了最先进的精确度。</p>

<p>​		也有一些工作对编码-译码器模型进行语言生成的预训练。 戴&amp;乐（2015）； Ramachandran等人。 （2016）利用语言模型或自动编码器对编码器和解码器进行预训练。 虽然可以观察到它们的改进，但它们的改进是有限的，而且不如语言理解的预训练方法（如BERT）那样普遍和显著。 Zhang&amp;Zong(2016)为预训练设计了一个句子重排序任务，但只针对编码器-解码器模型中的编码器部分。 Zoph等人。 （2016年）； Firat等人。 （2016）在相似的资源丰富的语言对上预训练模型，并在目标语言对上进行优化，目标语言对依赖于其他语言对上的监督数据。 最近，XLM（Lample&amp;Conneau，2019）为编码器和解码器预训练了类Bert模型，并在无监督机器翻译方面取得了以前的最新成果。 然而，XLM中的编码器和解码器是单独预训练的，编码器-解码器注意机制不能预训练，这对于基于序列到序列的语言生成任务来说是次优的。</p>

<p>​		与以往的工作不同，我们提出的MASS是精心设计的，仅使用未标记数据联合预训练编码器和解码器，可以应用于大多数语言生成任务。</p>

<h1 id="3质量">3.质量</h1>

<p>​			在本节中，我们首先介绍序列到序列学习的基本框架，然后提出MASS（掩码序列到序列预训练）。 然后，我们讨论了MASS和以前的预训练方法的区别，包括BERT中的掩码语言建模和标准语言建模。</p>

<p>##3.1. 序列到序列学习</p>

<p>​		我们将（x，y)∈(x，y）表示为一个句子对，其中x=(x1，x2，。。。，x m）是具有m个标记的源句子，y=(y1，y2，。。。，y n）是具有n个标记的目标句子，x和y是源域和目标域。 序列到序列模型学习参数来估计条件概率P（yx；？），通常使用对数似然作为目标函数:L（？；（X，Y))=∑(X，Y)∈(X，Y)log P（yx；？）。 条件概率P（yx；？）可以进一步按链规则分解:P（yx；？）=t=1P(yt y&lt;t，x；？），其中y&lt;t是位置t之前的继续标记。
​		序列到序列学习的一个主要方法是编码器-解码器框架:编码器读取源序列并生成一组表示； 解码器在给定源表示及其前面的令牌的情况下估计每个目标令牌的条件概率。 在编码器和解码器之间进一步引入了注意力机制（Bahdanau等人，2015a)，以确定在预测当前令牌时关注哪个源表示。</p>

<h2 id="32-掩蔽序列到序列预训练">3.2. 掩蔽序列到序列预训练</h2>

<p>​		在这一节中，我们介绍了一个新的无监督预测任务。 给定一个未配对的源句子x∈x，我们将x\u:v表示为x的修改版本，其中从位置u到v的片段被屏蔽，0&lt;u&lt;v&lt;m，m是句子x的标记数。 我们将k=v-u+1表示为从位置u到v被掩蔽的标记的数目。我们将每个被掩蔽的标记替换为一个特殊符号[M]，并且掩蔽句子的长度不变。 x，u:V表示x从u到V的句子片段。</p>

<p>​		MASS以掩蔽序列x\u:v为输入，通过预测句子片段xu:v来预训练序列到序列模型。 我们还使用对数似然作为目标函数:</p>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmgpj8qc9j30gc04gmxj.jpg" alt="image-20200609233807323" /></p>

<p>​		我们在图1中展示了一个示例，其中输入序列有8个标记，碎片x3x4x5x6被屏蔽。 注意，模型仅预测屏蔽片段x 3 x 4 x 5 x 6，给定x 3 x 4 x 5作为位置4−6的解码器输入，解码器将特殊屏蔽符号[M]作为其他位置（例如位置1−3和7−8）的输入。 虽然我们的方法适用于任何基于神经网络的编解码器框架，但我们在实验中选择Transformer，因为它在多序列到序列的学习任务中达到了最先进的性能。</p>

<p>​		实际上，BERT（Devlin et al.，2018）中的掩码语言建模和GPT（Radford et al.，2018）中的标准语言建模（Bengio et al.，2003；Mikolov et al.，2010）都可以看作是海量的特例。 我们有一个重要的超参数k，它表示句子被屏蔽片段的长度。 我们采用不同k值的方法可以覆盖与以往预训练方法相关的特例，如表1所示。</p>

<p>​		当k=1时，源语句中被屏蔽的片段只包含一个令牌，解码器预测此令牌时没有任何令牌作为输入，但以未被屏蔽的源令牌为条件，如图2a所示。 它成为BERT中使用的掩码语言建模。 人们可能会认为模型结构与掩码语言模型有一点不同。 然而，由于解码器的所有输入令牌都被屏蔽，因此解码器本身就像一个非线性分类器，类似于BERT中使用的softmax矩阵。 在这种情况下，条件概率为P(xux\u；？），u为掩蔽令牌的位置，这正是BERT3中所使用的掩蔽语言建模的公式。</p>

<h2 id="33-讨论">3.3. 讨论</h2>

<p>​		MASS是一种语言生成的预训练方法。 虽然它的特殊情况与以前的方法（包括GPT中的标准语言建模和BERT中的掩码语言建模）有关，但它与这些方法在总体上是不同的。</p>

<p>​		•标准语言建模早已用于预训练，最突出的是最近提出的ELMo（Peters et al.，2018）和OpenAI GPT（Radford et al.，2018）。 BERT为自然语言理解引入了两个预训练任务（掩蔽语言建模和下一句预测），并使用一个编码器为单个句子或一对句子提取表示。 标准语言建模和BERT都可以单独对编码器或解码器进行预训练。 虽然在语言理解任务上取得了有希望的结果，但它们并不适合于语言生成任务，后者通常利用编码器-解码器框架来生成条件序列。</p>

<p>​		•MASS旨在联合预训练编码器和解码器以完成语言生成任务。 首先，通过序列到序列的框架只预测被掩蔽的令牌，MASS迫使编码器理解未被掩蔽的令牌的含义，同时也鼓励解码器从编码器侧提取有用的信息。 第二，通过在解码器端预测连续令牌，解码器可以构建比仅仅预测离散令牌更好的语言建模能力。 第三，通过进一步掩蔽编码器侧未掩蔽的解码器输入令牌（例如，在预测片段x3x4x5x6时，仅将令牌x3x4x5作为输入，用[M]掩蔽其他令牌），鼓励解码器从编码器侧提取更多有用信息，而不是利用先前令牌中的丰富信息。</p>

<h1 id="4实验和结果">4.实验和结果</h1>

<p>​		在本节中，我们将详细介绍各种语言生成任务（包括NMT，文本摘要，会话响应生成）的大规模预训练和优化实验细节。</p>

<h2 id="41-mass预训练">4.1. MASS预训练</h2>

<p>​		模型配置我们选择Transformer（Vaswani et al.，2017）作为基本模型结构，它由6层编码器和6层解码器组成，嵌入/隐藏大小为1024，前馈滤波器大小为4096。 对于神经机器翻译任务，我们在源语和目标语的单语数据上预训练我们的模型。 我们分别在英语-法语，英语-德语，英语-罗马尼亚语三种语言对上进行实验。 对于其他语言生成任务，包括文本摘要和会话响应生成，我们分别只使用英语单语数据对模型进行预训练。 在神经机器翻译任务中，为了区分源语言和目标语言，我们在输入句子的每个标记上添加了一个语言嵌入，用于编码和译码器，该语言嵌入也是端到端学习的。 我们基于XLM4的代码库实现了我们的方法。</p>

<p>​		数据集我们使用了WMT新闻爬行数据集5中的所有单语数据，该数据集涵盖了2007年至2017年的英语，法语和德语分别为1.9亿，6200万和2.7亿个句子。 我们还在预训练阶段包括一种低资源语言，罗马尼亚语，以验证用低资源单语数据进行大规模预训练的有效性。 我们使用新闻抓取数据集中所有可用的罗马尼亚语句子，并用WMT16数据对其进行扩充，得到290万个句子。 我们删除长度超过175的句子。 对于每个任务，我们在源语言和目标语言之间联合学习一个具有字节对编码（Sennrich et al.，2016）的60,000个子词单元。</p>

<p>​		预训练细节我们通过用特殊符号[M]替换连续的令牌来掩盖片段，起始位置为随机的u。 继Devlin等人。 （2018），编码器中的屏蔽令牌在80%的时间中将是一个[M]令牌，10%的时间将是一个随机令牌，10%的时间将是一个不变的令牌。 我们将片段长度k设置为句子中标记总数的大约50%，并且还研究了不同的k来比较它们的准确性变化。 为了降低内存和计算成本，我们删除了解码器中的填充（屏蔽令牌），但保持未屏蔽令牌的位置嵌入不变（例如，如果前两个令牌被屏蔽并删除，则第三个令牌的位置仍为2，而不是0）。 这样，我们就可以得到类似的精度，并在解码器中减少50%的计算量。 我们使用学习率为10−4的Adam优化器（Kingma&amp;Ba，2015）进行预训练。 模型在8个NVIDIA V100 GPU卡上进行训练，每个小批包含3000个用于预训练的令牌。</p>

<p>​		为了验证MASS的有效性，我们在三个语言生成任务上调整了预先训练的模型:NMT，文本摘要和会话响应生成。 我们在这些任务上探索低资源设置，仅利用少量训练数据进行优化，以模拟低资源场景。 对于NMT，我们主要考察零资源（无监督）设置，因为近年来无监督NMT已经成为一项具有挑战性的任务（Artetxe et al.，2017；Lample et al.，2017；2018）。</p>

<h2 id="42-nmt上的微调">4.2. NMT上的微调</h2>

<p>​		在本节中，我们首先描述在无监督NMT上的实验，然后介绍在低资源NMT上的实验。</p>

<p>​		实验设置对于无监督的NMT，没有双语数据来调整预先训练的模型。 因此，我们利用在预训练阶段也使用的单语数据。 与Artetxe等人不同。 （2017年）； Lample等人。 （2017年；2018年）； Leng等人。 （2019），我们只是使用反翻译生成伪双语数据进行训练，而没有使用去噪自动编码器6。 在优化期间，我们使用Adam优化器(Kingma&amp;Ba，2015），初始学习率为10−4，批处理大小为每个GPU 2000个令牌。 在评估期间，我们使用Multi-Bleu.pl 7计算newstest2014英语-法语，newstest2016英语-德语和英语-罗马尼亚语的BLEU得分。</p>

<p>​		在无监督NMT上的结果我们的结果如表2所示。 在所有6个翻译方向上，我们的方法都优于之前的所有结果，包括没有预训练的方法（Lample et al.，2018）和有预训练的方法（Lample&amp;Conneau，2019）。 XLM（Lample&amp;Conneau，2019）是以前在编码器和解码器中利用BERT类预训练的最先进的方法，它包括几种预训练方法:掩码语言模型(MLM)和因果语言模型(CLM)。 在EN-FR上，我们的方法仍然比XLM高4.1个BLEU点。</p>

<p>​		与其他预训练方法相比，我们还将MASS与以往的语言生成任务预训练方法进行了比较。 第一个基线是BERT+LM，它使用BERT中的屏蔽语言建模来预训练编码器，使用标准语言建模来预训练解码器。 第二个基线是DAE，它简单地使用去噪自动编码器（Vincent et al.，2008）来预训练编码器和解码器。 我们使用BERT+LM和DAE对模型进行预训练，并使用XLM的相同调优策略对无监督翻译对进行调优（即DAE丢失+反向翻译）。 这些方法也适用于6层变压器设置。
​		如表3所示，BERT+LM获得比DAE更高的BLEU分数，并且MASS在所有无监督翻译对上的表现都优于BERT+LM和DAE。 虽然DAE通常利用一些去噪方法，如随机掩蔽令牌或交换相邻令牌，但解码器仍然可以通过编码器-解码器注意8轻松地学习复制未掩蔽的令牌。 另一方面，DAE中的解码器以完整的句子作为输入，足以像语言模型一样预测下一个令牌，而不被强制从编码器中提取额外的有用表示。</p>

<p>​		在低资源NMT环境下，我们分别从WMT14英-法，WMT16英-德和WMT16英-罗马尼亚的双语训练数据中抽取10K，100K，1M的成对句子，考察我们的方法在不同低资源场景下的性能。 我们使用在预训练阶段学习的相同的BPE码来表征训练句子对。 我们使用Adam优化器对成对数据进行20,000步的预训练模型调优，学习速率设置为10−4。 根据开发集上的精确度选择最佳模型。 我们报告在无监督设置中使用的相同测试集上的BLEU分数。 如图3所示，MASS的性能优于仅在双语数据上训练而没有在所有六个翻译方向上进行任何预训练的基线模型，证明了我们方法在低资源场景下的有效性。</p>

<h2 id="43-文本摘要实验设置的微调">4.3. 文本摘要实验设置的微调</h2>

<p>​		文本摘要是为一个较长的文本文档创建一个简短流畅的摘要的任务，这是一个典型的序列生成任务。 我们利用来自Gigaword语料库（Graff et al.，2003）9的不同规模(10K，100K，1M和3.8M)的训练数据对文本摘要任务进行预训练模型调优，Gigaword语料库由总共380万个英文文章标题对组成。 我们将文章作为编码器输入，将标题作为解码器输入进行优化。 我们在评估期间报告ROUGE-1，ROUGE2和ROUGE-L在千兆字测试集上的F1得分。 我们使用波束大小为5的波束搜索进行推断。</p>

<p>​		结果我们的结果如图4所示。 我们将MASS与只在配对数据上训练而没有任何预训练的模型进行比较。 在不同规模的调优数据上，MASS的性能始终优于基线（10K数据上的ROUGE点增益超过10个，100K数据上的ROUGE点增益超过5个），这表明MASS在低资源场景下使用不同规模的训练数据进行此任务时是有效的。</p>

<p>​		与其他预训练方法相比，我们进一步将MASS与4.2节中描述的BERT+LM和DAE的预训练方法进行了比较，在文本摘要任务上的数据为3.8M。 如表4所示，MASS在三个ROUGE分数上始终如一地优于两种预训练方法。</p>

<h2 id="44-会话反应生成的微调">4.4. 会话反应生成的微调</h2>

<p>​		实验设置会话反应生成为会话产生灵活的反应（Shang et al.，2015；Vinyals&amp;Le，2015）。 我们在康奈尔电影对话语料库(DanescuNiculescu-Mizil&amp;Lee，2011）10上进行了实验，该语料库包含140k对会话。 我们随机抽取10k/20k对作为验证/测试集，剩余数据用于训练。 我们在预训练阶段采用相同的优化超参数进行优化。 继Vinyals&amp;Le（2015）之后，我们以困惑(PPL)的方式报告了这一结果。
结果我们将MASS与在可用数据对上训练的基线进行比较。 我们对10K对（随机选择）和整个110K对进行实验，结果如表5所示。 在10K和110K数据上，MASS获得了比基线更低的PPL。</p>

<p>​		与其他预训练方法相比，我们还比较了MASS与BERT+LM和DAE预训练方法在会话反应生成方面的差异。 如表5所示，MASS分别在10K和110K训练数据上一致地优于两种PPL较低的预训练方法。</p>

<h2 id="45-不同k的mass研究分析">4.5. 不同k的MASS研究分析</h2>

<p>​		掩蔽片段k的长度是MASS的一个重要超参数，我们在3.2节中改变了k，以涵盖BERT和标准语言建模中掩蔽语言建模的特殊情况。 在这一部分中，我们研究了不同k下的MASS性能，其中k取自句子长度m的10%-90%，步长为10%，加上k=1和k=m。</p>

<p>​		我们观察了MASS在预训练后的表现，以及在几个语言生成任务（包括无监督的英法翻译，文本摘要和会话反应生成）上进行调整后的表现。 我们首先展示了在不同K值的英语和法语语言上，预训练模型的困惑(PPL)。 我们选择WMT En-Fr的newstest2013中的英语和法语句子作为验证集，并在图5a（英语）和5b（法语）中绘制PPL。 可以看出，当k在句子长度m的50%-70%之间时，预训练的模型达到了最佳的验证PPL。 然后，我们观察优化任务的性能。 我们在图5c中显示了无监督En-Fr翻译的BLEU验证分数曲线，在图5d中显示了文本摘要的ROUGE验证分数曲线，在图5e中显示了会话响应生成的PPL验证分数曲线。 可以看出，当k接近句子长度m的50%时，MASS在这些下游任务上取得最佳性能。 因此，在我们的实验中，我们将k=m的50%作为质量。</p>

<p>​		实际上，k=m的50%是编码器和解码器之间的良好平衡。 编码器侧或解码器侧中的有效令牌太少将使模型偏向于更多地集中在另一侧，这不适合于典型地利用编码器-解码器框架来提取编码器中的句子表示以及在解码器中建模和生成句子的语言生成任务。 极端情况是k=1（BERT中的屏蔽语言建模）和k=m（标准语言建模），如图2所示。 无论是k=1还是k=m都不能在下游的语言生成任务上取得良好的性能，如图5所示。</p>

<p>​		在我们掩蔽序列对序列预训练的消融研究中，我们有两个精心的设计:（1）我们在编码器端掩蔽连续的令牌，并由此在解码器端预测连续的令牌，这可以建立比仅仅预测离散令牌更好的语言建模能力。 （2）我们对编码器端未屏蔽的解码器输入令牌进行屏蔽（如图1中预测片段x3x4x5x6时，只取x3x4x5的令牌作为输入，用[M]屏蔽其他令牌），鼓励解码器从编码器端提取更多有用的信息，而不是撬动前面令牌的丰富信息。 在这一节中，我们进行了两个烧蚀研究，在质量上验证了两种设计的有效性。 第一项研究是随机屏蔽离散的令牌，而不是大量连续的令牌，表示为离散的。 第二个研究是将所有令牌馈送到解码器，而不是掩蔽在编码器侧未被掩蔽的解码器的输入令牌，表示为馈送。 我们将MASS与两种消融方法在无监督的英法翻译上进行比较，如表6所示。 可以看出分立和馈电的表现都比质量差，展示了两种设计在质量上的有效性。</p>

<h1 id="5结论">5.结论</h1>

<p>​		在这项工作中，我们提出了用于语言生成任务的MASS:masked sequence to sequence预训练，它在编码器-解码器框架中给定句子的剩余部分重构一个句子片段。 MASS只需预先训练一个模型，然后在多个语言生成任务（如神经机器翻译，文本摘要和会话响应生成）上进行调整。 通过对上述三项任务和总共八个数据集的实验，MASS在没有预训练或使用其他预训练方法的情况下，比基线取得了显著的改善。 更具体地说，MASS在三种语言对的无监督NMT测试中获得了最先进的BLEU分数，在英语-法语测试中比之前的最先进的BLEU分数高出4分以上。</p>

<p>​		对于未来的工作，我们将把MASS应用于更多的语言生成任务如句子释义，文本风格迁移和帖子编辑，以及其他序列生成任务（Ren et al.，2019）。 我们还将对我们提出的掩蔽序列到序列的预训练方法进行更多的理论和实证分析。</p>
:ET