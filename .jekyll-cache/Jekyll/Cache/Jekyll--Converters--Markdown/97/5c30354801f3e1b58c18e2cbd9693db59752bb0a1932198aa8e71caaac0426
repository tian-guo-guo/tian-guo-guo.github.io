I"g·<h2 id="ç¥ç»æœºå™¨ç¿»è¯‘ç»¼è¿°çš„å‚è€ƒæ–‡çŒ®åŠæ‘˜è¦">ç¥ç»æœºå™¨ç¿»è¯‘ç»¼è¿°çš„å‚è€ƒæ–‡çŒ®åŠæ‘˜è¦</h2>

<h5 id="åŸæ–‡æäºšè¶…ç†Šå¾·æ„å¼ æ°‘ç¥ç»æœºå™¨ç¿»è¯‘ç»¼è¿°jè®¡ç®—æœºå­¦æŠ¥201841122734-2755">åŸæ–‡ï¼š<a href="http://kns.cnki.net//KXReader/Detail?autoLogin=0&amp;TIMESTAMP=637018294707186250&amp;DBCODE=CJFQ&amp;TABLEName=CJFDLAST2019&amp;FileName=JSJX201812007&amp;RESULT=1&amp;SIGN=EYaeHZNroGKGlc2ghTcKRyejyW4%3d">æäºšè¶…,ç†Šå¾·æ„,å¼ æ°‘.ç¥ç»æœºå™¨ç¿»è¯‘ç»¼è¿°[J].è®¡ç®—æœºå­¦æŠ¥,2018,41(12):2734-2755.</a></h5>

<h5 id="æœ¬æ–‡å‚è€ƒæ–‡çŒ®é“¾æ¥-httpspanbaiducoms1obvf6ncue3lr5vmrhqusna-æå–ç -8dfj-å¤åˆ¶è¿™æ®µå†…å®¹åæ‰“å¼€ç™¾åº¦ç½‘ç›˜æ‰‹æœºappæ“ä½œæ›´æ–¹ä¾¿å“¦">æœ¬æ–‡å‚è€ƒæ–‡çŒ®ï¼šé“¾æ¥: https://pan.baidu.com/s/1ObvF6NCue3Lr5VMrHqUsNA æå–ç : 8dfj å¤åˆ¶è¿™æ®µå†…å®¹åæ‰“å¼€ç™¾åº¦ç½‘ç›˜æ‰‹æœºAppï¼Œæ“ä½œæ›´æ–¹ä¾¿å“¦</h5>
<h5 id="1-jiao-li-cheng-yang-shu-yuan-liu-fang-et-alseventy-years-beyond-neural-networksretrospect-and-prospectchinese-journal-of-computers-2016-39-8-1697-1716-in-chinese-ç„¦ææˆ-æ¨æ·‘åª›-åˆ˜èŠ³ç­‰ç¥ç»ç½‘ç»œä¸ƒåå¹´å›é¡¾ä¸å±•æœ›è®¡ç®—æœºå­¦æŠ¥-2016-39-8-1697-1716">[1] Jiao Li-Cheng, Yang Shu-Yuan, Liu Fang, et al.Seventy years beyond neural networks:Retrospect and prospect.Chinese Journal of Computers, 2016, 39 (8) :1697-1716 (in Chinese) (ç„¦ææˆ, æ¨æ·‘åª›, åˆ˜èŠ³ç­‰.ç¥ç»ç½‘ç»œä¸ƒåå¹´:å›é¡¾ä¸å±•æœ›.è®¡ç®—æœºå­¦æŠ¥, 2016, 39 (8) :1697-1716)</h5>

<h5 id="2-hinton-g-e-osindero-s-teh-y-wa-fast-learning-algorithm-for-deep-belief-netsneural-computation-2006-181527-1554">[2] Hinton G E, Osindero S, Teh Y-W.A fast learning algorithm for deep belief nets.Neural Computation, 2006, 18:1527-1554</h5>
<p>We show how to use â€œcomplementary priorsâ€ to eliminate the explaining-away effects thatmake inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of thewake-sleep algorithm. After fine-tuning, a networkwith three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to displaywhat the associativememory has in mind.
æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨â€œäº’è¡¥å…ˆéªŒâ€æ¥æ¶ˆé™¤åœ¨å…·æœ‰è®¸å¤šéšè—å±‚çš„ç´§å¯†è¿æ¥çš„ä¿¡å¿µç½‘ä¸­ä½¿æ¨ç†å˜å¾—å›°éš¾çš„è§£é‡Š-ç¦»å¼€æ•ˆåº”ã€‚ åˆ©ç”¨äº’è¡¥å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå¿«é€Ÿçš„è´ªå©ªç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ä¸€æ¬¡å­¦ä¹ ä¸€å±‚æœ‰å‘æ·±å±‚ä¿¡å¿µç½‘ç»œï¼Œå‰ææ˜¯é¡¶å±‚å’Œé¡¶å±‚å½¢æˆä¸€ä¸ªæ— å‘å…³è”å­˜å‚¨å™¨ã€‚ å¿«é€Ÿè´ªå©ªç®—æ³•è¢«ç”¨æ¥åˆå§‹åŒ–ä¸€ä¸ªè¾ƒæ…¢çš„å­¦ä¹ è¿‡ç¨‹ï¼Œä½¿ç”¨å”¤é†’ç¡çœ ç®—æ³•çš„å¯¹æ¯”ç‰ˆæœ¬æ¥å¾®è°ƒæƒé‡ã€‚ ç»è¿‡å¾®è°ƒï¼Œä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªéšè—å±‚çš„ç½‘ç»œå½¢æˆäº†ä¸€ä¸ªå¾ˆå¥½çš„æ‰‹å†™æ•°å­—å›¾åƒåŠå…¶æ ‡ç­¾è”åˆåˆ†å¸ƒçš„ç”Ÿæˆæ¨¡å‹ã€‚ è¿™ç§ç”Ÿæˆæ¨¡å‹æ¯”æœ€å¥½çš„åˆ¤åˆ«å­¦ä¹ ç®—æ³•æä¾›äº†æ›´å¥½çš„æ•°å­—åˆ†ç±»ã€‚ æ•°å­—æ‰€åœ¨çš„ä½ç»´æµå½¢æ˜¯ç”±é¡¶å±‚è”æƒ³è®°å¿†çš„è‡ªç”±èƒ½æ™¯è§‚ä¸­çš„é•¿æ²Ÿå£‘å»ºæ¨¡çš„ï¼Œé€šè¿‡ä½¿ç”¨æœ‰å‘è¿æ¥æ¥æ˜¾ç¤ºè”æƒ³è®°å¿†æ‰€æƒ³åˆ°çš„ä¸œè¥¿ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°æ¢ç´¢è¿™äº›æ²Ÿå£‘ã€‚</p>

<h5 id="3-krizhevsky-a-sutskever-i-hinton-g-eimagenet-classification-with-deep-convolutional-neural-networksproceedings-of-the-neural-information-processing-systems-nips-2012-lake-tahoe-usa-20121097-1105">[3] Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks//Proceedings of the Neural Information Processing Systems (NIPS 2012) .Lake Tahoe, USA, 2012:1097-1105</h5>
<p>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called â€œdropoutâ€ that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.
æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤§çš„ã€æ·±å·ç§¯çš„ç¥ç»ç½‘ç»œï¼Œå°†ImageNet LSVRC-2010ç«èµ›ä¸­çš„120ä¸‡å¹…é«˜åˆ†è¾¨ç‡å›¾åƒåˆ†ä¸º1000ä¸ªä¸åŒçš„ç±»åˆ«ã€‚ åœ¨æµ‹è¯•æ•°æ®ä¸Šï¼Œæˆ‘ä»¬åˆ†åˆ«è·å¾—äº†37.5%å’Œ17.0%çš„Top-1å’ŒTop-5é”™è¯¯ç‡ï¼Œè¿™æ¯”å…ˆå‰çš„æŠ€æœ¯æ°´å¹³è¦å¥½å¾—å¤šã€‚ è¯¥ç¥ç»ç½‘ç»œå…·æœ‰6000ä¸‡ä¸ªå‚æ•°å’Œ65ä¸‡ä¸ªç¥ç»å…ƒï¼Œç”±5ä¸ªå·ç§¯å±‚å’Œ3ä¸ªå®Œå…¨è¿æ¥çš„å±‚ç»„æˆï¼Œå…¶ä¸­5ä¸ªå·ç§¯å±‚ä¹‹åæ˜¯æœ€å¤§æ± å±‚ï¼Œæœ€åæ˜¯1000è·¯æœ€å¤§è½¯è¿æ¥å±‚ã€‚ ä¸ºäº†åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†éé¥±å’Œç¥ç»å…ƒå’Œä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„GPUå®ç°å·ç§¯è¿ç®—ã€‚ ä¸ºäº†å‡å°‘å®Œå…¨è¿æ¥å±‚ä¸­çš„è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€è¿‘å¼€å‘çš„ç§°ä¸ºâ€œè¾å­¦â€çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¢«è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚ æˆ‘ä»¬è¿˜åœ¨ILSVRC-2012ç«èµ›ä¸­è¾“å…¥äº†è¯¥æ¨¡å‹çš„ä¸€ä¸ªå˜ä½“ï¼Œå¹¶è·å¾—äº†15.3%çš„å‰5åæµ‹è¯•é”™è¯¯ç‡ï¼Œè€Œç¬¬äºŒåçš„æµ‹è¯•é”™è¯¯ç‡ä¸º26.2%ã€‚</p>

<h5 id="4-hinton-g-deng-l-yu-ddeep-neural-networks-for-acoustic-modeling-in-speech-recognitionthe-shared-views-of-four-research-groupsieee-signal-processing-magazine-2012-29-6-82-97">[4] Hinton G, Deng L, Yu D.Deep neural networks for acoustic modeling in speech recognition:The shared views of four research groups.IEEE Signal Processing Magazine, 2012, 29 (6) :82-97</h5>
<p>Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.
ç›®å‰å¤§å¤šæ•°è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä½¿ç”¨éšé©¬å°”å¯å¤«æ¨¡å‹(HMMS)æ¥å¤„ç†è¯­éŸ³çš„æ—¶é—´å˜å¼‚æ€§ï¼Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹(GMMS)æ¥ç¡®å®šæ¯ä¸ªHMMçš„æ¯ä¸ªçŠ¶æ€ä¸è¡¨ç¤ºå£°å­¦è¾“å…¥çš„ç³»æ•°å¸§æˆ–ç³»æ•°å¸§çš„çŸ­çª—å£çš„æ‹Ÿåˆç¨‹åº¦ã€‚ è¯„ä¼°æ‹Ÿåˆåº¦çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å‰é¦ˆç¥ç»ç½‘ç»œï¼Œè¯¥å‰é¦ˆç¥ç»ç½‘ç»œä»¥å¤šä¸ªç³»æ•°å¸§ä¸ºè¾“å…¥ï¼Œå¹¶åœ¨HMMçŠ¶æ€ä¸Šäº§ç”ŸåéªŒæ¦‚ç‡ä½œä¸ºè¾“å‡ºã€‚ æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDeep Neural Networksï¼ŒDNNsï¼‰å…·æœ‰è®¸å¤šéšè—å±‚ï¼Œå¹¶ä¸”ä½¿ç”¨æ–°çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œåœ¨å„ç§è¯­éŸ³è¯†åˆ«åŸºå‡†ä¸Šè¡¨ç°å‡ºä¼˜äºGMMçš„æ€§èƒ½ï¼Œæœ‰æ—¶è¡¨ç°å‡ºå¾ˆå¤§çš„ä¼˜åŠ¿ã€‚ æœ¬æ–‡æ¦‚è¿°äº†è¿™ä¸€è¿›å±•ï¼Œå¹¶ä»£è¡¨äº†å››ä¸ªç ”ç©¶å°ç»„çš„å…±åŒè§‚ç‚¹ï¼Œè¿™å››ä¸ªç ”ç©¶å°ç»„æœ€è¿‘æˆåŠŸåœ°å°†DNNç”¨äºè¯­éŸ³è¯†åˆ«ä¸­çš„å£°å­¦å»ºæ¨¡ã€‚</p>

<h5 id="5-collobert-r-weston-j-bottou-l-et-alnatural-language-processing-almost-from-scratchjournal-of-machine-learning-research-2011-12-1-2493-2537">[5] Collobert R, Weston J, Bottou L, et al.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 2011, 12 (1) :2493-2537</h5>
<p>We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.
æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ç¥ç»ç½‘ç»œç»“æ„å’Œå­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€åˆ†å—ã€å‘½åå®ä½“è¯†åˆ«å’Œè¯­ä¹‰è§’è‰²æ ‡æ³¨ã€‚ è¿™ç§å¤šåŠŸèƒ½æ€§æ˜¯é€šè¿‡è¯•å›¾é¿å…ç‰¹å®šäºä»»åŠ¡çš„å·¥ç¨‹æ¥å®ç°çš„ï¼Œå› æ­¤å¿½ç•¥äº†å¤§é‡çš„å…ˆéªŒçŸ¥è¯†ã€‚ æˆ‘ä»¬çš„ç³»ç»Ÿä¸æ˜¯ä¸ºæ¯ä¸ªä»»åŠ¡ä»”ç»†ä¼˜åŒ–äººå·¥è¾“å…¥ç‰¹æ€§ï¼Œè€Œæ˜¯åŸºäºå¤§é‡çš„å¤§éƒ¨åˆ†æœªæ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ å†…éƒ¨è¡¨ç¤ºã€‚ è¿™ä¸€å·¥ä½œéšåè¢«ç”¨ä½œæ„å»ºä¸€ä¸ªå…·æœ‰è‰¯å¥½æ€§èƒ½å’Œæœ€å°è®¡ç®—è¦æ±‚çš„å…è´¹å¯ç”¨æ ‡è®°ç³»ç»Ÿçš„åŸºç¡€ã€‚</p>

<h5 id="6-junczys-dowmunt-m-dwojak-t-hoang-his-neural-machine-translation-ready-for-deploymenta-case-study-on30translation-directionsarxiv-preprint161001108v2-2016">[6] Junczys-Dowmunt M, Dwojak T, Hoang H.Is neural machine translation ready for deployment?A case study on30translation directions.arXiv preprint/1610.01108v2, 2016</h5>
<p>In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efï¬cient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-persecond ratios.
æœ¬æ–‡å¯¹åŸºäºçŸ­è¯­çš„SMTå’Œç¥ç»æœºå™¨ç¿»è¯‘åœ¨30ä¸ªç¿»è¯‘æ–¹å‘ä¸Šçš„ç¿»è¯‘è´¨é‡è¿›è¡Œäº†æœ€å¤§çš„æ¯”è¾ƒã€‚ å¯¹äºåä¸ªæ–¹å‘ï¼Œæˆ‘ä»¬è¿˜åŒ…æ‹¬å±‚æ¬¡çŸ­è¯­ä¸ºåŸºç¡€çš„å¤šç›®æ ‡å†³ç­–ã€‚ å¯¹æœ€è¿‘å‘å¸ƒçš„è”åˆå›½å¹¶è¡Œè¯­æ–™åº“V1.0åŠå…¶å¤§å‹å…­å‘å¥å­å¯¹é½å­è¯­æ–™åº“è¿›è¡Œäº†å®éªŒã€‚ åœ¨è®ºæ–‡çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç¿»è¯‘é€Ÿåº¦çš„ä¸€äº›æ–¹é¢ï¼Œä»‹ç»äº†æˆ‘ä»¬çš„é«˜æ•ˆç¥ç»æœºå™¨ç¿»è¯‘è¯‘ç å™¨AMUNMTã€‚ æˆ‘ä»¬è¯æ˜ï¼Œå½“å‰çš„ç¥ç»æœºå™¨ç¿»è¯‘åœ¨æ¯”è¾ƒè¯/ç§’æ¯”æ—¶å·²ç»å¯ä»¥ç”¨äºç”Ÿäº§ç³»ç»Ÿã€‚</p>

<h5 id="7-sennrich-r-haddow-b-birch-aedinburgh-neural-machine-translation-systems-for-wmt-16proceedings-of-the-1st-conference-on-machine-translationberlin-germany-2016371-376">[7] Sennrich R, Haddow B, Birch A.Edinburgh neural machine translation systems for WMT 16//Proceedings of the 1st Conference on Machine Translation.Berlin, Germany, 2016:371-376</h5>
<p>We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: Englishâ†”Czech, Englishâ†”German, Englishâ†”Romanian and Englishâ†”Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a ï¬xed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3â€“11.2 B LEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated. 12
æˆ‘ä»¬å‚ä¸äº†2016å¹´WMTå…±äº«æ–°é—»ç¿»è¯‘ä»»åŠ¡ï¼Œä¸ºå››ç§è¯­è¨€å¯¹æ„å»ºäº†ç¥ç»ç¿»è¯‘ç³»ç»Ÿï¼Œæ¯ç§è¯­è¨€å¯¹éƒ½æ¥å—äº†åŒå‘åŸ¹è®­:è‹±è¯­å’Œæ·å…‹è¯­ã€è‹±è¯­å’Œå¾·è¯­ã€è‹±è¯­å’Œç½—é©¬å°¼äºšè¯­ä»¥åŠè‹±è¯­å’Œä¿„è¯­ã€‚ æˆ‘ä»¬çš„ç³»ç»ŸåŸºäºä¸€ä¸ªæ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨ï¼Œä½¿ç”¨BPEå­å­—åˆ†æ®µè¿›è¡Œå¼€æ”¾å¼è¯æ±‡ç¿»è¯‘ï¼Œå¹¶æä¾›å›ºå®šè¯æ±‡ã€‚ æˆ‘ä»¬å°è¯•ä½¿ç”¨å•è¯­æ–°é—»è¯­æ–™åº“çš„è‡ªåŠ¨å›è¯‘ä½œä¸ºé¢å¤–çš„è®­ç»ƒæ•°æ®ã€æ™®éè¾å­¦å’Œç›®æ ‡åŒå‘æ¨¡å‹ã€‚ æ‰€æœ‰æŠ¥å‘Šçš„æ–¹æ³•éƒ½æœ‰å¾ˆå¤§çš„æ”¹è¿›ï¼Œæˆ‘ä»¬çœ‹åˆ°åœ¨åŸºçº¿ç³»ç»Ÿä¸Šæœ‰4.3-11.2Bä½æµ“é“€çš„æ”¹è¿›ã€‚ åœ¨äººç±»çš„è¯„ä»·ä¸­ï¼Œæˆ‘ä»¬å‚ä¸çš„8ä¸ªç¿»è¯‘æ–¹å‘ä¸­æœ‰7ä¸ªæ–¹å‘ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ˜¯ï¼ˆç»‘å®šçš„ï¼‰æœ€ä½³çº¦æŸç³»ç»Ÿã€‚ 12</p>

<h5 id="8-zhou-jie-cao-ying-wang-xuguang-et-aldeep-recurrent-models-with-fast-forward-connections-for-neural-machine-translationtransactions-of-the-association-for-computational-linguistics-2016-4371-383">[8] Zhou Jie, Cao Ying, Wang Xuguang, et al.Deep recurrent models with fast-forward connections for neural machine translation.Transactions of the Association for Computational Linguistics, 2016, 4:371-383</h5>
<p>Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMTâ€™14 Englishto-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the ï¬rst time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difï¬cult WMTâ€™14 English-to-German task.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰æ˜¯åˆ©ç”¨ç¥ç»ç½‘ç»œè§£å†³æœºå™¨ç¿»è¯‘ï¼ˆMachine Translationï¼ŒMTï¼‰é—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼Œè¿‘å¹´æ¥å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚ ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°NMTæ¨¡å‹éƒ½æ¯”è¾ƒè‚¤æµ…ï¼Œå•ä¸€çš„NMTæ¨¡å‹ä¸ä¼ ç»Ÿçš„æœ€ä¼˜MTç³»ç»Ÿç›¸æ¯”ä»æœ‰ä¸€å®šçš„æ€§èƒ½å·®è·ã€‚ æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦é•¿çŸ­æ—¶è®°å¿†(LSTM)ç½‘ç»œçš„æ–°å‹çº¿æ€§è¿æ¥ï¼Œç§°ä¸ºå¿«è¿›è¿æ¥ï¼Œä»¥åŠä¸€ç§äº¤é”™å¼åŒå‘LSTMå±‚å †å ç»“æ„ã€‚ å¿«é€Ÿè½¬å‘è¿æ¥åœ¨ä¼ æ’­æ¢¯åº¦å’Œæ„å»ºæ·±åº¦ä¸º16çš„æ·±åº¦æ‹“æ‰‘ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ åœ¨WMTâ€™14è‹±æ³•ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å•ä¸€æ³¨æ„æ¨¡å‹è·å¾—äº†BLEU=37.7ï¼Œæ¯”ç›¸åº”çš„å•ä¸€æµ…å±‚æ³¨æ„æ¨¡å‹é«˜å‡º6.2ä¸ªBLEUç‚¹ã€‚ è¿™æ˜¯å•ä¸ªNMTæ¨¡å‹é¦–æ¬¡å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”æœ€ä½³ä¼ ç»Ÿæ¨¡å‹é«˜å‡º0.7ä¸ªBLEUç‚¹ã€‚ å³ä½¿ä¸ä½¿ç”¨æ³¨æ„æœºåˆ¶ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥å®ç°BLEU=36.3ã€‚ ç»è¿‡å¯¹æœªçŸ¥å•è¯çš„ç‰¹æ®Šå¤„ç†å’Œæ¨¡å‹çš„é›†æˆï¼Œæˆ‘ä»¬è·å¾—äº†è¿„ä»Šä¸ºæ­¢åœ¨è¯¥ä»»åŠ¡ä¸­æŠ¥å‘Šçš„æœ€å¥½çš„åˆ†æ•°ï¼ŒBLEU=40.4ã€‚ æˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿåœ¨æ›´å›°éš¾çš„WMTâ€™14è‹±è¯­å¯¹å¾·è¯­ä»»åŠ¡ä¸­å¾—åˆ°éªŒè¯ã€‚</p>

<h5 id="9-wu-yonghui-schuster-m-chen-zhifeng-et-algoogles-neural-machine-translation-systembridging-the-gap-between-human-and-machine-translationarxiv-preprint160908144v1-2016">[9] Wu Yonghui, Schuster M, Chen Zhifeng, et al.Googleâ€™s neural machine translation system:Bridging the gap between human and machine translation.arXiv preprint/1609.08144v1, 2016</h5>
<p>Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference â€“ sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMTâ€™s use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Googleâ€™s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the ï¬nal translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (â€œwordpiecesâ€) for both input and output. This method provides a good balance between the ï¬‚exibility of â€œcharacterâ€-delimited models and the eï¬ƒciency of â€œwordâ€-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores, we consider reï¬ning the models by using reinforcement learning, but we found that the improvement in the BLEU scores did not reï¬‚ect in the human evaluation. On the WMTâ€™14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Googleâ€™s phrase-based production system.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰æ˜¯ä¸€ç§å®ç°è‡ªåŠ¨ç¿»è¯‘çš„ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•ï¼Œå…·æœ‰å…‹æœä¼ ç»ŸçŸ­è¯­ç¿»è¯‘ç³»ç»Ÿè®¸å¤šç¼ºç‚¹çš„æ½œåŠ›ã€‚ ä¸å¹¸çš„æ˜¯ï¼ŒNMTç³»ç»Ÿåœ¨è®­ç»ƒå’Œç¿»è¯‘æ¨ç†æ–¹é¢éƒ½æ˜¯è®¡ç®—æ˜‚è´µçš„â€”â€”æœ‰æ—¶åœ¨éå¸¸å¤§çš„æ•°æ®é›†å’Œå¤§çš„æ¨¡å‹çš„æƒ…å†µä¸‹æ˜¯ä»¤äººæœ›è€Œå´æ­¥çš„ã€‚ å‡ ä½ä½œè€…è¿˜æŒ‡è´£NMTç³»ç»Ÿç¼ºä¹é²æ£’æ€§ï¼Œå°¤å…¶æ˜¯å½“è¾“å…¥å¥å­ä¸­åŒ…å«ç¨€æœ‰å•è¯æ—¶ã€‚ è¿™äº›é—®é¢˜é˜»ç¢äº†NMTåœ¨å®é™…éƒ¨ç½²å’ŒæœåŠ¡ä¸­çš„åº”ç”¨ï¼Œåœ¨å®é™…éƒ¨ç½²å’ŒæœåŠ¡ä¸­ï¼Œå‡†ç¡®æ€§å’Œé€Ÿåº¦éƒ½è‡³å…³é‡è¦ã€‚ åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Googleçš„ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»ŸGNMTï¼Œå®ƒè¯•å›¾è§£å†³å…¶ä¸­çš„è®¸å¤šé—®é¢˜ã€‚ æˆ‘ä»¬çš„æ¨¡å‹ç”±ä¸€ä¸ªæ·±åº¦LSTMç½‘ç»œç»„æˆï¼Œè¯¥ç½‘ç»œå…·æœ‰8ä¸ªç¼–ç å™¨å’Œ8ä¸ªè§£ç å™¨å±‚ï¼Œä½¿ç”¨å‰©ä½™è¿æ¥ä»¥åŠä»è§£ç å™¨ç½‘ç»œåˆ°ç¼–ç å™¨çš„æ³¨æ„è¿æ¥ã€‚ ä¸ºäº†æé«˜å¹¶è¡Œæ€§ï¼Œä»è€Œå‡å°‘è®­ç»ƒæ—¶é—´ï¼Œæˆ‘ä»¬çš„æ³¨æ„æœºåˆ¶å°†è§£ç å™¨çš„åº•å±‚è¿æ¥åˆ°ç¼–ç å™¨çš„é¡¶å±‚ã€‚ ä¸ºäº†åŠ å¿«æœ€ç»ˆçš„è½¬æ¢é€Ÿåº¦ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è®¡ç®—ä¸­ä½¿ç”¨äº†ä½ç²¾åº¦ç®—æ³•ã€‚ ä¸ºäº†æ”¹è¿›å¯¹ç¨€æœ‰å•è¯çš„å¤„ç†ï¼Œæˆ‘ä»¬å°†å•è¯åˆ†æˆä¸€ç»„æœ‰é™çš„å…¬å…±å­å•è¯å•å…ƒï¼ˆâ€œå•è¯â€ï¼‰ï¼Œç”¨äºè¾“å…¥å’Œè¾“å‡ºã€‚ è¯¥æ–¹æ³•å¾ˆå¥½åœ°å¹³è¡¡äº†â€œå­—ç¬¦â€åˆ†ç•Œæ¨¡å‹çš„çµæ´»æ€§å’Œâ€œå•è¯â€åˆ†ç•Œæ¨¡å‹çš„æ˜“ç”¨æ€§ï¼Œè‡ªç„¶åœ°å¤„ç†äº†ç¨€æœ‰å•è¯çš„ç¿»è¯‘ï¼Œæœ€ç»ˆæé«˜äº†ç³»ç»Ÿçš„æ•´ä½“å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„æ³¢æŸæœç´¢æŠ€æœ¯é‡‡ç”¨é•¿åº¦å½’ä¸€åŒ–è¿‡ç¨‹å¹¶ä½¿ç”¨è¦†ç›–æƒ©ç½šï¼Œè¿™é¼“åŠ±ç”Ÿæˆæœ€æœ‰å¯èƒ½è¦†ç›–æºå¥ä¸­æ‰€æœ‰å•è¯çš„è¾“å‡ºå¥ã€‚ ä¸ºäº†ç›´æ¥ä¼˜åŒ–ç¿»è¯‘BLEUåˆ†æ•°ï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥é‡æ–°å®šä¹‰æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å‘ç°BLEUåˆ†æ•°çš„æé«˜å¹¶æ²¡æœ‰åœ¨äººç±»è¯„ä¼°ä¸­é‡æ–°äº§ç”Ÿå½±å“ã€‚ åœ¨WMTâ€™14è‹±æ³•å’Œè‹±å¾·åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGNMTå–å¾—äº†æœ€å…ˆè¿›çš„ç«äº‰ç»“æœã€‚ é€šè¿‡å¯¹ä¸€ç»„å­¤ç«‹çš„ç®€å•å¥å­è¿›è¡Œäººå·¥å¹¶æ’è¯„ä¼°ï¼Œä¸è°·æ­ŒåŸºäºçŸ­è¯­çš„ç”Ÿäº§ç³»ç»Ÿç›¸æ¯”ï¼Œå®ƒå¹³å‡å‡å°‘äº†60%çš„ç¿»è¯‘é”™è¯¯ã€‚</p>

<h5 id="10-crego-j-kim-j-klein-g-et-alsystrans-pure-neural-machine-translation-systemsarxiv-preprint161005540v1-2016">[10] Crego J, Kim J, Klein G, et al.SYSTRANâ€™s pure neural machine translation systems.arXiv preprint/1610.05540v1, 2016</h5>
<p>Since the ï¬rst online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of NMT engines to replace their existing technologies. NMT systems have a large number of training conï¬gurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to productionready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efï¬cient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our ï¬rst ï¬ndings and we ï¬nally outline further work.
Our ultimate goal is to share our expertise to build competitive production systems for â€genericâ€ translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case speciï¬c engines integrated in real production workï¬‚ows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems.
è‡ªä»LISAï¼ˆBahdanauç­‰äººï¼Œ2014å¹´ï¼‰é¦–æ¬¡åœ¨çº¿æ¼”ç¤ºç¥ç»æœºå™¨ç¿»è¯‘(NMT)ä»¥æ¥ï¼ŒNMTçš„å¼€å‘æœ€è¿‘ä»å®éªŒå®¤è½¬å‘ç”Ÿäº§ç³»ç»Ÿï¼Œå‡ ä¸ªå®ä½“å®£å¸ƒæ¨å‡ºNMTå¼•æ“ä»¥å–ä»£å…¶ç°æœ‰æŠ€æœ¯å°±è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚ NMTç³»ç»Ÿæœ‰å¤§é‡çš„åŸ¹è®­é…ç½®ï¼Œæ­¤ç±»ç³»ç»Ÿçš„åŸ¹è®­è¿‡ç¨‹é€šå¸¸å¾ˆé•¿ï¼Œé€šå¸¸ä¸ºå‡ å‘¨ï¼Œå› æ­¤è¯•éªŒçš„ä½œç”¨è‡³å…³é‡è¦ï¼Œéœ€è¦å…±äº«ã€‚ åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å‘å¸ƒæ¶µç›–å¤šç§è¯­è¨€ï¼ˆ12ç§è¯­è¨€ï¼Œ32ç§è¯­è¨€å¯¹ï¼‰çš„åœ¨çº¿æ¼”ç¤ºç¨‹åºçš„åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ProductionReadyç³»ç»Ÿã€‚ æˆ‘ä»¬æ¢è®¨äº†ä¸åŒçš„å®è·µé€‰æ‹©:ä¸€ä¸ªæœ‰æ•ˆçš„ã€è¿›åŒ–çš„å¼€æºæ¡†æ¶ï¼› æ•°æ®å‡†å¤‡ï¼› ç½‘ç»œä½“ç³»ç»“æ„ï¼› å…¶ä»–å·²å®ç°çš„åŠŸèƒ½ï¼› ä¸ºç”Ÿäº§è€Œè°ƒæ•´ï¼› ç­‰ã€‚æˆ‘ä»¬è®¨è®ºäº†è¯„ä¼°æ–¹æ³•ï¼Œä»‹ç»äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä»½æ–‡ä»¶ï¼Œå¹¶æœ€ç»ˆæ¦‚è¿°äº†è¿›ä¸€æ­¥çš„å·¥ä½œã€‚ 
æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯åˆ†äº«æˆ‘ä»¬çš„ä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºâ€œé€šç”¨â€ç¿»è¯‘å»ºç«‹æœ‰ç«äº‰åŠ›çš„ç”Ÿäº§ç³»ç»Ÿã€‚ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¸®åŠ©å»ºç«‹ä¸€ä¸ªåä½œæ¡†æ¶ï¼Œä»¥åŠ å¿«æŠ€æœ¯çš„é‡‡ç”¨ï¼Œä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å·¥ä½œï¼Œå¹¶ä½¿é›†æˆåœ¨å®é™…ç”Ÿäº§å·¥ä½œæµç¨‹ä¸­çš„ç”¨ä¾‹è§„èŒƒå¼•æ“èƒ½å¤Ÿäº¤ä»˜ç»™ä¸šç•Œ/ç”±ä¸šç•Œé‡‡ç”¨ã€‚ æŒæ¡è¯¥æŠ€æœ¯å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºé€‚åˆç‰¹å®šéœ€è¦çš„ç¿»è¯‘å¼•æ“ï¼Œå…¶æ€§èƒ½ä¼˜äºå½“å‰æœ€ç®€å•/ç»Ÿä¸€çš„ç³»ç»Ÿã€‚</p>

<h5 id="11-neco-r-p-forcada-m-lasynchronous-translations-with-recurrent-neural-netsproceedings-of-the-international-conference-on-neural-networkshouston-usa-19972535-2540">[11] Neco R P, Forcada M L.Asynchronous translations with recurrent neural nets//Proceedings of the International Conference on Neural Networks.Houston, USA, 1997:2535-2540</h5>
<p>In recent years, many researchers have explored the relation between discrete-time recurrent neural networks(DTRNN) and finite-state machines (FSMs) either by showing their computational equivalence or by trainingthem to perform as finite-state recognizers from examples. Most of this work has focussed on the simplest class of deterministic state machines, that is deterministic finite automata and Mealy (or Moore) machines. The class of translations these machines can perform is very limited, mainly because these machines output symbols at the same rate as they input symbols, and therefore, the input and the translation have the same length; one may call these translations synchronous. Real-life translations are more complex: word reorderings, deletions, and insertions are common in natural-language translations; or, in speech-to-phoneme conversion, the number of frames corresponding to each phoneme is different and depends on the particular speaker or word. There are, however, simple deterministic, finite-state machines ( extensions of Mealy machines) that may perform these classes of asynchronous or time- warped translations. A simple DTRNN model with input and output control lines inspired on this class of machines is presented and successfully applied to simple asynchronous translation tasks with interesting results regarding generalization. Training of these nets from input-output pairs is complicated by the fact that the time alignment between the target output sequence and the input sequence is unknown and has to be learned: we propose a new error function to tackle this problem. This approach to the induction of asynchronous translators is discussed in connection with other approaches.
è¿‘å¹´æ¥ï¼Œè®¸å¤šç ”ç©¶äººå‘˜é€šè¿‡è¯æ˜ç¦»æ•£æ—¶é—´é€’å½’ç¥ç»ç½‘ç»œ(DTRNN)å’Œæœ‰é™çŠ¶æ€æœºçš„è®¡ç®—ç­‰ä»·æ€§æˆ–é€šè¿‡å®ä¾‹è®­ç»ƒå®ƒä»¬ä½œä¸ºæœ‰é™çŠ¶æ€è¯†åˆ«å™¨æ¥ç ”ç©¶å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚ è¿™æ–¹é¢çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æœ€ç®€å•çš„ä¸€ç±»ç¡®å®šæ€§çŠ¶æ€æœºä¸Šï¼Œå³ç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœºå’ŒMealyï¼ˆæˆ–Mooreï¼‰æœºã€‚ è¿™äº›æœºå™¨èƒ½å¤Ÿæ‰§è¡Œçš„ç¿»è¯‘ç§ç±»éå¸¸æœ‰é™ï¼Œä¸»è¦æ˜¯å› ä¸ºè¿™äº›æœºå™¨ä»¥ä¸å®ƒä»¬è¾“å…¥ç¬¦å·ç›¸åŒçš„é€Ÿç‡è¾“å‡ºç¬¦å·ï¼Œå› æ­¤ï¼Œè¾“å…¥å’Œç¿»è¯‘å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼› å¯ä»¥å°†è¿™äº›è½¬æ¢ç§°ä¸ºåŒæ­¥çš„ã€‚ ç°å®ç”Ÿæ´»ä¸­çš„ç¿»è¯‘æ¯”è¾ƒå¤æ‚:è¯çš„é‡æ–°æ’åºã€åˆ é™¤å’Œæ’å…¥åœ¨è‡ªç„¶è¯­è¨€ç¿»è¯‘ä¸­å¾ˆå¸¸è§ï¼› æˆ–è€…ï¼Œåœ¨è¯­éŸ³åˆ°éŸ³ç´ çš„è½¬æ¢ä¸­ï¼Œå¯¹åº”äºæ¯ä¸ªéŸ³ç´ çš„å¸§çš„æ•°ç›®æ˜¯ä¸åŒçš„ï¼Œå¹¶ä¸”å–å†³äºç‰¹å®šçš„è¯´è¯è€…æˆ–å•è¯ã€‚ ç„¶è€Œï¼Œæœ‰ä¸€äº›ç®€å•çš„ç¡®å®šæ€§æœ‰é™çŠ¶æ€æœºï¼ˆMealyæœºçš„æ‰©å±•ï¼‰å¯ä»¥æ‰§è¡Œè¿™äº›ç±»å¼‚æ­¥æˆ–æ—¶é—´æ‰­æ›²çš„è½¬æ¢ã€‚ åœ¨è¿™ç±»æœºå™¨ä¸Šæå‡ºäº†ä¸€ç§å…·æœ‰è¾“å…¥è¾“å‡ºæ§åˆ¶çº¿çš„ç®€å•DTRNNæ¨¡å‹ï¼Œå¹¶æˆåŠŸåœ°åº”ç”¨äºç®€å•çš„å¼‚æ­¥ç¿»è¯‘ä»»åŠ¡ï¼Œå¾—åˆ°äº†æœ‰è¶£çš„æ¨å¹¿ç»“æœã€‚ ç”±äºç›®æ ‡è¾“å‡ºåºåˆ—å’Œè¾“å…¥åºåˆ—ä¹‹é—´çš„æ—¶é—´å¯¹é½æ˜¯æœªçŸ¥çš„ï¼Œéœ€è¦å­¦ä¹ ï¼Œå› æ­¤ä»è¾“å…¥-è¾“å‡ºå¯¹ä¸­è®­ç»ƒè¿™äº›ç½‘ç»œå˜å¾—å¤æ‚:æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯¯å·®å‡½æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ æœ¬æ–‡ç»“åˆå…¶å®ƒæ–¹æ³•è®¨è®ºäº†å¼‚æ­¥ç¿»è¯‘å™¨çš„å½’çº³æ³•ã€‚</p>

<h5 id="12-castano-m-a-casacuberta-fa-connectionist-approach-to-machine-translationproceedings-of-the-5th-european-conference-on-speech-communication-and-technologyrhodes-greece-19971-4">[12] Castano M A, Casacuberta F.A connectionist approach to machine translation//Proceedings of the 5th European Conference on Speech Communication and Technology.Rhodes, Greece, 1997:1-4</h5>
<p>Connectionist Models can be considered as an encouraging approach to Example-Based Machine Translation. However, the neural translators developed in the literature are quite complex and require great human effort to classify and prepare training data. This paper presents an effective and more simple text-to-text connectionist translator with which translations from the source to the target language can be directly, automatically and successfully approached. The neural system, which is based on an Elman Simple Recurrent Network, was trained to tackle a simple pseudo-natural Machine Translation task.
è¿æ¥ä¸»ä¹‰æ¨¡å‹æ˜¯ä¸€ç§ä»¤äººé¼“èˆçš„åŸºäºå®ä¾‹çš„æœºå™¨ç¿»è¯‘æ–¹æ³•ã€‚ ç„¶è€Œï¼Œæ–‡çŒ®ä¸­å¼€å‘çš„ç¥ç»ç¿»è¯‘å™¨ç›¸å½“å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„äººåŠ›æ¥åˆ†ç±»å’Œå‡†å¤‡è®­ç»ƒæ•°æ®ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ã€æ›´ç®€å•çš„æ–‡æœ¬åˆ°æ–‡æœ¬è¿æ¥ä¸»ä¹‰ç¿»è¯‘å™¨ï¼Œå®ƒå¯ä»¥ç›´æ¥ã€è‡ªåŠ¨å’ŒæˆåŠŸåœ°å¤„ç†æºè¯­åˆ°ç›®çš„è¯­çš„ç¿»è¯‘ã€‚ åŸºäºElmanç®€å•é€’å½’ç½‘ç»œçš„ç¥ç»ç³»ç»Ÿè¢«è®­ç»ƒæ¥å¤„ç†ç®€å•çš„ä¼ªè‡ªç„¶æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚</p>

<h5 id="13-zhang-jiajun-zong-chengqingdeep-neural-networks-in-machine-translationan-overviewintelligent-systems-ieee-2015-30-5-16-25">[13] Zhang Jiajun, Zong Chengqing.Deep neural networks in machine translation:An overview.Intelligent Systems IEEE, 2015, 30 (5) :16-25</h5>
<p>Due to the powerful capacity of feature learning and representation, deep neural networks (DNNs) have made big breakthroughs in speech recognition and image processing. Following recent success in signal variable processing, researchers want to figure out whether DNNs can achieve similar progress in symbol variable processing, such as natural language processing (NLP). As one of the more challenging NLP tasks, machine translation (MT) has become a testing ground for researchers who want to evaluate various kinds of DNNs.
MT aims to find for the source language sentence the most probable target language sentence that shares the most similar meaning. Essentially, MT is a sequence-to-sequence prediction task. This article gives a comprehensive overview of applications of DNNs in MT from two views: indirect application, which attempts to improve standard MT systems, and direct application, which adopts DNNs to design a purely neural MT model. We can elaborate further:
â€¢ Indirect application designs new features with DNNs in the framework of standard MT systems, which consist of multiple submodels (such as translation selection and language models). For example, DNNs can be leveraged to represent the source language contextâ€™s semantics and better predict translation candidates. 
â€¢ Direct application regards MT as a sequence-to-sequence prediction task and, without using any information from standard MT systems, designs two deep neural networksâ€”an encoder, which learns continuous representations of source language sentences, and a decoder, which generates the target language sentence with source sentence representation.
Letâ€™s start by examining DNNs themselves.
æ·±å±‚ç¥ç»ç½‘ç»œï¼ˆDeep Neural Networksï¼ŒDNNsï¼‰ä»¥å…¶å¼ºå¤§çš„ç‰¹å¾å­¦ä¹ å’Œè¡¨ç¤ºèƒ½åŠ›ï¼Œåœ¨è¯­éŸ³è¯†åˆ«å’Œå›¾åƒå¤„ç†é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ éšç€è¿‘å¹´æ¥åœ¨ä¿¡å·å˜é‡å¤„ç†æ–¹é¢çš„æˆåŠŸï¼Œç ”ç©¶äººå‘˜å¸Œæœ›å¼„æ¸…æ¥šDNNèƒ½å¦åœ¨ç¬¦å·å˜é‡å¤„ç†æ–¹é¢å–å¾—ç±»ä¼¼çš„è¿›å±•ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ã€‚ æœºå™¨ç¿»è¯‘ï¼ˆMachine Translationï¼ŒMTï¼‰ä½œä¸ºä¸€é¡¹æ›´å…·æŒ‘æˆ˜æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå·²ç»æˆä¸ºç ”ç©¶è€…è¯„ä¼°å„ç§DNNçš„ä¸€ä¸ªè¯•éªŒåœºã€‚ 
æœºå™¨ç¿»è¯‘çš„ç›®çš„æ˜¯ä¸ºæºè¯­è¨€å¥å­æ‰¾åˆ°æœ€å¯èƒ½çš„ç›®æ ‡è¯­è¨€å¥å­ï¼Œå…·æœ‰æœ€ç›¸ä¼¼çš„æ„ä¹‰ã€‚ æœ¬è´¨ä¸Šï¼ŒMTæ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„é¢„æµ‹ä»»åŠ¡ã€‚ æœ¬æ–‡ä»é—´æ¥åº”ç”¨å’Œç›´æ¥åº”ç”¨ä¸¤ä¸ªæ–¹é¢å¯¹DNNSåœ¨MTä¸­çš„åº”ç”¨è¿›è¡Œäº†ç»¼è¿°ï¼Œé—´æ¥åº”ç”¨æ˜¯å¯¹æ ‡å‡†MTç³»ç»Ÿçš„æ”¹è¿›ï¼Œç›´æ¥åº”ç”¨æ˜¯é‡‡ç”¨DNNSè®¾è®¡ä¸€ä¸ªçº¯ç¥ç»MTæ¨¡å‹ã€‚ æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥é˜è¿°: 
â€¢é—´æ¥åº”ç”¨ç¨‹åºåœ¨æ ‡å‡†MTç³»ç»Ÿçš„æ¡†æ¶å†…ä½¿ç”¨DNNè®¾è®¡æ–°åŠŸèƒ½ï¼Œè¯¥ç³»ç»Ÿç”±å¤šä¸ªå­æ¨¡å‹ï¼ˆå¦‚ç¿»è¯‘é€‰æ‹©å’Œè¯­è¨€æ¨¡å‹ï¼‰ç»„æˆã€‚ ä¾‹å¦‚ï¼ŒDNNå¯ä»¥ç”¨æ¥è¡¨ç¤ºæºè¯­è¨€ä¸Šä¸‹æ–‡çš„è¯­ä¹‰å¹¶æ›´å¥½åœ°é¢„æµ‹ç¿»è¯‘å€™é€‰ã€‚ 
â€¢ç›´æ¥åº”ç”¨ç¨‹åºå°†MTè§†ä¸ºåºåˆ—å¯¹åºåˆ—çš„é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨ä¸ä½¿ç”¨æ¥è‡ªæ ‡å‡†MTç³»ç»Ÿçš„ä»»ä½•ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œè®¾è®¡äº†ä¸¤ä¸ªæ·±å±‚ç¥ç»ç½‘ç»œâ€”â€”ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œç¼–ç å™¨å­¦ä¹ æºè¯­è¨€å¥å­çš„è¿ç»­è¡¨ç¤ºï¼Œè§£ç å™¨ç”Ÿæˆå…·æœ‰æºå¥å­è¡¨ç¤ºçš„ç›®æ ‡è¯­è¨€å¥å­ã€‚ 
è®©æˆ‘ä»¬ä»æ£€æŸ¥DNNæœ¬èº«å¼€å§‹ã€‚</p>

<h5 id="14-kalchbrenner-n-blunsom-precurrent-continuous-translation-modelsproceedings-of-the-acl-conference-on-empirical-methods-in-natural-language-processing-emnlp-2013-seattle-usa-20131700-1709">[14] Kalchbrenner N, Blunsom P.Recurrent continuous translation models//Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP 2013) .Seattle, USA, 2013:1700-1709</h5>
<p>We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show ï¬rst that our models obtain a perplexity with respect to gold translations that is &gt; 43% lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.
æˆ‘ä»¬å¼•å…¥äº†ä¸€ç±»æ¦‚ç‡è¿ç»­ç¿»è¯‘æ¨¡å‹ï¼Œç§°ä¸ºé€’å½’è¿ç»­ç¿»è¯‘æ¨¡å‹ï¼Œå®ƒå®Œå…¨åŸºäºå•è¯ã€çŸ­è¯­å’Œå¥å­çš„è¿ç»­è¡¨ç¤ºï¼Œè€Œä¸ä¾èµ–äºå¯¹é½æˆ–çŸ­è¯­ç¿»è¯‘å•å…ƒã€‚ è¿™äº›æ¨¡å‹æœ‰ç”Ÿæˆå’Œæ¡ä»¶æ–¹é¢ã€‚ è¯‘æ–‡çš„ç”Ÿæˆç”¨ç›®æ ‡é€’å½’è¯­è¨€æ¨¡å‹å»ºæ¨¡ï¼Œè€Œå¯¹æºå¥çš„åˆ¶çº¦ç”¨å·ç§¯å¥æ¨¡å‹å»ºæ¨¡ã€‚ é€šè¿‡å„ç§å®éªŒï¼Œæˆ‘ä»¬é¦–å…ˆè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å¯¹äºGoldç¿»è¯‘çš„å›°æƒ‘åº¦æ¯”æœ€å…ˆè¿›çš„åŸºäºå¯¹é½çš„ç¿»è¯‘æ¨¡å‹ä½äº†43%ä»¥ä¸Šã€‚ å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå°½ç®¡ç¼ºä¹å¯¹é½ï¼Œä½†å®ƒä»¬å¯¹æºå¥çš„è¯­åºã€å¥æ³•å’Œæ„ä¹‰éƒ½éå¸¸æ•æ„Ÿã€‚ æœ€åï¼Œæˆ‘ä»¬è¯æ˜ï¼Œå½“é‡æ–°è¯„åˆ†N-æœ€ä½³ç¿»è¯‘åˆ—è¡¨æ—¶ï¼Œå®ƒä»¬ä¸æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸åŒ¹é…ã€‚</p>

<h5 id="15-sutskever-i-vinyals-o-le-q-vsequence-to-sequence-learning-with-neural-networksproceedings-of-the-neural-information-processing-systems-nips-2014-montreal-canada-20143104-3112">[15] Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks//Proceedings of the Neural Information Processing Systems (NIPS 2014) .Montreal, Canada, 2014:3104-3112</h5>
<p>Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difï¬cult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a ï¬xed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTMâ€™s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difï¬culty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTMâ€™s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.
æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰æ˜¯ä¸€ç§åŠŸèƒ½å¼ºå¤§çš„æ¨¡å‹ï¼Œåœ¨ä¸åŒçš„å­¦ä¹ ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ å°½ç®¡DNNæ— è®ºä½•æ—¶åªè¦æœ‰å¤§çš„æ ‡è®°è®­ç»ƒé›†å¯ç”¨éƒ½èƒ½å¾ˆå¥½åœ°å·¥ä½œï¼Œä½†å®ƒä»¬ä¸èƒ½ç”¨äºå°†åºåˆ—æ˜ å°„åˆ°åºåˆ—ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„åºåˆ—å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹åºåˆ—ç»“æ„ä½œäº†æœ€å°çš„å‡è®¾ã€‚ æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¤šå±‚é•¿çŸ­æ—¶å­˜å‚¨å™¨(LSTM)å°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°å›ºå®šç»´åº¦çš„å‘é‡ï¼Œç„¶åä½¿ç”¨å¦ä¸€ä¸ªæ·±åº¦LSTMä»å‘é‡ä¸­è§£ç ç›®æ ‡åºåˆ—ã€‚ æˆ‘ä»¬çš„ä¸»è¦ç»“æœæ˜¯ï¼Œåœ¨WMT-14æ•°æ®é›†çš„è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼ŒLSTMç”Ÿæˆçš„ç¿»è¯‘åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šçš„BLEUåˆ†æ•°ä¸º34.8ï¼Œå…¶ä¸­LSTMçš„BLEUåˆ†æ•°æ˜¯å¯¹è¯æ±‡å¤–å•è¯çš„æƒ©ç½šã€‚ æ­¤å¤–ï¼ŒLSTMå¯¹é•¿å¥æ²¡æœ‰åŒºåˆ«ã€‚ ä¸ºäº†æ¯”è¾ƒï¼ŒåŸºäºçŸ­è¯­çš„SMTç³»ç»Ÿåœ¨ç›¸åŒçš„æ•°æ®é›†ä¸Šå®ç°äº†33.3çš„BLEUåˆ†æ•°ã€‚ å½“æˆ‘ä»¬ä½¿ç”¨LSTMå¯¹ä¸Šè¿°SMTç³»ç»Ÿäº§ç”Ÿçš„1000ä¸ªå‡è®¾è¿›è¡Œé‡æ–°æ’åºæ—¶ï¼Œå…¶BLEUåˆ†æ•°å¢åŠ åˆ°36.5ï¼Œè¿™æ¥è¿‘äºå…ˆå‰çš„æŠ€æœ¯æ°´å¹³ã€‚ LSTMè¿˜å­¦ä¹ å¯¹è¯­åºæ•æ„Ÿä¸”å¯¹ä¸»åŠ¨è¯­æ€å’Œè¢«åŠ¨è¯­æ€ç›¸å¯¹ä¸å˜çš„æœ‰æ„ä¹‰çŸ­è¯­å’Œå¥å­è¡¨ç¤ºã€‚ æœ€åï¼Œæˆ‘ä»¬å‘ç°åœ¨æ‰€æœ‰æºå¥ï¼ˆè€Œä¸æ˜¯ç›®æ ‡å¥ï¼‰ä¸­é¢ å€’è¯åºæ˜¾è‘—æé«˜äº†LSTMçš„æ€§èƒ½ï¼Œå› ä¸ºè¿™æ ·åšåœ¨æºå¥å’Œç›®æ ‡å¥ä¹‹é—´å¼•å…¥äº†è®¸å¤šçŸ­æœŸä¾èµ–æ€§ï¼Œä½¿å¾—ä¼˜åŒ–é—®é¢˜å˜å¾—å®¹æ˜“ã€‚</p>

<h5 id="16-cho-k-van-merrienboer-b-gulcehre-c-et-allearning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translationarxiv-preprint14061078v2-2014">[16] Cho K, van Merrienboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-Decoder for statistical machine translation.arXiv preprint/1406.1078v2, 2014</h5>
<p>In this paper, we propose a novel neural network model called RNN Encoder Decoder that consists of two recurrent neural networks (RNN). One RNN en- codes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN EncoderDecoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”±ä¸¤ä¸ªé€’å½’ç¥ç»ç½‘ç»œ(RNN)ç»„æˆçš„æ–°çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç§°ä¸ºRNNç¼–ç å™¨è¯‘ç å™¨ã€‚ ä¸€ä¸ªrnnå°†ç¬¦å·åºåˆ—ç¼–ç ä¸ºå›ºå®šé•¿åº¦å‘é‡è¡¨ç¤ºï¼Œå¦ä¸€ä¸ªrnnå°†è¯¥è¡¨ç¤ºè§£ç ä¸ºå¦ä¸€ç¬¦å·åºåˆ—ã€‚ è¯¥æ¨¡å‹çš„ç¼–ç å™¨å’Œè§£ç å™¨è¢«è”åˆè®­ç»ƒä»¥åœ¨ç»™å®šæºåºåˆ—çš„æƒ…å†µä¸‹æœ€å¤§åŒ–ç›®æ ‡åºåˆ—çš„æ¡ä»¶æ¦‚ç‡ã€‚ å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨RNNç¼–ç å™¨è®¡ç®—çš„çŸ­è¯­å¯¹æ¡ä»¶æ¦‚ç‡ä½œä¸ºç°æœ‰å¯¹æ•°çº¿æ€§æ¨¡å‹çš„é™„åŠ ç‰¹å¾ï¼Œç»Ÿè®¡æœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„æ€§èƒ½å¾—åˆ°äº†æ”¹å–„ã€‚ å®šæ€§åœ°ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ¨¡å‹å­¦ä¹ è¯­è¨€çŸ­è¯­çš„è¯­ä¹‰å’Œå¥æ³•æ„ä¹‰çš„è¡¨ç¤ºã€‚</p>

<h5 id="17-cho-k-van-merrienboer-b-bahdanau-d-et-alon-the-properties-of-neural-machine-translationencoder-decoder-approachesproceedings-of-the-ssst-8-eighth-workshop-on-syntax-semantics-and-structure-in-statistical-translationdoha-qatar-2014103-111">[17] Cho K, van Merrienboer B, Bahdanau D, et al.On the properties of neural machine translation:encoder-decoder approaches//Proceedings of the SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.Doha, Qatar, 2014:103-111</h5>
<p>Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a ï¬xed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoderâ€“Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we ï¬nd that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.
ç¥ç»æœºå™¨ç¿»è¯‘æ˜¯ä¸€ç§æ¯”è¾ƒæ–°çš„åŸºäºç¥ç»ç½‘ç»œçš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘æ–¹æ³•ã€‚ ç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹é€šå¸¸ç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆã€‚ ç¼–ç å™¨ä»å¯å˜é•¿åº¦è¾“å…¥è¯­å¥ä¸­æå–å›ºå®šé•¿åº¦è¡¨ç¤ºï¼Œè§£ç å™¨ä»è¯¥è¡¨ç¤ºä¸­ç”Ÿæˆæ­£ç¡®çš„è½¬æ¢ã€‚ æœ¬æ–‡åˆ©ç”¨ä¸¤ä¸ªæ¨¡å‹åˆ†æäº†ç¥ç»æœºå™¨ç¿»è¯‘çš„æ€§è´¨ï¼› RNNç¼–è§£ç å™¨å’Œä¸€ç§æ–°æå‡ºçš„é—¨é™é€’å½’å·ç§¯ç¥ç»ç½‘ç»œã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œç¥ç»æœºå™¨ç¿»è¯‘å¯¹æ— ç”Ÿè¯çŸ­å¥çš„ç¿»è¯‘æ•ˆæœè¾ƒå¥½ï¼Œä½†éšç€å¥å­é•¿åº¦å’Œç”Ÿè¯æ•°ç›®çš„å¢åŠ ï¼Œå…¶ç¿»è¯‘æ•ˆæœè¿…é€Ÿä¸‹é™ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ‰€æå‡ºçš„é—¨é™é€’å½’å·ç§¯ç½‘ç»œèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å¥å­çš„è¯­æ³•ç»“æ„ã€‚</p>

<h5 id="18-jean-s-cho-k-bengio-yon-using-very-large-target-vocabulary-for-neural-machine-translationproceedings-of-the53rd-annual-meeting-of-the-association-for-computational-linguistics-and-the-7th-international-joint-conference-on-natural-language-processing-acl-2015-beijing-china-20151-10">[18] Jean S, Cho K, Bengio Y.On using very large target vocabulary for neural machine translation//Proceedings of the53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:1-10</h5>
<p>Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efï¬ciently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the Englishâ†’German and Englishâ†’French translation tasks of WMTâ€™14.
ç¥ç»æœºå™¨ç¿»è¯‘æ˜¯æœ€è¿‘æå‡ºçš„ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„æœºå™¨ç¿»è¯‘æ–¹æ³•ï¼Œä¸ç°æœ‰çš„åŸºäºçŸ­è¯­çš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘æ–¹æ³•ç›¸æ¯”ï¼Œç¥ç»æœºå™¨ç¿»è¯‘å·²ç»å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚ å°½ç®¡è¿‘å¹´æ¥ç¥ç»æœºå™¨ç¿»è¯‘å–å¾—äº†å¾ˆå¤§çš„æˆåŠŸï¼Œä½†ç”±äºè®­ç»ƒå¤æ‚åº¦å’Œè§£ç å¤æ‚åº¦ä¸ç›®æ ‡è¯çš„æ•°é‡æˆæ­£æ¯”åœ°å¢åŠ ï¼Œç¥ç»æœºå™¨ç¿»è¯‘åœ¨å¤„ç†æ›´å¤§çš„è¯æ±‡é‡æ–¹é¢ä»æœ‰å…¶å±€é™æ€§ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé‡è¦æ€§æŠ½æ ·çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸æˆ‘ä»¬ä½¿ç”¨éå¸¸å¤§çš„ç›®æ ‡è¯æ±‡é‡ï¼Œè€Œä¸å¢åŠ è®­ç»ƒå¤æ‚åº¦ã€‚ æˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿åœ¨ç›®æ ‡è¯æ±‡é‡éå¸¸å¤§çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åªé€‰æ‹©æ•´ä¸ªç›®æ ‡è¯æ±‡é‡çš„ä¸€å°éƒ¨åˆ†ï¼Œè§£ç ä¹Ÿèƒ½æœ‰æ•ˆåœ°å®Œæˆã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹ä¸åŸºäºLSTMçš„ç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹å’Œè¯æ±‡é‡è¾ƒå°çš„åŸºçº¿æ¨¡å‹ç›¸åŒ¹é…ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³ä¼˜äºåè€…ã€‚ æ­¤å¤–ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨å‡ ä¸ªæ¨¡å‹çš„é›†åˆï¼Œå…·æœ‰éå¸¸å¤§çš„ç›®æ ‡è¯æ±‡é‡æ—¶ï¼Œæˆ‘ä»¬åœ¨WMTâ€™14çš„è‹±è¯­â†’å¾·è¯­å’Œè‹±è¯­â†’æ³•è¯­ç¿»è¯‘ä»»åŠ¡ä¸Šè·å¾—äº†ä¸ç°æœ‰æŠ€æœ¯æ°´å¹³ï¼ˆç”¨BLEUè¡¡é‡ï¼‰ç›¸å½“çš„æ€§èƒ½ã€‚</p>

<h5 id="19-jean-s-firat-o-cho-k-et-almontreal-neural-machine-translation-systems-for-wmt15proceedings-of-the-10th-workshop-on-statistical-machine-translationlisbon-portugal-2015134-140">[19] Jean S, Firat O, Cho K, et al.Montreal neural machine translation systems for WMTâ€™15//Proceedings of the 10th Workshop on Statistical Machine Translation.Lisbon, Portugal, 2015:134-140</h5>
<p>Neural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, including Englishâ†’French and Englishâ†’German. The main purpose of the Montreal Institute for Learning Algorithms (MILA) submission to WMTâ€™15 is to evaluate this new approach on a greater variety of language pairs. Furthermore, the human evaluation campaign may help us and the research community to better understand the behaviour of our systems. We use the RNNsearch architecture, which adds an attention mechanism to the encoderdecoder. We also leverage some of the recent developments in NMT, including the use of large vocabularies, unknown word replacement and, to a limited degree, the inclusion of monolingual language models.
ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ç³»ç»Ÿæœ€è¿‘åœ¨ä¸€äº›ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„ç»“æœï¼ŒåŒ…æ‹¬è‹±è¯­â†’æ³•è¯­å’Œè‹±è¯­â†’å¾·è¯­ã€‚ è’™ç‰¹åˆ©å°”å­¦ä¹ ç®—æ³•ç ”ç©¶æ‰€(MILA)æäº¤ç»™WMTâ€™15çš„ä¸»è¦ç›®çš„æ˜¯è¯„ä¼°è¿™ç§æ–°æ–¹æ³•åœ¨æ›´å¤šè¯­è¨€å¯¹ä¸Šçš„å¤šæ ·æ€§ã€‚ æ­¤å¤–ï¼Œäººç±»è¯„ä¼°æ´»åŠ¨å¯èƒ½æœ‰åŠ©äºæˆ‘ä»¬å’Œç ”ç©¶ç•Œæ›´å¥½åœ°äº†è§£æˆ‘ä»¬ç³»ç»Ÿçš„è¡Œä¸ºã€‚ æˆ‘ä»¬ä½¿ç”¨RNNSearchä½“ç³»ç»“æ„ï¼Œå®ƒä¸ºç¼–ç å™¨è§£ç å™¨æ·»åŠ äº†ä¸€ä¸ªå…³æ³¨æœºåˆ¶ã€‚ æˆ‘ä»¬è¿˜åˆ©ç”¨äº†NMTçš„ä¸€äº›æœ€æ–°å‘å±•ï¼ŒåŒ…æ‹¬å¤§é‡è¯æ±‡çš„ä½¿ç”¨ã€æœªçŸ¥è¯çš„æ›¿æ¢ï¼Œä»¥åŠåœ¨ä¸€å®šç¨‹åº¦ä¸ŠåŒ…æ‹¬å•è¯­è¯­è¨€æ¨¡å‹ã€‚</p>

<h5 id="20-bentivogli-l-bisazza-a-cettolo-m-et-alneural-versus-phrase-based-machine-translation-qualityproceedings-of-the2016conference-on-empirical-methods-in-natural-language-processing-emnlp-2016-austin-usa-2016257-267">[20] Bentivogli L, Bisazza A, Cettolo M, et al.Neural versus phrase-based machine translation quality//Proceedings of the2016Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:257-267</h5>
<p>Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-ofthe-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural vs. phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models â€“ such as the reordering of verbs â€“ while pointing out other aspects that remain to be improved.
åœ¨ç»Ÿè®¡æœºå™¨ç¿»è¯‘(SMT)é¢†åŸŸï¼Œç¥ç»ç½‘ç»œæ–¹æ³•(NMT)æœ€è¿‘æˆä¸ºèƒ½å¤ŸæŒ‘æˆ˜åŸºäºçŸ­è¯­çš„æ–¹æ³•(PBMT)é•¿æœŸä¸»å¯¼åœ°ä½çš„ç¬¬ä¸€ç§æŠ€æœ¯ã€‚ ç‰¹åˆ«æ˜¯ï¼Œåœ¨2015å¹´IWSLTè¯„ä¼°æ´»åŠ¨ä¸­ï¼ŒNMTåœ¨è‹±å¾·ä¸¤ç§è¯­è¨€ä¸­çš„è¡¨ç°è¶…è¿‡äº†å…¬è®¤çš„æœ€å…ˆè¿›çš„PBMTç³»ç»Ÿï¼Œç”±äºå½¢æ€å’Œå¥æ³•å·®å¼‚ï¼Œè¿™ä¸€è¯­è¨€å¯¹å°¤å…¶å›°éš¾ã€‚ ä¸ºäº†äº†è§£NMTåœ¨å“ªäº›æ–¹é¢æä¾›äº†æ¯”PBMTæ›´å¥½çš„ç¿»è¯‘è´¨é‡ï¼Œæˆ‘ä»¬å¯¹åŸºäºç¥ç»å’ŒçŸ­è¯­çš„SMTè¾“å‡ºè¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œåˆ©ç”¨äº†ä¸“ä¸šç¿»è¯‘äººå‘˜å¯¹IWSLTæ•°æ®è¿›è¡Œçš„é«˜è´¨é‡åæœŸç¼–è¾‘ã€‚ æˆ‘ä»¬çš„åˆ†æé¦–æ¬¡æä¾›äº†å…³äºå“ªäº›è¯­è¨€ç°è±¡æœ€å¥½ç”¨ç¥ç»æ¨¡å‹æ¥å»ºæ¨¡çš„æœ‰ç”¨è§è§£ï¼Œæ¯”å¦‚åŠ¨è¯çš„é‡æ–°æ’åºï¼ŒåŒæ—¶æŒ‡å‡ºäº†æœ‰å¾…æ”¹è¿›çš„å…¶ä»–æ–¹é¢ã€‚</p>

<h5 id="21-zong-cheng-qingstatistical-machine-translation2nd-editionbeijingtsinghua-university-press-2013-in-chinese-å®—æˆåº†ç»Ÿè®¡è‡ªç„¶è¯­è¨€å¤„ç†ç¬¬2ç‰ˆåŒ—äº¬æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾-2013">[21] Zong Cheng-Qing.Statistical Machine Translation.2nd Edition.Beijing:Tsinghua University Press, 2013 (in Chinese) (å®—æˆåº†.ç»Ÿè®¡è‡ªç„¶è¯­è¨€å¤„ç†.ç¬¬2ç‰ˆ.åŒ—äº¬:æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾, 2013)</h5>

<h5 id="22-bahdanau-d-cho-k-bengio-yneural-machine-translation-by-jointly-learning-to-align-and-translatearxiv-preprint14090473v6-2014">[22] Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate.arXiv preprint/1409.0473v6, 2014</h5>
<p>Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoderâ€“decoders and encode a source sentence into a ï¬xed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ï¬xed-length vector is a bottleneck in improving the performance of this basic encoderâ€“decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.
ç¥ç»æœºå™¨ç¿»è¯‘æ˜¯è¿‘å¹´æ¥æå‡ºçš„ä¸€ç§æœºå™¨ç¿»è¯‘æ–¹æ³•ã€‚ ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘ä¸åŒï¼Œç¥ç»æœºå™¨ç¿»è¯‘çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªå¯ä»¥è”åˆè°ƒæ•´çš„å•ä¸€ç¥ç»ç½‘ç»œï¼Œä»¥ä½¿ç¿»è¯‘æ€§èƒ½è¾¾åˆ°æœ€å¤§ã€‚ æœ€è¿‘æå‡ºçš„ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘çš„æ¨¡å‹é€šå¸¸å±äºç¼–ç å™¨-è§£ç å™¨å®¶æ—ï¼Œå¹¶ä¸”å°†æºè¯­å¥ç¼–ç ä¸ºé™å®šé•¿åº¦çš„å‘é‡ï¼Œè§£ç å™¨ä»è¯¥å‘é‡ç”Ÿæˆç¿»è¯‘ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çŒœæƒ³ä½¿ç”¨å›ºå®šé•¿åº¦å‘é‡æ˜¯æé«˜è¿™ç§åŸºæœ¬ç¼–ç å™¨-è§£ç å™¨æ¶æ„æ€§èƒ½çš„ç“¶é¢ˆï¼Œå¹¶å»ºè®®é€šè¿‡å…è®¸æ¨¡å‹è‡ªåŠ¨ï¼ˆè½¯ï¼‰æœç´¢æºè¯­å¥ä¸­ä¸é¢„æµ‹ç›®æ ‡å•è¯ç›¸å…³çš„éƒ¨åˆ†ï¼Œè€Œä¸å¿…æ˜¾å¼åœ°å°†è¿™äº›éƒ¨åˆ†å½¢æˆç¡¬æ®µï¼Œæ¥æ‰©å±•è¿™ä¸€ç‚¹ã€‚ é€šè¿‡è¿™ç§æ–°çš„ç¿»è¯‘æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†ä¸ç°æœ‰çš„åŸºäºçŸ­è¯­çš„è‹±æ³•ç¿»è¯‘ç³»ç»Ÿç›¸å½“çš„ç¿»è¯‘æ€§èƒ½ã€‚ æ­¤å¤–ï¼Œå®šæ€§åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹å‘ç°çš„ï¼ˆè½¯ï¼‰æ’åˆ—ä¸æˆ‘ä»¬çš„ç›´è§‰å¾ˆå¥½åœ°å»åˆã€‚</p>

<h5 id="23-tu-zhaopeng-lu-zhengdong-liu-yang-et-almodeling-coverage-for-neural-machine-translationproceedings-of-the54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-201676-85">[23] Tu Zhaopeng, Lu Zhengdong, Liu Yang, et al.Modeling coverage for neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:76-85</h5>
<p>Attention mechanism has enhanced stateof-the-art Neural Machine Translation(NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.1
æ³¨æ„æœºåˆ¶é€šè¿‡è”åˆå­¦ä¹ å¯¹é½å’Œç¿»è¯‘æ¥å¢å¼ºæœ€å…ˆè¿›çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ã€‚ ç„¶è€Œï¼Œå®ƒå¾€å¾€å¿½ç•¥è¿‡å»çš„å¯¹é½ä¿¡æ¯ï¼Œè¿™å¾€å¾€å¯¼è‡´è¿‡åº¦ç¿»è¯‘å’Œç¿»è¯‘ä¸è¶³ã€‚ é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºè¦†ç›–çš„NMTã€‚ æˆ‘ä»¬ç»´æŠ¤ä¸€ä¸ªè¦†ç›–å‘é‡æ¥è·Ÿè¸ªå…³æ³¨å†å²ã€‚ å¤ç›–å‘é‡è¢«åé¦ˆåˆ°æ³¨æ„æ¨¡å‹ä¸­ï¼Œä»¥å¸®åŠ©è°ƒæ•´æœªæ¥çš„æ³¨æ„ï¼Œè¿™ä½¿å¾—NMTç³»ç»Ÿèƒ½å¤Ÿæ›´å¤šåœ°è€ƒè™‘æœªç¿»è¯‘çš„æºè¯ã€‚ å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†çš„åŸºäºæ³¨æ„çš„NMT.1æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç¿»è¯‘è´¨é‡å’Œå¯¹é½è´¨é‡</p>

<h5 id="24-druck-g-ganchev-k-graca-jrich-prior-knowledge-in-learning-for-nlpproceedings-of-the-49th-annual-meeting-of-the-association-for-computational-linguistics-acl-2011-portland-usa-20111-57">[24] Druck G, Ganchev K, Graca J.Rich prior knowledge in learning for NLP//Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011) .Portland, USA, 2011:1-57</h5>

<h5 id="25-tu-zhaopeng-liu-yang-shang-lifeng-et-alneural-machine-translation-with-reconstructionproceedings-of-the-31st-aaai-conference-on-artificial-intelligence-aaai-2017-san-francisco-usa-20173097-3103">[25] Tu Zhaopeng, Liu Yang, Shang Lifeng, et al.Neural machine translation with reconstruction//Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI 2017) .San Francisco, USA, 2017:3097-3103</h5>
<p>Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-theart NMT and statistical MT systems.
å°½ç®¡ç«¯åˆ°ç«¯ç¥ç»æœºå™¨ç¿»è¯‘(NMT)åœ¨è¿‡å»çš„ä¸¤å¹´ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å®ƒä¹Ÿå­˜åœ¨ä¸€ä¸ªä¸»è¦çš„ç¼ºé™·:ç”±NMTç³»ç»Ÿç”Ÿæˆçš„è¯‘æ–‡å¾€å¾€ä¸å¤Ÿå……åˆ†ã€‚ äººä»¬æ™®éè®¤ä¸ºï¼ŒNMTå€¾å‘äºé‡å¤ç¿»è¯‘ä¸€äº›æºè¯ï¼Œè€Œé”™è¯¯åœ°å¿½ç•¥äº†å…¶ä»–è¯ã€‚ ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„NMTç¼–è§£ç é‡æ„æ¡†æ¶ã€‚ è¯¥é‡æ„å™¨ç»“åˆåˆ°NMTæ¨¡å‹ä¸­ï¼Œè®¾æ³•ä»è¾“å‡ºç›®æ ‡å¥çš„éšè—å±‚é‡æ„è¾“å…¥æºå¥ï¼Œä»¥ç¡®ä¿å°½å¯èƒ½å¤šåœ°å°†æºç«¯çš„ä¿¡æ¯è½¬æ¢åˆ°ç›®æ ‡ç«¯ã€‚ å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„NMTå’Œç»Ÿè®¡MTç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†NMTè¾“å‡ºçš„å……åˆ†æ€§ï¼Œè·å¾—äº†æ›´å¥½çš„ç¿»è¯‘ç»“æœã€‚</p>

<h5 id="26-cheng-yong-xu-wei-he-zhongjun-et-alsemi-supervised-learning-for-neural-machine-translationproceedings-of-the54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-20161965-1974">[26] Cheng Yong, Xu Wei, He Zhongjun, et al.Semi-supervised learning for neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1965-1974</h5>
<p>While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.
ç«¯åˆ°ç«¯ç¥ç»æœºå™¨ç¿»è¯‘(NMT)è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†NMTç³»ç»Ÿä»…ä¾é å¹¶è¡Œè¯­æ–™åº“è¿›è¡Œå‚æ•°ä¼°è®¡ã€‚ ç”±äºå¹³è¡Œè¯­æ–™åº“åœ¨æ•°é‡ã€è´¨é‡å’Œè¦†ç›–é¢ç­‰æ–¹é¢éƒ½å—åˆ°é™åˆ¶ï¼Œå°¤å…¶æ˜¯å¯¹äºèµ„æºè¾ƒå°‘çš„è¯­è¨€ï¼Œåˆ©ç”¨å•ä¸€è¯­è¨€è¯­æ–™åº“æ¥æé«˜å›½å®¶è¯­è¨€æµ‹è¯•çš„è´¨é‡å·²æˆä¸ºä¸€ç§è¶‹åŠ¿ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠç›‘ç£çš„NMTæ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæ ‡æ³¨ï¼ˆå¹³è¡Œè¯­æ–™åº“ï¼‰å’Œæœªæ ‡æ³¨ï¼ˆå•è¯­è¯­æ–™åº“ï¼‰æ•°æ®çš„çº§è”ã€‚ å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ä¸€ä¸ªè‡ªåŠ¨ç¼–ç å™¨æ¥é‡å»ºå•è¯­è¯­æ–™åº“ï¼Œå…¶ä¸­æºåˆ°ç›®æ ‡å’Œç›®æ ‡åˆ°æºç¿»è¯‘æ¨¡å‹åˆ†åˆ«å……å½“ç¼–ç å™¨å’Œè§£ç å™¨ã€‚ è¯¥æ–¹æ³•ä¸ä»…å¯ä»¥åˆ©ç”¨ç›®çš„è¯­çš„å•è¯­è¯­æ–™åº“ï¼Œè€Œä¸”å¯ä»¥åˆ©ç”¨æºè¯­çš„å•è¯­è¯­æ–™åº“ã€‚ åœ¨ä¸­è‹±æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰çš„SMTå’ŒNMTç³»ç»Ÿæœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>

<h5 id="27-zhang-biao-xiong-deyi-su-jinsongvariational-neural-machine-translationproceedings-of-the-2016conference-on-empirical-methods-in-natural-language-processing-emnlp2016-austin-usa-2016521-530">[27] Zhang Biao, Xiong Deyi, Su Jinsong.Variational neural machine translation//Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing (EMNLP2016) .Austin, USA, 2016:521-530</h5>
<p>Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and EnglishGerman translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.
ç¥ç»æœºå™¨ç¿»è¯‘çš„æ¨¡å‹é€šå¸¸æ¥è‡ªäºä¸€ä¸ªæœ‰åŒºåˆ«çš„ç¼–è§£ç å™¨å®¶æ—ï¼Œå®ƒä»¬å­¦ä¹ ç»™å®šæºå¥çš„ç›®æ ‡å¥çš„æ¡ä»¶åˆ†å¸ƒã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§å­¦ä¹ ç¥ç»æœºå™¨ç¿»è¯‘æ¡ä»¶åˆ†å¸ƒçš„å˜åˆ†æ¨¡å‹:ä¸€ç§å¯è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„å˜åˆ†ç¼–ç å™¨è¯‘ç å™¨æ¨¡å‹ã€‚ ä¸ä¼ ç»Ÿçš„ä»…ä»æºå¥çš„éšè—è¡¨ç¤ºç”Ÿæˆç›®æ ‡è¯‘æ–‡çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ä¸åŒï¼Œå˜åˆ†æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªè¿ç»­çš„æ½œåœ¨å˜é‡ï¼Œä»¥æ˜¾å¼åœ°å»ºæ¨¡æºå¥çš„åº•å±‚è¯­ä¹‰ï¼Œå¹¶æŒ‡å¯¼ç›®æ ‡è¯‘æ–‡çš„ç”Ÿæˆã€‚ ä¸ºäº†è¿›è¡Œæœ‰æ•ˆçš„åéªŒæ¨ç†å’Œå¤§è§„æ¨¡è®­ç»ƒï¼Œæˆ‘ä»¬æ„é€ äº†ä¸€ä¸ªåŒæ—¶æ»¡è¶³æºè¾¹å’Œç›®æ ‡è¾¹æ¡ä»¶çš„ç¥ç»åéªŒé€¼è¿‘å™¨ï¼Œå¹¶ç”¨ä¸€ç§é‡æ–°å‚æ•°åŒ–æŠ€æœ¯æ¥ä¼°è®¡å˜åˆ†ä¸‹ç•Œã€‚ æ±‰è‹±å’Œè‹±å¾·ä¸¤ç§è¯­è¨€çš„ç¿»è¯‘å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„å˜åˆ†ç¥ç»æœºå™¨ç¿»è¯‘æ–¹æ³•æ¯”æ™®é€šçš„ç¥ç»æœºå™¨ç¿»è¯‘æ–¹æ³•æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>

<h5 id="28-wang-mingxuan-lu-zhengdong-li-hang-et-almemoryenhanced-decoder-for-neural-machine-translationproceedings-of-the-2016-conference-on-empirical-methods-in-natural-language-processing-emnlp-2016-austin-usa-2016278-286">[28] Wang Mingxuan, Lu Zhengdong, Li Hang, et al.Memoryenhanced decoder for neural machine translation//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:278-286</h5>
<p>We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memoryenhanced RNN decoder is called MEMDEC. At each time during decoding, MEMDEC will read from this memory and write to this memory once, both with content-based addressing.
Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step.
Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses, yielding the best performance achieved with the same training set.
æå‡ºäº†ä¸€ç§åŸºäºå¤–éƒ¨å­˜å‚¨å™¨çš„ç¥ç»æœºå™¨ç¿»è¯‘å™¨(NMT)ä¸­çš„RNNè¯‘ç å™¨ï¼Œä½œä¸ºå¯¹è¯‘ç RNNä¸­çŠ¶æ€çš„è‡ªç„¶è€Œæœ‰åŠ›çš„æ‰©å±•ã€‚ è¿™ç§å†…å­˜å¢å¼ºçš„RNNè§£ç å™¨ç§°ä¸ºMEMDECã€‚ åœ¨è§£ç æœŸé—´ï¼Œmemdecå°†æ¯æ¬¡ä»è¯¥å­˜å‚¨å™¨è¯»å–å¹¶å†™å…¥è¯¥å­˜å‚¨å™¨ä¸€æ¬¡ï¼Œä¸¤è€…éƒ½ä½¿ç”¨åŸºäºå†…å®¹çš„å¯»å€ã€‚ 
ä¸å…ˆå‰å·¥ä½œï¼ˆBahdanau et al.ï¼Œ2014ï¼‰ä¸­å­˜å‚¨æºè¯­å¥è¡¨ç¤ºçš„æ— ç•Œå†…å­˜ä¸åŒï¼ŒMEMDECä¸­çš„å†…å­˜æ˜¯ä¸€ä¸ªå…·æœ‰é¢„å®šå¤§å°çš„çŸ©é˜µï¼Œå…¶è®¾è®¡ç”¨äºåœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤æ›´å¥½åœ°æ•è·å¯¹è§£ç è¿‡ç¨‹é‡è¦çš„ä¿¡æ¯ã€‚ 
æˆ‘ä»¬å¯¹æ±‰è‹±ç¿»è¯‘çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„è®­ç»ƒæ¡ä»¶ä¸‹ï¼ŒåœŸæ‹¨é¼ å’Œæ‘©è¥¿çš„ç¿»è¯‘èƒ½åŠ›åˆ†åˆ«æé«˜äº†4.8å’Œ5.3å€ï¼Œå–å¾—äº†æœ€å¥½çš„ç¿»è¯‘æ•ˆæœã€‚</p>

<h5 id="29-elman-j-lfinding-structure-in-timecognitive-science-1990-14-2-179-211">[29] Elman J L.Finding structure in time.Cognitive Science, 1990, 14 (2) :179-211</h5>

<h5 id="30-goodfellow-i-bengio-y-courville-adeep-learningcambridge-usamit-press-2015">[30] Goodfellow I, Bengio Y, Courville A.Deep Learning.Cambridge, USA:MIT Press, 2015</h5>

<h5 id="31-bengio-y-simard-p-frasconi-plearning-long-term-dependencies-with-gradient-descent-is-difficultieeetransactions-on-neural-networks-1994-5-2-157-166">[31] Bengio Y, Simard P, Frasconi P.Learning long-term dependencies with gradient descent is difficult.IEEETransactions on Neural Networks, 1994, 5 (2) :157-166</h5>
<p>Abstract- Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. 
æ‘˜è¦-é€’å½’ç¥ç»ç½‘ç»œå¯ç”¨äºå°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°è¾“å‡ºåºåˆ—ï¼Œä¾‹å¦‚ç”¨äºè¯†åˆ«ã€ç”Ÿäº§æˆ–é¢„æµ‹é—®é¢˜ã€‚ ç„¶è€Œï¼Œåœ¨è®­ç»ƒé€’å½’ç¥ç»ç½‘ç»œä»¥æ‰§è¡Œè¾“å…¥/è¾“å‡ºåºåˆ—ä¸­å­˜åœ¨çš„æ—¶é—´å¶ç„¶æ€§è·¨è¶Šè¾ƒé•¿é—´éš”çš„ä»»åŠ¡æ–¹é¢ï¼Œå·²ç»æŠ¥é“äº†å®é™…å›°éš¾ã€‚ æˆ‘ä»¬å±•ç¤ºäº†åŸºäºæ¢¯åº¦çš„å­¦ä¹ ç®—æ³•ä¸ºä»€ä¹ˆéšç€è¦æ•è·çš„ä¾èµ–é¡¹çš„æŒç»­æ—¶é—´çš„å¢åŠ è€Œé¢ä¸´æ—¥ç›Šå›°éš¾çš„é—®é¢˜ã€‚ è¿™äº›ç»“æœæ­ç¤ºäº†åœ¨é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œæœ‰æ•ˆå­¦ä¹ å’Œé•¿æ—¶é—´é”å®šä¿¡æ¯ä¹‹é—´çš„æŠ˜è¡·ã€‚ åŸºäºå¯¹è¿™ä¸€é—®é¢˜çš„ç†è§£ï¼Œè€ƒè™‘äº†æ ‡å‡†æ¢¯åº¦ä¸‹é™çš„æ›¿ä»£æ–¹æ³•ã€‚</p>

<h5 id="32-hochreiter-s-schmidhuber-jlong-short-term-memoryneural-computation-1997-9-8-1735-1780">[32] Hochreiter S, Schmidhuber J.Long short-term memory.Neural Computation, 1997, 9 (8) :1735-1780</h5>
<p>Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiterâ€™s (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O . 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.
é€šè¿‡é€’å½’åå‘ä¼ æ’­å­¦ä¹ åœ¨å»¶é•¿çš„æ—¶é—´é—´éš”ä¸Šå­˜å‚¨ä¿¡æ¯éœ€è¦å¾ˆé•¿çš„æ—¶é—´ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºé”™è¯¯å›æµä¸è¶³å’Œè¡°å‡ã€‚ æˆ‘ä»¬ç®€è¦å›é¡¾äº†Hochreiter(1991)å¯¹è¿™ä¸€é—®é¢˜çš„åˆ†æï¼Œç„¶åé€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„ã€æœ‰æ•ˆçš„ã€åŸºäºæ¢¯åº¦çš„æ–¹æ³•ç§°ä¸ºé•¿çŸ­æ—¶è®°å¿†(LSTM)æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ åœ¨ä¸é€ æˆæŸå®³çš„æƒ…å†µä¸‹æˆªæ–­æ¢¯åº¦ï¼ŒLSTMå¯ä»¥é€šè¿‡åœ¨ç‰¹å®šå•å…ƒå†…å¼ºåˆ¶æ’å®šè¯¯å·®æµé€šè¿‡æ’å®šè¯¯å·®è½¬ç›˜ï¼Œå­¦ä¹ æ¡¥æ¥è¶…è¿‡1000ä¸ªç¦»æ•£æ—¶é—´æ­¥é•¿çš„æœ€å°æ—¶é—´æ»åã€‚ ä¹˜æ³•é—¨å•å…ƒå­¦ä¹ æ‰“å¼€å’Œå…³é—­å¯¹æ’å®šé”™è¯¯æµçš„è®¿é—®ã€‚ LSTMåœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šéƒ½æ˜¯å±€éƒ¨çš„ï¼› è¯¥ç®—æ³•çš„æ—¶é—´æ­¥é•¿å’Œæƒé‡çš„è®¡ç®—å¤æ‚åº¦ä¸ºOã€‚ 1.æˆ‘ä»¬å¯¹äººå·¥æ•°æ®çš„å®éªŒåŒ…æ‹¬å±€éƒ¨ã€åˆ†å¸ƒå¼ã€å®å€¼å’Œæœ‰å™ªå£°çš„æ¨¡å¼è¡¨ç¤ºã€‚ ä¸å®æ—¶é€’å½’å­¦ä¹ ã€æ—¶é—´åå‘ä¼ æ’­ã€é€’å½’çº§è”ç›¸å…³ã€Elmanç½‘ç»œå’Œç¥ç»åºåˆ—åˆ†å—ç›¸æ¯”ï¼ŒLSTMå¯ä»¥è·å¾—æ›´å¤šçš„æˆåŠŸè¿è¡Œï¼Œå¹¶ä¸”å­¦ä¹ é€Ÿåº¦æ›´å¿«ã€‚ LSTMè¿˜å¯ä»¥è§£å†³å¤æ‚çš„ã€äººå·¥çš„ã€é•¿æ—¶é—´æ»åçš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡ä»¥å‰çš„é€’å½’ç½‘ç»œç®—æ³•ä»æœªè§£å†³è¿‡ã€‚</p>

<h5 id="33-chung-j-gulcehre-c-cho-k-et-alempirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modelingarxiv-preprint14123555v1-2014">[33] Chung J, Gulcehre C, Cho K, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling.arXiv preprint/1412.3555v1, 2014</h5>
<p>In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.
æœ¬æ–‡æ¯”è¾ƒäº†é€’å½’ç¥ç»ç½‘ç»œ(RNNS)ä¸­ä¸åŒç±»å‹çš„é€’å½’å•å…ƒã€‚ ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å…³æ³¨å®ç°é€‰é€šæœºåˆ¶çš„æ›´å¤æ‚çš„å•å…ƒï¼Œä¾‹å¦‚é•¿çŸ­æ—¶è®°å¿†(LSTM)å•å…ƒå’Œæœ€è¿‘æå‡ºçš„é€‰é€šé€’å½’å•å…ƒ(GRU)ã€‚ åœ¨å¤è°ƒéŸ³ä¹å»ºæ¨¡å’Œè¯­éŸ³ä¿¡å·å»ºæ¨¡çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™äº›é€’å½’å•å…ƒè¿›è¡Œè¯„ä¼°ã€‚ æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›å…ˆè¿›çš„é€’å½’å•ä½ç¡®å®æ¯”æ›´ä¼ ç»Ÿçš„é€’å½’å•ä½ï¼Œå¦‚tanhå•ä½æ›´å¥½ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°GRUä¸LSTMç›¸å½“ã€‚</p>

<h5 id="34-socher-r-huang-e-h-pennington-jdynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detectionproceedings-of-the-neural-information-processing-systems-nips-2011-granada-spain-2011801-809">[34] Socher R, Huang E H, Pennington J.Dynamic pooling and unfolding recursive autoencoders for paraphrase detection//Proceedings of the Neural Information Processing Systems (NIPS 2011) .Granada, Spain, 2011:801-809</h5>
<p>Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.
é‡Šä¹‰æ£€æµ‹çš„ä»»åŠ¡æ˜¯æ£€æŸ¥ä¸¤ä¸ªå¥å­å¹¶ç¡®å®šå®ƒä»¬æ˜¯å¦å…·æœ‰ç›¸åŒçš„æ„ä¹‰ã€‚ ä¸ºäº†è·å¾—è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œéœ€è¦å¯¹è¿™ä¸¤ä¸ªè¯­å¥è¿›è¡Œæ·±å…¥çš„å¥æ³•å’Œè¯­ä¹‰åˆ†æã€‚ ä»‹ç»äº†ä¸€ç§åŸºäºé€’å½’è‡ªåŠ¨ç¼–ç å™¨(RAE)çš„é‡Šä¹‰æ£€æµ‹æ–¹æ³•ã€‚ æˆ‘ä»¬çš„æ— ç›‘ç£RAESåŸºäºä¸€ä¸ªæ–°çš„å±•å¼€ç›®æ ‡ï¼Œå­¦ä¹ å¥æ³•æ ‘ä¸­çŸ­è¯­çš„ç‰¹å¾å‘é‡ã€‚ è¿™äº›ç‰¹å¾è¢«ç”¨æ¥è¡¡é‡ä¸¤ä¸ªå¥å­åœ¨è¯å’ŒçŸ­è¯­æ–¹é¢çš„ç›¸ä¼¼æ€§ã€‚ ç”±äºå¥å­å¯ä»¥æ˜¯ä»»æ„é•¿åº¦çš„ï¼Œå› æ­¤å¾—åˆ°çš„ç›¸ä¼¼æ€§åº¦é‡çŸ©é˜µçš„å¤§å°æ˜¯å¯å˜çš„ã€‚ æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŠ¨æ€æ± å±‚ï¼Œå®ƒä»å¯å˜å¤§å°çš„çŸ©é˜µä¸­è®¡ç®—å‡ºå›ºå®šå¤§å°çš„è¡¨ç¤ºã€‚ ç„¶åï¼Œå°†æ± è¡¨ç¤ºç”¨ä½œåˆ†ç±»å™¨çš„è¾“å…¥ã€‚ åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„MSRPé‡Šä¹‰è¯­æ–™åº“ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–ç°æœ‰çš„æ–¹æ³•ã€‚</p>

<h5 id="35-graves-a-wayne-g-reynolds-m-et-alhybrid-computing-using-a-neural-network-with-dynamic-external-memorynature-2016-538471-476">[35] Graves A, Wayne G, Reynolds M, et al.Hybrid computing using a neural network with dynamic external memory.Nature, 2016, 538:471-476</h5>

<h5 id="36-graves-a-wayne-g-danihelka-ineural-turing-machinesarxiv-preprint14105401v2-2014">[36] Graves A, Wayne G, Danihelka I.Neural turing machines.arXiv preprint/1410.5401v2, 2014</h5>
<p>We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.
æˆ‘ä»¬é€šè¿‡å°†ç¥ç»ç½‘ç»œä¸å¤–éƒ¨è®°å¿†èµ„æºè€¦åˆæ¥æ‰©å±•ç¥ç»ç½‘ç»œçš„èƒ½åŠ›ï¼Œå®ƒä»¬å¯ä»¥é€šè¿‡æ³¨æ„è¿‡ç¨‹ä¸å¤–éƒ¨è®°å¿†èµ„æºäº¤äº’ã€‚ è¯¥ç»„åˆç³»ç»Ÿç±»ä¼¼äºå›¾çµæœºæˆ–å†¯Â·è¯ºä¼Šæ›¼ç»“æ„ï¼Œä½†æ˜¯æ˜¯å¯åŒºåˆ†çš„ç«¯åˆ°ç«¯çš„ï¼Œå…è®¸å®ƒä»¥æ¢¯åº¦ä¸‹é™çš„æ–¹å¼è¢«æœ‰æ•ˆåœ°è®­ç»ƒã€‚ åˆæ­¥ç»“æœè¡¨æ˜ï¼Œç¥ç»å›¾çµæœºå¯ä»¥ä»è¾“å…¥å’Œè¾“å‡ºç¤ºä¾‹ä¸­æ¨æ–­å‡ºç®€å•çš„ç®—æ³•ï¼Œå¦‚å¤åˆ¶ã€æ’åºå’Œè”æƒ³å›å¿†ã€‚</p>

<h5 id="37-weston-j-chopra-s-bordes-amemory-networksarxiv-preprint14103916v11-2014">[37] Weston J, Chopra S, Bordes A.Memory networks.arXiv preprint/1410.3916v11, 2014</h5>
<p>We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.
æˆ‘ä»¬æè¿°äº†ä¸€ç±»æ–°çš„å­¦ä¹ æ¨¡å‹ï¼Œç§°ä¸ºè®°å¿†ç½‘ç»œã€‚ æ¨ç†æˆåˆ†ä¸é•¿æ—¶è®°å¿†æˆåˆ†ç›¸ç»“åˆçš„è®°å¿†ç½‘ç»œæ¨ç† ä»–ä»¬å­¦ä¹ å¦‚ä½•è”åˆä½¿ç”¨è¿™äº›ã€‚ é•¿æœŸè®°å¿†å¯ä»¥è¯»å†™ï¼Œç›®çš„æ˜¯ç”¨æ¥é¢„æµ‹ã€‚ åœ¨é—®ç­”ç³»ç»Ÿä¸­ï¼Œé•¿æ—¶è®°å¿†ä½œä¸ºä¸€ä¸ªï¼ˆåŠ¨æ€ï¼‰çŸ¥è¯†åº“ï¼Œè¾“å‡ºæ˜¯ä¸€ç§è¯­ç¯‡ååº”ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†ç ”ç©¶ã€‚ æˆ‘ä»¬åœ¨ä¸€ä¸ªå¤§è§„æ¨¡çš„é—®ç­”ä»»åŠ¡ï¼Œä»¥åŠä¸€ä¸ªæ›´å°ï¼Œä½†æ›´å¤æ‚çš„ç©å…·ä»»åŠ¡ï¼Œä»ä¸€ä¸ªæ¨¡æ‹Ÿçš„ä¸–ç•Œäº§ç”Ÿè¯„ä¼°ä»–ä»¬ã€‚ åœ¨åè€…ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é“¾æ¥å¤šä¸ªæ”¯æŒå¥æ¥å›ç­”éœ€è¦ç†è§£åŠ¨è¯å†…æ¶µçš„é—®é¢˜ï¼Œä»è€Œå±•ç¤ºäº†è¿™ç§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>

<h5 id="38-xu-k-ba-j-kiros-r-et-alshow-attend-and-tellneural-image-caption-generation-with-visual-attentionarxiv-preprint150203044-2015">[38] Xu K, Ba J, Kiros R, et al.Show, attend and tell:neural image caption generation with visual attention.arXiv preprint/1502.03044, 2015</h5>
<p>Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.
å—æœºå™¨ç¿»è¯‘å’Œç›®æ ‡æ£€æµ‹ç ”ç©¶çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å›¾åƒå†…å®¹è‡ªåŠ¨å­¦ä¹ æ¨¡å‹ã€‚ æˆ‘ä»¬æè¿°äº†å¦‚ä½•ä½¿ç”¨æ ‡å‡†çš„åå‘ä¼ æ’­æŠ€æœ¯ä»¥ç¡®å®šæ€§çš„æ–¹å¼è®­ç»ƒè¯¥æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡æœ€å¤§åŒ–å˜åˆ†ä¸‹ç•Œæ¥éšæœºåœ°è®­ç»ƒè¯¥æ¨¡å‹ã€‚ æˆ‘ä»¬è¿˜é€šè¿‡å¯è§†åŒ–å±•ç¤ºäº†è¯¥æ¨¡å‹å¦‚ä½•åœ¨è¾“å‡ºåºåˆ—ä¸­ç”Ÿæˆç›¸åº”å•è¯çš„åŒæ—¶è‡ªåŠ¨å­¦ä¹ å°†è§†çº¿å›ºå®šåœ¨æ˜¾è‘—å¯¹è±¡ä¸Šã€‚ æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†:Flickr8Kã€Flickr30Kå’ŒMS Cocoä¸Šä»¥æœ€å…ˆè¿›çš„æ€§èƒ½éªŒè¯äº†æ³¨æ„åŠ›çš„ä½¿ç”¨ã€‚</p>

<h5 id="39-luong-m-t-pham-h-manning-c-deffective-approaches-to-attention-based-neural-machine-translationproceedings-of-the-2015-conference-on-empirical-methods-in-natural-language-processing-emnlp-2015-lisbon-portugal-20151412-1421">[39] Luong M-T, Pham H, Manning C D.Effective approaches to attention-based neural machine translation//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) .Lisbon, Portugal, 2015:1412-1421</h5>
<p>An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a signiï¬cant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMTâ€™15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1
è¿‘å¹´æ¥ï¼Œæ³¨æ„æœºåˆ¶è¢«ç”¨æ¥æ”¹è¿›ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ï¼Œåœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­æœ‰é€‰æ‹©åœ°å…³æ³¨æºå¥çš„æŸäº›éƒ¨åˆ†ã€‚ ç„¶è€Œï¼Œå¯¹äºåŸºäºæ³¨æ„åŠ›çš„NMTï¼Œå¾ˆå°‘æœ‰äººæ¢ç´¢æœ‰ç”¨çš„ä½“ç³»ç»“æ„ã€‚ æœ¬æ–‡æ¢è®¨äº†ä¸¤ç±»ç®€å•è€Œæœ‰æ•ˆçš„æ³¨æ„æœºåˆ¶:ä¸€ç§æ˜¯å§‹ç»ˆå…³æ³¨æ‰€æœ‰æºè¯çš„æ•´ä½“æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯ä¸€æ¬¡åªå…³æ³¨æºè¯å­é›†çš„å±€éƒ¨æ–¹æ³•ã€‚ æˆ‘ä»¬ä»ä¸¤ä¸ªæ–¹é¢è®ºè¯äº†è¿™ä¸¤ç§ç¿»è¯‘æ–¹æ³•åœ¨è‹±è¯­å’Œå¾·è¯­ä¹‹é—´çš„ç¿»è¯‘ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ åœ¨æœ¬åœ°å…³æ³¨çš„æƒ…å†µä¸‹ï¼Œä¸å·²ç»é›†æˆäº†è¾å­¦ç­‰å·²çŸ¥æŠ€æœ¯çš„éå…³æ³¨ç³»ç»Ÿç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†5.0ä¸ªBLEUç‚¹çš„æ˜¾è‘—å¢ç›Šã€‚ æˆ‘ä»¬çš„é›†æˆæ¨¡å‹ä½¿ç”¨äº†ä¸åŒçš„æ³¨æ„ç»“æ„ï¼Œåœ¨WMTâ€™15è‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­äº§ç”Ÿäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„ç»“æœï¼Œå¾—åˆ°äº†25.9ä¸ªBLEUç‚¹ï¼Œæ¯”NMTæ”¯æŒçš„ç°æœ‰æœ€ä½³ç³»ç»Ÿå’Œä¸€ä¸ªN-Gramé‡æ’åºå™¨æé«˜äº†1.0ä¸ªBLEUç‚¹ã€‚ 1</p>

<h5 id="40-liu-l-utiyama-m-finch-a-et-alneural-machine-translation-with-supervised-attentionproceedings-of-the-coling2016-the-26th-international-conference-on-computational-linguisticsosaka-japan-20163093-3102">[40] Liu L, Utiyama M, Finch A, et al.Neural machine translation with supervised attention//Proceedings of the COLING2016, the 26th International Conference on Computational Linguistics.Osaka, Japan, 2016:3093-3102</h5>
<p>The attention mechanisim is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.
æ³¨æ„åŠ›æœºåˆ¶å¯¹ç¥ç»æœºå™¨ç¿»è¯‘å¾ˆæœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿé€šè¿‡åœ¨ç›®æ ‡è¯å’Œæºè¯ä¹‹é—´äº§ç”Ÿå¯¹é½æ¥åŠ¨æ€åœ°ç¼–ç æºå¥ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œå®ƒåœ¨å¯¹å‡†ç²¾åº¦æ–¹é¢å·²ç»è¢«è¯æ˜æ¯”ä¼ ç»Ÿçš„å¯¹å‡†æ¨¡å‹å·®ã€‚ æœ¬æ–‡ä»é‡æ–°æ’åºçš„è§’åº¦å¯¹è¿™ä¸€é—®é¢˜è¿›è¡Œäº†åˆ†æå’Œè§£é‡Šï¼Œæå‡ºäº†ä¸€ç§åœ¨å¸¸è§„å¯¹é½æ¨¡å‹æŒ‡å¯¼ä¸‹å­¦ä¹ çš„ç›‘ç£æ³¨æ„ã€‚ åœ¨ä¸¤ä¸ªæ±‰è‹±ç¿»è¯‘ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼Œç›‘ç£æ³¨æ„æœºåˆ¶æ¯”åŸºäºæ ‡å‡†æ³¨æ„çš„NMTæœ‰æ›´å¥½çš„å¯¹é½æ•ˆæœã€‚</p>

<h5 id="41-liu-yang-sun-maosongcontrastive-unsupervised-word-alignment-with-non-local-featuresproceedings-of-the-29th-aaai-conference-on-artificial-intelligence-aaai-2015-austin-usa-20152295-2301">[41] Liu Yang, Sun Maosong.Contrastive unsupervised word alignment with non-local features//Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI 2015) .Austin, USA, 2015:2295-2301</h5>
<p>Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over stateof-the-art unsupervised word alignment methods.
è¯å¯¹é½æ˜¯ä¸€é¡¹é‡è¦çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå®ƒåæ˜ äº†è‡ªç„¶è¯­è¨€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚ è¿‘å¹´æ¥ï¼Œå¯¹æ•°çº¿æ€§æ¨¡å‹çš„æ— ç›‘ç£å­¦ä¹ ç»“åˆäº†äº§ç”Ÿå¼å­¦ä¹ å’Œåˆ¤åˆ«å¼å­¦ä¹ çš„ä¼˜ç‚¹ï¼Œå—åˆ°äº†å¹¿æ³›çš„å…³æ³¨ã€‚ ç„¶è€Œï¼Œä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨:è®¡ç®—å¯¹æ•è·è‡ªç„¶è¯­è¨€ä¹‹é—´çš„å·®å¼‚è‡³å…³é‡è¦çš„éæœ¬åœ°ç‰¹å¾çš„æœŸæœ›æ˜¯å¾ˆéš¾çš„ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯¹æ¯”çš„æ–¹æ³•ï¼Œæ—¨åœ¨åŒºåˆ†è§‚å¯Ÿåˆ°çš„è®­ç»ƒæ ·æœ¬å’Œå™ªå£°ã€‚ å®ƒä¸ä»…å¼•å…¥å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼æ— ç›‘ç£å­¦ä¹ ï¼Œè€Œä¸”æ¶ˆé™¤äº†åˆ’åˆ†å‡½æ•°ã€‚ åŸºäºå¯¹æ•°çº¿æ€§è¯å¯¹é½æ¨¡å‹çš„æ¦‚ç‡è´¨é‡é€šå¸¸æ˜¯é«˜åº¦é›†ä¸­çš„ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨Top-Nå¯¹é½æ¥è¿‘ä¼¼å¯¹åéªŒåˆ†å¸ƒçš„æœŸæœ›ã€‚ è¿™å…è®¸é«˜æ•ˆå’Œå‡†ç¡®åœ°è®¡ç®—éæœ¬åœ°ç‰¹å¾çš„æœŸæœ›ã€‚ å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æ— ç›‘ç£è¯å¯¹é½æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>

<h5 id="42-cheng-yong-shen-shiqi-he-zhongjun-et-alagreementbased-joint-training-for-bidirectional-attention-based-neural-machine-translationproceedings-of-the-25th-international-joint-conference-on-artificial-intelligence-ijcai-2016-new-york-usa-20162761-2767">[42] Cheng Yong, Shen Shiqi, He Zhongjun, et al.Agreementbased joint training for bidirectional attention-based neural machine translation//Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI 2016) .New York, USA, 2016:2761-2767</h5>
<p>The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on ChineseEnglish and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training.
æ³¨æ„æœºåˆ¶åœ¨æé«˜ç«¯åˆ°ç«¯ç¥ç»æœºå™¨ç¿»è¯‘ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ ç„¶è€Œï¼Œç”±äºè‡ªç„¶è¯­è¨€ä¹‹é—´é”™ç»¼å¤æ‚çš„ç»“æ„å·®å¼‚ï¼ŒåŸºäºå•å‘æ³¨æ„çš„æ¨¡å‹å¯èƒ½åªæ•æ‰åˆ°æ³¨æ„è§„å¾‹çš„éƒ¨åˆ†æ–¹é¢ã€‚ æå‡ºäº†ä¸€ç§åŸºäºåè®®çš„åŒå‘æ³¨æ„ç«¯åˆ°ç«¯ç¥ç»æœºå™¨ç¿»è¯‘è”åˆè®­ç»ƒæ–¹æ³•ã€‚ æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯ç‹¬ç«‹åœ°è®­ç»ƒæºåˆ°ç›®æ ‡å’Œç›®æ ‡åˆ°æºç¿»è¯‘æ¨¡å‹ï¼Œè€Œæ˜¯é¼“åŠ±è¿™ä¸¤ä¸ªäº’è¡¥çš„æ¨¡å‹åœ¨ç›¸åŒçš„è®­ç»ƒæ•°æ®ä¸Šå°±å•è¯å¯¹é½çŸ©é˜µè¾¾æˆä¸€è‡´ã€‚ åœ¨æ±‰è‹±å’Œè‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºåè®®çš„è”åˆè®­ç»ƒæ¯”ç‹¬ç«‹è®­ç»ƒæ˜¾è‘—æé«˜äº†å¯¹é½å’Œç¿»è¯‘è´¨é‡ã€‚</p>

<h5 id="43-brown-p-f-pietra-v-j-d-pietra-s-a-d-et-althe-mathematics-of-statistical-machine-translationparameter-estimationcomputational-linguistics-1993-19-2-263-311">[43] Brown P F, Pietra V J D, Pietra S A D, et al.The mathematics of statistical machine translation:Parameter estimation.Computational Linguistics, 1993, 19 (2) :263-311</h5>
<p>We describe a series o,ffive statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal linguistic content they would work well on other pairs o,f languages. We also ,feel, again because of the minimal linguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.
æˆ‘ä»¬æè¿°äº†ä¸€ç³»åˆ—çš„Oï¼Œ5ä¸ªç»Ÿè®¡æ¨¡å‹Oï¼ŒFçš„ç¿»è¯‘è¿‡ç¨‹ï¼Œå¹¶ç»™å‡ºäº†ä¼°è®¡å‚æ•°Oï¼ŒFçš„ç®—æ³•ï¼Œè¿™äº›æ¨¡å‹ç»™å‡ºäº†ä¸€ç»„Oï¼ŒFå¯¹Oï¼ŒFå¥å­ï¼Œå®ƒä»¬æ˜¯ç›¸äº’ç¿»è¯‘çš„Oï¼ŒFã€‚ æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ¦‚å¿µOï¼ŒFé€å­—å¯¹é½è¿™æ ·çš„å¯¹Oï¼ŒFå¥å­ã€‚ å¯¹äºä»»ä½•ç»™å®šçš„è¿™æ ·çš„å¥å­å¯¹ï¼Œæ¯ä¸ªOï¼ŒFæˆ‘ä»¬çš„æ¨¡å‹ä¸ºæ¯ä¸ªå¯èƒ½çš„é€å­—å¯¹é½åˆ†é…ä¸€ä¸ªæ¦‚ç‡ã€‚ æˆ‘ä»¬ç»™å‡ºäº†ä¸€ä¸ªç®—æ³•æ¥å¯»æ‰¾è¿™äº›å¯¹é½ä¸­æœ€æœ‰å¯èƒ½çš„oï¼Œfã€‚ è™½ç„¶è¯¥ç®—æ³•æ˜¯æ¬¡ä¼˜çš„ï¼Œä½†ç”±æ­¤å¾—åˆ°çš„å¯¹é½å¾ˆå¥½åœ°è§£é‡Šäº†Oã€Få¯¹å¥å­ä¸­çš„é€å­—å…³ç³»ã€‚ æˆ‘ä»¬ä»åŠ æ‹¿å¤§è®®ä¼šè®®äº‹å½•ä¸­è·å¾—äº†å¤§é‡çš„æ³•æ–‡å’Œè‹±æ–‡æ•°æ®ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬çš„å·¥ä½œä»…é™äºè¿™ä¸¤ç§è¯­æ–‡ï¼› ä½†æ˜¯æˆ‘ä»¬è§‰å¾—ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç®—æ³•æœ‰æœ€å°çš„è¯­è¨€å†…å®¹ï¼Œä»–ä»¬å°†å¾ˆå¥½åœ°å·¥ä½œåœ¨å…¶ä»–å¯¹Oï¼ŒFè¯­è¨€ã€‚ æˆ‘ä»¬è¿˜è®¤ä¸ºï¼ŒåŒæ ·ç”±äºæˆ‘ä»¬çš„ç®—æ³•çš„æœ€å°è¯­è¨€å†…å®¹Oï¼Œæˆ‘ä»¬æœ‰ç†ç”±è®¤ä¸ºï¼Œé€å­—å¯¹é½æ˜¯ä»»ä½•è¶³å¤Ÿå¤§çš„åŒè¯­è¯­æ–™åº“æ‰€å›ºæœ‰çš„ã€‚</p>

<h5 id="44-feng-shi-liu-shujie-li-mu-et-alimplicit-distortion-and-fertility-models-for-attention-based-encoder-decoder-nmtmodelarxiv-preprint160103317v3-2016">[44] Feng Shi, Liu Shujie, Li Mu, et al.Implicit distortion and fertility models for attention-based encoder-decoder NMTmodel.arXiv preprint/1601.03317v3, 2016</h5>
<p>Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoderdecoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attentionbased encoder-decoder.
è¿‘å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚ å¤§å¤šæ•°NMTæ¨¡å‹éµå¾ªç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ã€‚ ä¸ºäº†ä½¿ç¼–è§£ç å™¨æ¨¡å‹æ›´åŠ çµæ´»ï¼Œå°†æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥æœºå™¨ç¿»è¯‘ä»¥åŠè¯­éŸ³è¯†åˆ«å’Œå›¾åƒå­—å¹•ç­‰ä»»åŠ¡ä¸­ã€‚ æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“å¯¹é½ä¸æ­£ç¡®æ—¶ï¼ŒåŸºäºæ³¨æ„çš„ç¼–ç å™¨-è§£ç å™¨çš„ç¿»è¯‘è´¨é‡ä¼šå—åˆ°æ˜¾è‘—æŸå®³ã€‚ æˆ‘ä»¬æŠŠè¿™äº›é—®é¢˜å½’å’äºç¼ºä¹æ‰­æ›²å’Œç”Ÿè‚²æ¨¡å‹ã€‚ é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è¯‘ç å™¨ï¼Œå¹¶ä¸å…¶ä»–æœºå™¨ç¿»è¯‘æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ¯”åŸæ¥çš„åŸºäºæ³¨æ„çš„ç¼–è§£ç å™¨æé«˜äº†2ä¸ªBLEUç‚¹ã€‚</p>

<h5 id="45-cohn-t-hoang-c-d-v-vymolova-e-et-alincorporating-structural-alignment-biases-into-an-attentional-neural-translation-modelarxiv-preprint160101085v1-2016">[45] Cohn T, Hoang C D V, Vymolova E, et al.Incorporating structural alignment biases into an attentional neural translation model.arXiv preprint/1601.01085v1, 2016</h5>
<p>Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.
è¿‘å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚ å¤§å¤šæ•°NMTæ¨¡å‹éµå¾ªç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ã€‚ ä¸ºäº†ä½¿ç¼–è§£ç å™¨æ¨¡å‹æ›´åŠ çµæ´»ï¼Œå°†æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥æœºå™¨ç¿»è¯‘ä»¥åŠè¯­éŸ³è¯†åˆ«å’Œå›¾åƒå­—å¹•ç­‰ä»»åŠ¡ä¸­ã€‚ æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“å¯¹é½ä¸æ­£ç¡®æ—¶ï¼ŒåŸºäºæ³¨æ„çš„ç¼–ç å™¨-è§£ç å™¨çš„ç¿»è¯‘è´¨é‡ä¼šå—åˆ°æ˜¾è‘—æŸå®³ã€‚ æˆ‘ä»¬æŠŠè¿™äº›é—®é¢˜å½’å’äºç¼ºä¹æ‰­æ›²å’Œç”Ÿè‚²æ¨¡å‹ã€‚ é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è¯‘ç å™¨ï¼Œå¹¶ä¸å…¶ä»–æœºå™¨ç¿»è¯‘æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ¯”åŸæ¥çš„åŸºäºæ³¨æ„çš„ç¼–è§£ç å™¨æé«˜äº†2ä¸ªBLEUç‚¹ã€‚</p>

<h5 id="46-zhang-jinchao-wang-mingxuan-liu-qun-et-alincorporating-word-reordering-knowledge-into-attention-based-neural-machine-translationproceedings-of-the-55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-20171524-1534">[46] Zhang Jinchao, Wang Mingxuan, Liu Qun, et al.Incorporating word reordering knowledge into attention-based neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1524-1534</h5>
<p>This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.
æœ¬æ–‡æå‡ºäº†ä¸‰ç§å¤±çœŸæ¨¡å‹ï¼Œå°†è¯çš„é‡æ–°æ’åºçŸ¥è¯†æ˜¾å¼åœ°èå…¥åˆ°åŸºäºæ³¨æ„çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ä¸­ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç¿»è¯‘æ€§èƒ½ã€‚ æˆ‘ä»¬æå‡ºçš„æ¨¡å‹ä½¿æ³¨æ„æœºåˆ¶èƒ½å¤Ÿå…¼é¡¾è¯ä¹‰è¦æ±‚å’Œè¯åºæƒ©ç½šã€‚ æ±‰è‹±ç¿»è¯‘å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜è¯å¯¹é½è´¨é‡ï¼Œåœ¨åŸºäºåŸºæœ¬æ³¨æ„çš„NMTåŸºç¡€ä¸Šå®ç°æ˜¾è‘—çš„ç¿»è¯‘æ”¹è¿›ã€‚ ä¸å·²æœ‰çš„åŒç±»è¯­æ–™åº“ç›¸æ¯”ï¼Œæœ¬ç³»ç»Ÿåœ¨ç¿»è¯‘è´¨é‡æ–¹é¢å–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚</p>

<h5 id="47-tu-zhaopeng-liu-yang-lu-zhengdong-et-alcontext-gates-for-neural-machine-translationtransactions-of-the-association-for-computational-linguistics-2017-587-99">[47] Tu Zhaopeng, Liu Yang, Lu Zhengdong, et al.Context gates for neural machine translation.Transactions of the Association for Computational Linguistics, 2017, 5:87-99</h5>
<p>In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the adequacy and fluency of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attentionbased NMT system by +2.3 BLEU points.
åœ¨ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ä¸­ï¼Œç›®æ ‡è¯çš„ç”Ÿæˆæ—¢ä¾èµ–äºæºè¯­å¢ƒï¼Œä¹Ÿä¾èµ–äºç›®æ ‡è¯­å¢ƒã€‚ æˆ‘ä»¬å‘ç°ï¼Œæºè¯­è¯­å¢ƒç›´æ¥å½±å“è¯‘æ–‡çš„å……åˆ†æ€§ï¼Œè€Œç›®çš„è¯­è¯­å¢ƒåˆ™å½±å“è¯‘æ–‡çš„æµåˆ©æ€§ã€‚ ä»ç›´è§‰ä¸Šè®²ï¼Œå®è¯çš„ç”Ÿæˆåº”è¯¥æ›´å¤šåœ°ä¾èµ–äºæºè¯­å¢ƒï¼Œè™šè¯çš„ç”Ÿæˆåº”è¯¥æ›´å¤šåœ°ä¾èµ–äºç›®æ ‡è¯­å¢ƒã€‚ ç”±äºç¼ºä¹å¯¹æºè¯­å’Œç›®çš„è¯­è¯­å¢ƒå½±å“çš„æœ‰æ•ˆæ§åˆ¶ï¼Œä¼ ç»Ÿçš„å›½å®¶è¯­è¨€æµ‹è¯•å¾€å¾€äº§ç”Ÿæµåˆ©ä½†ä¸å……åˆ†çš„ç¿»è¯‘ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡é—¨ï¼Œå®ƒåŠ¨æ€åœ°æ§åˆ¶æºä¸Šä¸‹æ–‡å’Œç›®æ ‡ä¸Šä¸‹æ–‡å¯¹ç›®æ ‡è¯ç”Ÿæˆçš„è´¡çŒ®æ¯”ã€‚ è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´ä»”ç»†åœ°æ§åˆ¶æ¥è‡ªä¸Šä¸‹æ–‡çš„ä¿¡æ¯æµæ¥æé«˜NMTçš„å……åˆ†æ€§å’Œæµç•…æ€§ã€‚ å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”æ ‡å‡†çš„åŸºäºæ³¨æ„çš„NMTç³»ç»Ÿæé«˜äº†+2.3bleuç‚¹ã€‚</p>

<h5 id="48-kim-y-jernite-y-sontag-d-et-alcharacter-aware-neural-language-modelsarxiv-preprint150806615v4-2015">[48] Kim Y, Jernite Y, Sontag D, et al.Character-aware neural language models.arXiv preprint/1508.06615v4, 2015</h5>
<p>We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.
æˆ‘ä»¬æè¿°äº†ä¸€ä¸ªä»…ä¾èµ–äºå­—ç¬¦çº§è¾“å…¥çš„ç®€å•ç¥ç»è¯­è¨€æ¨¡å‹ã€‚ é¢„æµ‹ä»ç„¶æ˜¯åœ¨æ–‡å­—ä¸€çº§ä½œå‡ºçš„ã€‚ æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’Œå­—ç¬¦ä¸Šçš„å…¬è·¯ç½‘ï¼Œå…¶è¾“å‡ºè¢«æä¾›ç»™é•¿çŸ­æ—¶è®°å¿†(LSTM)é€’å½’ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼ˆRNN-LMï¼‰ã€‚ åœ¨è‹±å›½å®¾å¤•æ³•å°¼äºšå·çš„æ ‘ä¸›ä¸­ï¼Œè¯¥æ¨¡å‹ä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹ä¸ç›¸ä¸Šä¸‹ï¼Œå°½ç®¡å‚æ•°å°‘äº†60%ã€‚ å¯¹äºå½¢æ€ä¸°å¯Œçš„è¯­è¨€ï¼ˆé˜¿æ‹‰ä¼¯æ–‡ã€æ·å…‹æ–‡ã€æ³•æ–‡ã€å¾·æ–‡ã€è¥¿ç­ç‰™æ–‡ã€ä¿„æ–‡ï¼‰ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½ä¼˜äºè¯çº§/è¯­ç´ çº§LSTMåŸºçº¿ï¼Œå‚æ•°ä¹Ÿè¾ƒå°‘ã€‚ ç»“æœè¡¨æ˜ï¼Œåœ¨è®¸å¤šè¯­è¨€ä¸­ï¼Œå­—ç¬¦è¾“å…¥å¯¹äºè¯­è¨€å»ºæ¨¡æ˜¯è¶³å¤Ÿçš„ã€‚ å¯¹ä»æ¨¡å‹çš„å­—ç¬¦ç»„æˆéƒ¨åˆ†è·å¾—çš„å•è¯è¡¨ç¤ºçš„åˆ†æè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»…ä»å­—ç¬¦ç¼–ç è¯­ä¹‰å’Œæ­£å­—æ³•ä¿¡æ¯ã€‚</p>

<h5 id="49-sennrich-r-haddow-b-birch-aneural-machine-translation-of-rare-words-with-subword-unitsarxiv-preprint150807909v3-2015">[49] Sennrich R, Haddow B, Birch A.Neural machine translation of rare words with subword units.arXiv preprint/1508.07909v3, 2015</h5>
<p>Neural machine translation (NMT) models typically operate with a ï¬xed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks Englishâ†’German and Englishâ†’Russian by up to 1.1 and 1.3 B LEU, respectively.
ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹é€šå¸¸ä½¿ç”¨å›ºå®šçš„è¯æ±‡ï¼Œä½†ç¿»è¯‘æ˜¯ä¸€ä¸ªå¼€æ”¾çš„è¯æ±‡é—®é¢˜ã€‚ å…ˆå‰çš„å·¥ä½œæ˜¯é€šè¿‡å€’é€€åˆ°å­—å…¸ä¸­æ¥è§£å†³è¯æ±‡é‡ä¸è¶³çš„å•è¯çš„ç¿»è¯‘é—®é¢˜ã€‚ æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ›´ç®€å•ã€æ›´æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½¿NMTæ¨¡å‹èƒ½å¤Ÿé€šè¿‡å°†ç¨€æœ‰å’ŒæœªçŸ¥è¯ç¼–ç ä¸ºå­è¯å•å…ƒåºåˆ—æ¥è¿›è¡Œå¼€æ”¾è¯æ±‡ç¿»è¯‘ã€‚ è¿™æ˜¯åŸºäºè¿™æ ·ä¸€ç§ç›´è§‰ï¼Œå³å„ç§è¯ç±»å¯ä»¥é€šè¿‡æ¯”è¯æ›´å°çš„å•ä½æ¥ç¿»è¯‘ï¼Œä¾‹å¦‚åç§°ï¼ˆé€šè¿‡å­—ç¬¦å¤åˆ¶æˆ–éŸ³è¯‘ï¼‰ã€å¤åˆè¯ï¼ˆé€šè¿‡æ„è¯ç¿»è¯‘ï¼‰ä»¥åŠåŒæºè¯å’Œå¤–æ¥è¯ï¼ˆé€šè¿‡è¯­éŸ³å’Œå½¢æ€è½¬æ¢ï¼‰ã€‚ æˆ‘ä»¬è®¨è®ºäº†ä¸åŒçš„åˆ†è¯æŠ€æœ¯ï¼ŒåŒ…æ‹¬ç®€å•å­—ç¬¦NGRAMæ¨¡å‹å’ŒåŸºäºå­—èŠ‚å¯¹ç¼–ç å‹ç¼©ç®—æ³•çš„åˆ†è¯æŠ€æœ¯çš„é€‚ç”¨æ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¡¨æ˜ï¼Œå¯¹äºWMT15ç¿»è¯‘ä»»åŠ¡ï¼Œå­è¯æ¨¡å‹æ¯”é€€é¿è¯å…¸åŸºçº¿åˆ†åˆ«æé«˜äº†1.1å’Œ1.3bLEUã€‚</p>

<h5 id="50-chung-j-cho-k-bengio-ya-character-level-decoder-without-explicit-segmentation-for-neural-machine-translationproceedings-of-the-54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-20161693-1703">[50] Chung J, Cho K, Bengio Y.A character-level decoder without explicit segmentation for neural machine translation//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1693-1703</h5>
<p>The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoderâ€“decoder with a subword-level encoder and a character-level decoder on four language pairsâ€“En-Cs, En-De, En-Ru and En-Fiâ€“ using the parallel corpora from WMTâ€™15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.
ç°æœ‰çš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œæ— è®ºæ˜¯åŸºäºçŸ­è¯­çš„è¿˜æ˜¯åŸºäºç¥ç»ç½‘ç»œçš„ï¼Œå‡ ä¹å®Œå…¨ä¾èµ–äºå…·æœ‰æ˜¾å¼åˆ†è¯çš„è¯çº§å»ºæ¨¡ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜:ç¥ç»æœºå™¨ç¿»è¯‘èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œä»»ä½•æ˜¾å¼åˆ†å‰²çš„æƒ…å†µä¸‹ç”Ÿæˆå­—ç¬¦åºåˆ—å—ï¼Ÿ ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨WMTâ€™15ä¸­çš„å¹¶è¡Œè¯­æ–™åº“ï¼Œåœ¨å››ä¸ªè¯­è¨€å¯¹en-CSã€en-DEã€en-RUå’Œen-FIä¸Šè¯„ä¼°äº†ä¸€ä¸ªå…·æœ‰å­å­—çº§ç¼–ç å™¨å’Œå­—ç¬¦çº§è§£ç å™¨çš„åŸºäºæ³¨æ„çš„ç¼–ç å™¨-è§£ç å™¨ã€‚ å®éªŒè¡¨æ˜ï¼Œåœ¨å››ç§è¯­è¨€å¯¹ä¸Šï¼Œé‡‡ç”¨å­—ç¬¦çº§è¯‘ç å™¨çš„æ¨¡å‹éƒ½ä¼˜äºé‡‡ç”¨å­å­—çº§è¯‘ç å™¨çš„æ¨¡å‹ã€‚ æ­¤å¤–ï¼Œå…·æœ‰å­—ç¬¦çº§è§£ç å™¨çš„ç¥ç»æ¨¡å‹é›†æˆåœ¨EN-CSã€EN-DEå’ŒEN-FIä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„éç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œåœ¨EN-RUä¸Šçš„æ€§èƒ½ç›¸å½“ã€‚</p>

<h5 id="51-costa-jussa-m-r-fonollosa-j-a-rcharacter-based-neural-machine-translationarxiv-preprint160300810v2-2016">[51] Costa-Jussa M R, Fonollosa J A R.Character-based neural machine translation.arXiv preprint/1603.00810v2, 2016</h5>
<p>We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå®ƒæŠŠè¾“å…¥å’Œè¾“å‡ºçš„å¥å­çœ‹æˆæ˜¯å­—ç¬¦åºåˆ—è€Œä¸æ˜¯å•è¯ã€‚ ç”±äºè¯çº§ä¿¡æ¯æä¾›äº†ä¸€ä¸ªé‡è¦çš„åå€šæºï¼Œæˆ‘ä»¬çš„è¾“å…¥æ¨¡å‹å°†å­—ç¬¦åºåˆ—çš„è¡¨ç¤ºç»„æˆè¯çš„è¡¨ç¤ºï¼ˆç”±ç©ºæ ¼è¾¹ç•Œå†³å®šï¼‰ï¼Œç„¶åä½¿ç”¨è”åˆæ³¨æ„/ç¿»è¯‘æ¨¡å‹å¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œç¿»è¯‘ã€‚ åœ¨ç›®æ ‡è¯­è¨€ä¸­ï¼Œç¿»è¯‘è¢«å»ºæ¨¡ä¸ºå•è¯å‘é‡åºåˆ—ï¼Œä½†æ˜¯æ¯ä¸ªå•è¯ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªå­—ç¬¦ï¼Œæ¡ä»¶æ˜¯æ¯ä¸ªå•è¯ä¸­çš„å‰å‡ ä¸ªå­—ç¬¦ç”Ÿæˆã€‚ ç”±äºå•è¯çš„è¡¨ç¤ºå’Œç”Ÿæˆæ˜¯åœ¨å­—ç¬¦çº§æ‰§è¡Œçš„ï¼Œå› æ­¤æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿè§£é‡Šå’Œç”Ÿæˆçœ‹ä¸è§çš„å•è¯å½¢å¼ã€‚ è¿™ç§æ–¹æ³•çš„ç¬¬äºŒä¸ªå¥½å¤„æ˜¯ï¼Œå®ƒå‡è½»äº†ä¸æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„é¢„å¤„ç†/æ ‡è®°åŒ–ç›¸å…³çš„è®¸å¤šæŒ‘æˆ˜ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè·å¾—ä¸ä¼ ç»Ÿçš„åŸºäºè¯çš„æ¨¡å‹ç›¸å½“çš„ç¿»è¯‘æ•ˆæœã€‚</p>

<h5 id="52-su-jinsong-tan-zhixing-xiong-deyi-et-allatticebased-recurrent-neural-network-encoders-for-neural-machine-translationproceedings-of-the-31st-aaai-conference-on-artificial-intelligence-aaai-2017-san-francisco-usa-20173302-3308">[52] Su Jinsong, Tan Zhixing, Xiong Deyi, et al.Latticeî€‘based recurrent neural network encoders for neural machine translation//Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI 2017) .San Francisco, USA, 2017:3302-3308</h5>
<p>Neural machine translation (NMT) heavily relies on wordlevel modelling to learn semantic representations of input sentences. However, for languages without natural word delimiters (e.g., Chinese) where input sentences have to be tokenized first, conventional NMT is confronted with two issues: 1) it is difficult to find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1-best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network (RNN) encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on Chinese-English translation demonstrate the superiorities of the proposed encoders over the conventional encoder.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè¯çº§å»ºæ¨¡æ¥å­¦ä¹ è¾“å…¥å¥å­çš„è¯­ä¹‰è¡¨ç¤ºã€‚ ç„¶è€Œï¼Œå¯¹äºæ²¡æœ‰è‡ªç„¶è¯åˆ†éš”ç¬¦çš„è¯­è¨€ï¼ˆå¦‚æ±‰è¯­ï¼‰ï¼Œè¾“å…¥å¥å­é¦–å…ˆéœ€è¦æ ‡è®°ï¼Œä¼ ç»Ÿçš„NMTé¢ä¸´ä¸¤ä¸ªé—®é¢˜:1ï¼‰éš¾ä»¥æ‰¾åˆ°æºå¥å­å»ºæ¨¡çš„æœ€ä½³æ ‡è®°ç²’åº¦ï¼›2ï¼‰1-æœ€ä½³æ ‡è®°ä¸­çš„é”™è¯¯å¯èƒ½ä¼ æ’­åˆ°NMTçš„ç¼–ç å™¨ã€‚ ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­—æ ¼çš„é€’å½’ç¥ç»ç½‘ç»œ(RNN)ç¼–ç å™¨ï¼Œå®ƒå°†æ ‡å‡†RNNæ¨å¹¿åˆ°å­—æ ¼æ‹“æ‰‘ã€‚ è¯¥ç¼–ç å™¨ä»¥ä¸€ä¸ªå­—æ ¼ä½œä¸ºè¾“å…¥ï¼Œè¯¥å­—æ ¼ç´§å‡‘åœ°ç¼–ç å¤šä¸ªä»¤ç‰ŒåŒ–ï¼Œå¹¶å­¦ä¹ ä»ä»»æ„å¤šä¸ªè¾“å…¥å’Œåœ¨å‰æ—¶é—´æ­¥ä¸­çš„éšè—çŠ¶æ€ç”Ÿæˆæ–°çš„éšè—çŠ¶æ€ã€‚ å› æ­¤ï¼ŒåŸºäºè¯æ ¼çš„ç¼–ç å™¨ä¸ä»…å‡è½»äº†æ ‡è®°é”™è¯¯çš„è´Ÿé¢å½±å“ï¼Œè€Œä¸”æ›´å…·æœ‰è¡¨ç°åŠ›å’ŒåµŒå…¥è¾“å…¥å¥å­çš„çµæ´»æ€§ã€‚ æ±‰è‹±ç¿»è¯‘å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç¼–ç å™¨ä¼˜äºä¼ ç»Ÿç¼–ç å™¨ã€‚</p>

<h5 id="53-yang-zhen-chen-wei-wang-feng-et-ala-characteraware-encoder-for-neural-machine-translationproceedings-of-the-coling-2016-the-26th-international-conference-on-computational-linguisticsosaka-japan-20163063-3070">[53] Yang Zhen, Chen Wei, Wang Feng, et al.A characteraware encoder for neural machine translation//Proceedings of the COLING 2016, the 26th International Conference on Computational Linguistics.Osaka, Japan, 2016:3063-3070</h5>
<p>This article proposes a novel character-aware neural machine translation (NMT) model that views the input sequences as sequences of characters rather than words. On the use of row convolution (Amodei et al., 2015), the encoder of the proposed model composes word-level information from the input sequences of characters automatically. Since our model doesnâ€™t rely on the boundaries between each word (as the whitespace boundaries in English), it is also applied to languages without explicit word segmentations (like Chinese). Experimental results on Chinese-English translation tasks show that the proposed character-aware NMT model can achieve comparable translation performance with the traditional word based NMT models. Despite the target side is still word based, the proposed model is able to generate much less unknown words.
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾æ„ŸçŸ¥ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è¾“å…¥åºåˆ—çœ‹æˆæ˜¯å­—ç¬¦åºåˆ—è€Œä¸æ˜¯å•è¯åºåˆ—ã€‚ åœ¨ä½¿ç”¨è¡Œå·ç§¯ï¼ˆAmodeiç­‰äººï¼Œ2015ï¼‰æ—¶ï¼Œæ‰€æå‡ºçš„æ¨¡å‹çš„ç¼–ç å™¨ä»å­—ç¬¦çš„è¾“å…¥åºåˆ—è‡ªåŠ¨åœ°åˆæˆå­—çº§ä¿¡æ¯ã€‚ ç”±äºæˆ‘ä»¬çš„æ¨¡å‹ä¸ä¾èµ–äºæ¯ä¸ªå•è¯ä¹‹é—´çš„è¾¹ç•Œï¼ˆå¦‚è‹±è¯­ä¸­çš„ç©ºæ ¼è¾¹ç•Œï¼‰ï¼Œå®ƒä¹Ÿé€‚ç”¨äºæ²¡æœ‰æ˜¾å¼åˆ†è¯çš„è¯­è¨€ï¼ˆå¦‚æ±‰è¯­ï¼‰ã€‚ åœ¨æ±‰è‹±ç¿»è¯‘ä»»åŠ¡ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„åŸºäºå­—ç¬¦æ„ŸçŸ¥çš„NMTæ¨¡å‹èƒ½å¤Ÿè·å¾—ä¸ä¼ ç»Ÿçš„åŸºäºå•è¯çš„NMTæ¨¡å‹ç›¸å½“çš„ç¿»è¯‘æ€§èƒ½ã€‚ å°½ç®¡ç›®æ ‡ç«¯ä»ç„¶æ˜¯åŸºäºå•è¯çš„ï¼Œä½†æ‰€æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´å°‘çš„æœªçŸ¥å•è¯ã€‚</p>

<h5 id="54-ling-w-trancoso-i-dyer-c-et-alcharacter-based-neural-machine-translationarxiv-preprint151104586v1-2015">[54] Ling W, Trancoso I, Dyer C, et al.Character-based neural machine translation.arXiv preprint/1511.04586v1, 2015</h5>
<p>We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models.
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå®ƒæŠŠè¾“å…¥å’Œè¾“å‡ºçš„å¥å­çœ‹æˆæ˜¯å­—ç¬¦åºåˆ—è€Œä¸æ˜¯å•è¯ã€‚ ç”±äºè¯çº§ä¿¡æ¯æä¾›äº†ä¸€ä¸ªé‡è¦çš„åå€šæºï¼Œæˆ‘ä»¬çš„è¾“å…¥æ¨¡å‹å°†å­—ç¬¦åºåˆ—çš„è¡¨ç¤ºç»„æˆè¯çš„è¡¨ç¤ºï¼ˆç”±ç©ºæ ¼è¾¹ç•Œå†³å®šï¼‰ï¼Œç„¶åä½¿ç”¨è”åˆæ³¨æ„/ç¿»è¯‘æ¨¡å‹å¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œç¿»è¯‘ã€‚ åœ¨ç›®æ ‡è¯­è¨€ä¸­ï¼Œç¿»è¯‘è¢«å»ºæ¨¡ä¸ºå•è¯å‘é‡åºåˆ—ï¼Œä½†æ˜¯æ¯ä¸ªå•è¯ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªå­—ç¬¦ï¼Œæ¡ä»¶æ˜¯æ¯ä¸ªå•è¯ä¸­çš„å‰å‡ ä¸ªå­—ç¬¦ç”Ÿæˆã€‚ ç”±äºå•è¯çš„è¡¨ç¤ºå’Œç”Ÿæˆæ˜¯åœ¨å­—ç¬¦çº§æ‰§è¡Œçš„ï¼Œå› æ­¤æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿè§£é‡Šå’Œç”Ÿæˆçœ‹ä¸è§çš„å•è¯å½¢å¼ã€‚ è¿™ç§æ–¹æ³•çš„ç¬¬äºŒä¸ªå¥½å¤„æ˜¯ï¼Œå®ƒå‡è½»äº†ä¸æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„é¢„å¤„ç†/æ ‡è®°åŒ–ç›¸å…³çš„è®¸å¤šæŒ‘æˆ˜ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè·å¾—ä¸ä¼ ç»Ÿçš„åŸºäºè¯çš„æ¨¡å‹ç›¸å½“çš„ç¿»è¯‘æ•ˆæœã€‚</p>

<h5 id="55-lee-j-cho-k-hofmann-tfully-character-level-neural-machine-translation-without-explicit-segmentationarxiv-preprint161003017v1-2016">[55] Lee J, Cho K, Hofmann T.Fully character-level neural machine translation without explicit segmentation.arXiv preprint/1610.03017v1, 2016</h5>
<p>Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subwordlevel encoder on WMTâ€™15 DE-EN and CSEN, and gives comparable performance on FIEN and RU-EN. We then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.
ç°æœ‰çš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿå¤§å¤šæ˜¯åœ¨è¯çš„å±‚æ¬¡ä¸Šè¿è¡Œçš„ï¼Œä¾èµ–äºæ˜¾å¼çš„åˆ†è¯æ¥æå–ä»¤ç‰Œã€‚ ä»‹ç»äº†ä¸€ç§ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†æºå­—ç¬¦åºåˆ—æ˜ å°„åˆ°ç›®æ ‡å­—ç¬¦åºåˆ—è€Œä¸è¿›è¡Œä»»ä½•åˆ†å‰²ã€‚ æˆ‘ä»¬ä½¿ç”¨å­—ç¬¦çº§å·ç§¯ç½‘ç»œï¼Œåœ¨ç¼–ç å™¨å¤„ä½¿ç”¨æœ€å¤§æ± æ¥ç¼©çŸ­æºè¡¨ç¤ºçš„é•¿åº¦ï¼Œä»è€Œåœ¨æ•è·å±€éƒ¨è§„å¾‹æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä»¥ä¸å­å­—çº§æ¨¡å‹ç›¸å½“çš„é€Ÿåº¦å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚ æˆ‘ä»¬çš„å­—ç¬¦å¯¹å­—ç¬¦æ¨¡å‹åœ¨WMTâ€™15DE-ENå’ŒCSENä¸Šçš„æ€§èƒ½ä¼˜äºæœ€è¿‘æå‡ºçš„ä½¿ç”¨å­å­—çº§ç¼–ç å™¨çš„åŸºçº¿ï¼Œå¹¶ä¸”åœ¨FIENå’ŒRU-ENä¸Šçš„æ€§èƒ½ç›¸å½“ã€‚ ç„¶åï¼Œæˆ‘ä»¬è¯æ˜äº†é€šè¿‡è®­ç»ƒä¸€ä¸ªå¤šå¯¹ä¸€ç¿»è¯‘ä»»åŠ¡çš„æ¨¡å‹ï¼Œè·¨å¤šä¸ªè¯­è¨€å…±äº«å•ä¸ªå­—ç¬¦çº§ç¼–ç å™¨æ˜¯å¯èƒ½çš„ã€‚ åœ¨è¿™ç§å¤šè¯­è¨€è®¾ç½®ä¸­ï¼Œå­—ç¬¦çº§ç¼–ç å™¨åœ¨æ‰€æœ‰è¯­è¨€å¯¹ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå­å­—çº§ç¼–ç å™¨ã€‚ æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨CS-ENã€FI-ENå’ŒRU-ENä¸‰ç§è¯­è¨€ä¸­ï¼Œå¤šè¯­è¨€æ–‡å­—çº§ç¿»è¯‘çš„è´¨é‡ç”šè‡³è¶…è¿‡äº†ä¸“é—¨é’ˆå¯¹è¯¥è¯­è¨€å¯¹è®­ç»ƒçš„æ¨¡å‹ï¼Œæ— è®ºæ˜¯åœ¨BLEUè¯„åˆ†è¿˜æ˜¯åœ¨äººç±»åˆ¤æ–­æ–¹é¢éƒ½æ˜¯å¦‚æ­¤ã€‚</p>

<h5 id="56-luong-m-t-le-q-v-sutskever-i-et-almulti-task-sequence-to-sequence-learningarxiv-preprint151106114v4-2015">[56] Luong M-T, Le Q V, Sutskever I, et al.Multi-task sequence to sequence learning.arXiv preprint/1511.06114v4, 2015</h5>
<p>Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting â€“ where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting â€“ useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting â€“ where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.
åºåˆ—åˆ°åºåˆ—å­¦ä¹ æ˜¯è¿‘å¹´æ¥å‡ºç°çš„ä¸€ç§æ–°çš„ç›‘ç£å­¦ä¹ æ¨¡å¼ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå®ƒçš„å¤§å¤šæ•°åº”ç”¨ç¨‹åºåªé›†ä¸­äºä¸€ä¸ªä»»åŠ¡ï¼Œè€Œæ²¡æœ‰å¤šå°‘å·¥ä½œä¸ºå¤šä¸ªä»»åŠ¡æ¢ç´¢è¿™ä¸ªæ¡†æ¶ã€‚ æœ¬æ–‡ç ”ç©¶äº†åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ä¸‰ç§å¤šä»»åŠ¡å­¦ä¹ (MTL)è®¾ç½®:(a)ä¸€å¯¹å¤šè®¾ç½®â€”â€”ç¼–ç å™¨åœ¨æœºå™¨ç¿»è¯‘å’Œå¥æ³•åˆ†æç­‰å¤šä¸ªä»»åŠ¡ä¹‹é—´å…±äº«ï¼›(b)å¤šå¯¹ä¸€è®¾ç½®â€”â€”å½“åªæœ‰è§£ç å™¨å¯ä»¥å…±äº«æ—¶æœ‰ç”¨ï¼Œä¾‹å¦‚åœ¨ç¿»è¯‘å’Œå›¾åƒå­—å¹•ç”Ÿæˆçš„æƒ…å†µä¸‹ï¼›ä»¥åŠ(c)å¤šå¯¹å¤šè®¾ç½®â€”â€”å¤šä¸ªç¼–ç å™¨å’Œè§£ç å™¨å…±äº«ï¼Œå³å¤šä¸ªç¼–ç å™¨å’Œè§£ç å™¨æ— ç›‘ç£çš„ç›®æ ‡å’Œç¿»è¯‘ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨WMTæ ‡å‡†ä¸Šï¼Œå°‘é‡è§£æå’Œå›¾åƒå­—å¹•æ•°æ®çš„è®­ç»ƒå¯ä»¥ä½¿è‹±å¾·ç¿»è¯‘è´¨é‡æé«˜1.5ä¸ªBLEUç‚¹ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å·²ç»å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„ç»“æœï¼Œå³ä½¿ç”¨93.0F1è¿›è¡Œæˆåˆ†è§£æã€‚ æœ€åï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸¤ä¸ªæ— ç›‘ç£å­¦ä¹ ç›®æ ‡:è‡ªåŠ¨ç¼–ç å™¨å’Œè·³è¿‡æ€æƒ³åœ¨MTLä¸Šä¸‹æ–‡ä¸­çš„æœ‰è¶£ç‰¹æ€§:ä¸è·³è¿‡æ€æƒ³ç›¸æ¯”ï¼Œè‡ªåŠ¨ç¼–ç å™¨åœ¨å›°æƒ‘æ–¹é¢çš„å¸®åŠ©è¾ƒå°ï¼Œä½†åœ¨BLEUåˆ†æ•°æ–¹é¢çš„å¸®åŠ©è¾ƒå¤§ã€‚</p>

<h5 id="57-dong-daxiang-wu-hua-he-wei-et-almulti-task-learning-for-multiple-language-translationproceedings-of-the-53rd-annual-meeting-of-the-association-for-computational-linguistics-and-the-7th-international-joint-conference-on-natural-language-processing-acl-2015-beijing-china-20151723-1732">[57] Dong Daxiang, Wu Hua, He Wei, et al.Multi-task learning for multiple language translation//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:1723-1732</h5>
<p>In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.
æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§èƒ½å¤ŸåŒæ—¶å°†å¥å­ä»ä¸€ç§æºè¯­è¨€ç¿»è¯‘æˆå¤šç§ç›®æ ‡è¯­è¨€çš„æœºå™¨ç¿»è¯‘æ¨¡å‹çš„å­¦ä¹ é—®é¢˜ã€‚ æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå—åˆ°äº†æœ€è¿‘æå‡ºçš„ç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹çš„å¯å‘ï¼Œè¯¥æ¨¡å‹å°†æœºå™¨ç¿»è¯‘æ¦‚æ‹¬ä¸ºä¸€ä¸ªåºåˆ—å­¦ä¹ é—®é¢˜ã€‚ æˆ‘ä»¬å°†ç¥ç»æœºå™¨ç¿»è¯‘æ‰©å±•åˆ°ä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…±äº«æºè¯­è¨€è¡¨ç¤ºï¼Œå¹¶åˆ†ç¦»ä¸åŒç›®æ ‡è¯­è¨€ç¿»è¯‘çš„å»ºæ¨¡ã€‚ æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥åº”ç”¨äºå¤§é‡å¹¶è¡Œæ•°æ®æˆ–æœ‰é™å¹¶è¡Œæ•°æ®å¯ç”¨çš„æƒ…å†µã€‚ å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹éƒ½èƒ½åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè·å¾—æ¯”å•ç‹¬å­¦ä¹ æ¨¡å‹æ›´é«˜çš„ç¿»è¯‘è´¨é‡ã€‚</p>

<h5 id="58-zoph-b-knight-kmulti-source-neural-translationarxiv-preprint160100710-2016">[58] Zoph B, Knight K.Multi-source neural translation.arXiv preprint/1601.00710, 2016</h5>
<p>We build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.
æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¤šæºæœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Œä»¥ä½¿ç»™å®šæ³•è¯­å’Œå¾·è¯­æºçš„ç›®æ ‡è‹±è¯­å­—ç¬¦ä¸²çš„æ¦‚ç‡è¾¾åˆ°æœ€å¤§ã€‚ ä½¿ç”¨ç¥ç»ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡ ç§ç»„åˆæ–¹æ³•ï¼Œå¹¶åœ¨ä¸€ä¸ªéå¸¸å¼ºçš„åŸºäºæ³¨æ„çš„ç¥ç»ç¿»è¯‘æ¨¡å‹ä¸ŠæŠ¥å‘Šäº†é«˜è¾¾+4.8bleuçš„å¢åŠ ã€‚</p>

<h5 id="59-firat-o-cho-k-bengio-ymulti-way-multilingual-neural-machine-translation-with-a-shared-attention-mechanismproceedings-of-the-15th-annual-conference-of-the-north-american-chapter-of-the-association-for-computational-linguistics-naacl-2016-san-diego-usa-2016866-875">[59] Firat O, Cho K, Bengio Y.Multi-way, multilingual neural machine translation with a shared attention mechanism//Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016) .San Diego, USA, 2016:866-875</h5>
<p>We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMTâ€™15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.
æˆ‘ä»¬æå‡ºäº†å¤šé€”å¾„ã€å¤šè¯­è¨€çš„ç¥ç»æœºå™¨ç¿»è¯‘ã€‚ è¯¥æ–¹æ³•ä½¿å¾—å•ä¸ªç¥ç»ç¿»è¯‘æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªè¯­è¨€ä¹‹é—´è¿›è¡Œç¿»è¯‘ï¼Œå¹¶ä¸”å…·æœ‰ä»…éšè¯­è¨€æ•°é‡çº¿æ€§å¢é•¿çš„å¤šä¸ªå‚æ•°ã€‚ é€šè¿‡åœ¨æ‰€æœ‰è¯­è¨€å¯¹ä¹‹é—´å…±äº«ä¸€ä¸ªæ³¨æ„æœºåˆ¶ï¼Œè¿™æ˜¯å¯èƒ½çš„ã€‚ æˆ‘ä»¬åŒæ—¶åœ¨WMTâ€™15çš„10ä¸ªè¯­è¨€å¯¹ä¸Šè®­ç»ƒäº†æ‰€æå‡ºçš„å¤šè·¯ã€å¤šè¯­è¨€æ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿåˆ°ä¸ä»…åœ¨ä¸€ä¸ªè¯­è¨€å¯¹ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½æœ‰æ˜æ˜¾çš„æé«˜ã€‚ ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å‘ç°è¯¥æ¨¡å‹æ˜¾è‘—æé«˜äº†ä½èµ„æºè¯­è¨€å¯¹çš„ç¿»è¯‘è´¨é‡ã€‚</p>

<h5 id="60-johnson-m-schuster-m-le-q-v-et-algoogles-multilingual-neural-machine-translation-systemenabling-zero-shot-translationtransactions-of-the-association-for-computational-linguistics-2017-5339-351">[60] Johnson M, Schuster M, Le Q V, et al.Googleâ€™s multilingual neural machine translation system:Enabling zero-shot translation.Transactions of the Association for Computational Linguistics, 2017, 5:339-351</h5>
<p>We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes an encoder, decoder and attention module, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. On the WMTâ€™14 benchmarks, a single multilingual model achieves comparable performance for Englishâ†’French and surpasses state-of-the-art results for Englishâ†’German. Similarly, a single multilingual model surpasses state-of-the-art results for Frenchâ†’English and Germanâ†’English on WMTâ€™14 and WMTâ€™15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨å•ä¸€çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¹‹é—´è¿›è¡Œç¿»è¯‘ã€‚ æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆä¸éœ€è¦å¯¹æ ‡å‡†NMTç³»ç»Ÿçš„æ¨¡å‹ä½“ç³»ç»“æ„è¿›è¡Œæ›´æ”¹ï¼Œè€Œæ˜¯åœ¨è¾“å…¥è¯­å¥çš„å¼€å¤´å¼•å…¥ä¸€ä¸ªäººå·¥æ ‡è®°æ¥æŒ‡å®šæ‰€éœ€çš„ç›®æ ‡è¯­è¨€ã€‚ æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ï¼ŒåŒ…æ‹¬ç¼–ç å™¨ã€è§£ç å™¨å’Œæ³¨æ„æ¨¡å—ï¼Œä¿æŒä¸å˜ï¼Œå¹¶åœ¨æ‰€æœ‰è¯­è¨€ä¸­å…±äº«ã€‚ ä½¿ç”¨å…±äº«çš„è¯æ¡è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨å•ä¸ªæ¨¡å‹å®ç°å¤šè¯­è¨€NMTï¼Œè€Œä¸å¢åŠ ä»»ä½•å‚æ•°ï¼Œè¿™æ¯”ä»¥å‰çš„å¤šè¯­è¨€NMTææ¡ˆè¦ç®€å•å¾—å¤šã€‚ åœ¨WMTâ€™14åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸€ä¸ªå•ä¸€çš„å¤šè¯­è¨€æ¨¡å‹åœ¨è‹±è¯­â†’æ³•è¯­çš„æµ‹è¯•ä¸­å–å¾—äº†å¯æ¯”çš„æ€§èƒ½ï¼Œå¹¶è¶…è¿‡äº†è‹±è¯­â†’å¾·è¯­çš„æœ€æ–°æµ‹è¯•ç»“æœã€‚ åŒæ ·ï¼Œåœ¨WMTâ€™14å’ŒWMTâ€™15åŸºå‡†ä¸Šï¼Œä¸€ä¸ªå•ä¸€çš„å¤šè¯­ç§æ¨¡å‹è¶…è¿‡äº†æ³•è¯­â†’è‹±è¯­å’Œå¾·è¯­â†’è‹±è¯­çš„æœ€æ–°ç»“æœã€‚ åœ¨ç”Ÿäº§è¯­æ–™åº“ä¸­ï¼Œå¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤šè¯­ç§å¤š é™¤äº†æé«˜æ¨¡å‹è®­ç»ƒè¯­è¨€å¯¹çš„ç¿»è¯‘è´¨é‡å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å¯ä»¥å­¦ä¹ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»æœªè§è¿‡çš„è¯­è¨€å¯¹ä¹‹é—´æ‰§è¡Œéšå¼æ¡¥æ¥ï¼Œè¿™è¡¨æ˜è¿ç§»å­¦ä¹ å’Œé›¶é•œå¤´ç¿»è¯‘å¯¹äºç¥ç»ç¿»è¯‘æ˜¯å¯èƒ½çš„ã€‚ æœ€åï¼Œæˆ‘ä»¬ç»™å‡ºäº†æ¨¡å‹ä¸­ä¸€ä¸ªé€šç”¨çš„ä¸­é—´è¯­è¨€è¡¨ç¤ºçš„åˆ†æï¼Œå¹¶ç»™å‡ºäº†æ··åˆè¯­è¨€æ—¶ä¸€äº›æœ‰è¶£çš„ä¾‹å­ã€‚</p>

<h5 id="61-gulcehre-c-ahn-s-nallapati-r-et-alpointing-the-unknown-wordsproceedings-of-the-54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-2016140-149">[61] Gulcehre C, Ahn S, Nallapati R, et al.Pointing the unknown words//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:140-149</h5>
<p>The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both the traditional countbased and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context. We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.
ç¨€æœ‰è¯å’ŒæœªçŸ¥è¯é—®é¢˜æ˜¯å½±å“è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæ€§èƒ½çš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„åŸºäºè®¡æ•°çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿå’Œæ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ³¨æ„åŠ›çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ç¨€æœ‰è¯å’Œéšå½¢è¯çš„å¤„ç†æ–¹æ³•ã€‚ æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ä¸¤ä¸ªSoftMaxå±‚æ¥é¢„æµ‹æ¡ä»¶è¯­è¨€æ¨¡å‹ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯:ä¸€ä¸ªé¢„æµ‹å•è¯åœ¨æºå¥ä¸­çš„ä½ç½®ï¼Œå¦ä¸€ä¸ªé¢„æµ‹å•è¯åœ¨å…¥å›´è¯æ±‡è¡¨ä¸­çš„ä½ç½®ã€‚ åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ï¼Œç”±MLPæ ¹æ®ä¸Šä¸‹æ–‡è‡ªé€‚åº”åœ°é€‰æ‹©è¦ä½¿ç”¨å“ªä¸ªSoftMaxå±‚ã€‚ æˆ‘ä»¬å·¥ä½œçš„åŠ¨æœºæ¥è‡ªä¸€ä¸ªå¿ƒç†å­¦è¯æ®ï¼Œå³å½“ä¸€ä¸ªç‰©ä½“çš„åå­—ä¸ä¸ºäººæ‰€çŸ¥æ—¶ï¼Œäººç±»è‡ªç„¶å€¾å‘äºæŒ‡å‘ä¸Šä¸‹æ–‡æˆ–ç¯å¢ƒä¸­çš„ç‰©ä½“ã€‚ æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸¤ä¸ªä»»åŠ¡çš„æ”¹è¿›ï¼Œä¸€ä¸ªæ˜¯Europarlè‹±æ³•å¹³è¡Œè¯­æ–™åº“çš„ç¥ç»æœºå™¨ç¿»è¯‘ï¼Œå¦ä¸€ä¸ªæ˜¯åƒå…†å­—æ•°æ®é›†çš„æ–‡æœ¬æ‘˜è¦ã€‚</p>

<h5 id="62-luong-m-t-sutskever-i-le-q-v-et-aladdressing-the-rare-word-problem-in-neural-machine-translationproceedings-of-the-53rd-annual-meeting-of-the-association-for-computational-linguistics-and-the-7th-international-joint-conference-on-natural-language-processing-acl-2015-beijing-china-201511-19">[62] Luong M-T, Sutskever I, Le Q V, et al.Addressing the rare word problem in neural machine translation//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:11-19</h5>
<p>Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMTâ€™14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMTâ€™14 contest task.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰æ˜¯ä¸€ç§æ–°çš„æœºå™¨ç¿»è¯‘æ–¹æ³•ï¼Œå…¶ç»“æœä¸ä¼ ç»Ÿçš„æœºå™¨ç¿»è¯‘æ–¹æ³•ç›¸å½“ã€‚ ä¼ ç»Ÿçš„NMTç³»ç»Ÿçš„ä¸€ä¸ªæ˜¾ç€ç¼ºç‚¹æ˜¯ä¸èƒ½æ­£ç¡®ç¿»è¯‘éå¸¸ç½•è§çš„å•è¯:ç«¯åˆ°ç«¯NMTçš„è¯æ±‡é‡ç›¸å¯¹è¾ƒå°‘ï¼Œåªæœ‰ä¸€ä¸ªUNKç¬¦å·æ¥è¡¨ç¤ºæ¯ä¸€ä¸ªå¯èƒ½çš„è¯æ±‡é‡ä¸è¶³(OOV)çš„å•è¯ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå¹¶å®ç°äº†ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ æˆ‘ä»¬å¯¹ä¸€ä¸ªNMTç³»ç»Ÿè¿›è¡Œæ•°æ®è®­ç»ƒï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä¸€ä¸ªè¯å¯¹é½ç®—æ³•çš„è¾“å‡ºæ¥å¢å¼ºï¼Œä»è€Œå…è®¸NMTç³»ç»Ÿé’ˆå¯¹ç›®æ ‡å¥å­ä¸­çš„æ¯ä¸ªOOVè¯å‘å‡ºå…¶å¯¹åº”è¯åœ¨æºå¥å­ä¸­çš„ä½ç½®ã€‚ è¯¥ä¿¡æ¯éšååœ¨ä½¿ç”¨å­—å…¸ç¿»è¯‘æ¯ä¸ªOOVå•è¯çš„åå¤„ç†æ­¥éª¤ä¸­ä½¿ç”¨ã€‚ æˆ‘ä»¬åœ¨WMTâ€™14è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¸ä½¿ç”¨è¯¥æŠ€æœ¯çš„ç­‰æ•ˆNMTç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æä¾›äº†é«˜è¾¾2.8ä¸ªBLEUç‚¹çš„æ˜¾è‘—æ”¹è¿›ã€‚ æˆ‘ä»¬çš„NMTç³»ç»Ÿæœ‰37.5ä¸ªBLEUåˆ†ï¼Œæ˜¯ç¬¬ä¸€ä¸ªè¶…è¿‡WMTâ€™14ç«èµ›ä»»åŠ¡æœ€ä½³æˆç»©çš„ç³»ç»Ÿã€‚</p>

<h5 id="63-li-xiaoqing-zhang-jiajun-zong-chengqingtowards-zero-unknown-word-in-neural-machine-translationproceedings-of-the-25th-international-joint-conference-on-artificial-intelligence-ijcai-2016-new-york-usa-20162852-2858">[63] Li Xiaoqing, Zhang Jiajun, Zong Chengqing.Towards zero unknown word in neural machine translation//Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI 2016) .New York, USA, 2016:2852-2858</h5>
<p>Neural Machine translation has shown promising results in recent years. In order to control the computational complexity, NMT has to employ a small vocabulary, and massive rare words outside the vocabulary are all replaced with a single unk symbol. Besides the inability to translate rare words, this kind of simple approach leads to much increased ambiguity of the sentences since meaningless unks break the structure of sentences, and thus hurts the translation and reordering of the in-vocabulary words. To tackle this problem, we propose a novel substitution-translation-restoration method. In substitution step, the rare words in a testing sentence are replaced with similar in-vocabulary words based on a similarity model learnt from monolingual data. In translation and restoration steps, the sentence will be translated with a model trained on new bilingual data with rare words replaced, and finally the translations of the replaced words will be substituted by that of original ones. Experiments on Chinese-to-English translation demonstrate that our proposed method can achieve more than 4 BLEU points over the attention-based NMT. When compared to the recently proposed method handling rare words in NMT, our method can also obtain an improvement by nearly 3 BLEU points.
è¿‘å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘å–å¾—äº†ä»¤äººé¼“èˆçš„æˆæœã€‚ ä¸ºäº†æ§åˆ¶è®¡ç®—å¤æ‚åº¦ï¼ŒNMTå¿…é¡»ä½¿ç”¨ä¸€ä¸ªå°è¯æ±‡è¡¨ï¼Œè¯æ±‡è¡¨ä¹‹å¤–çš„å¤§é‡ç¨€æœ‰è¯æ±‡éƒ½è¢«ä¸€ä¸ªUNKç¬¦å·æ›¿æ¢ã€‚ è¿™ç§ç®€å•çš„æ–¹æ³•é™¤äº†ä¸èƒ½ç¿»è¯‘ç¨€æœ‰è¯æ±‡å¤–ï¼Œè¿˜ä¼šå¯¼è‡´å¥å­çš„æ­§ä¹‰æ€§å¤§å¤§å¢åŠ ï¼Œå› ä¸ºæ— æ„ä¹‰çš„unksç ´åäº†å¥å­çš„ç»“æ„ï¼Œä»è€ŒæŸå®³äº†è¯æ±‡ä¸­è¯æ±‡çš„ç¿»è¯‘å’Œé‡æ–°æ’åºã€‚ é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ›¿æ¢-å¹³ç§»-æ¢å¤æ–¹æ³•ã€‚ åœ¨æ›¿æ¢æ­¥éª¤ä¸­ï¼ŒåŸºäºä»å•è¯­æ•°æ®å­¦ä¹ çš„ç›¸ä¼¼æ€§æ¨¡å‹ï¼Œå°†æµ‹è¯•å¥ä¸­çš„ç¨€æœ‰è¯æ›¿æ¢ä¸ºè¯æ±‡ä¸­çš„ç›¸ä¼¼è¯ã€‚ åœ¨ç¿»è¯‘å’Œæ¢å¤æ­¥éª¤ä¸­ï¼Œç”¨ä¸€ä¸ªæ–°çš„åŒè¯­æ•°æ®è®­ç»ƒæ¨¡å‹å¯¹å¥å­è¿›è¡Œç¿»è¯‘ï¼Œå°†ç¨€æœ‰è¯æ›¿æ¢ï¼Œæœ€åå°†æ›¿æ¢è¯çš„ç¿»è¯‘æ›¿æ¢ä¸ºåŸå§‹è¯çš„ç¿»è¯‘ã€‚ æ±‰è‹±ç¿»è¯‘å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•æ¯”åŸºäºæ³¨æ„åŠ›çš„NMTèƒ½è·å¾—4ä¸ªä»¥ä¸Šçš„BLEUç‚¹ã€‚ ä¸æ–°è¿‘æå‡ºçš„å¤„ç†NMTä¸­ç¨€æœ‰è¯çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥è·å¾—è¿‘3ä¸ªBLEUç‚¹çš„æ”¹è¿›ã€‚</p>

<h5 id="64-hirschmann-f-nam-j-furnkranz-jwhat-makes-word-level-neural-machine-translation-harda-case-study-on-englishgerman-translationproceedings-of-the-coling-2016-the26th-international-conference-on-computational-linguisticsosaka-japan-20163199-3208">[64] Hirschmann F, Nam J, Furnkranz J.What makes word-level neural machine translation hard:A case study on EnglishGerman translation//Proceedings of the COLING 2016, the26th International Conference on Computational Linguistics.Osaka, Japan, 2016:3199-3208</h5>
<p>Traditional machine translation systems often require heavy feature engineering and the combination of multiple techniques for solving different subproblems. In recent years, several endto-end learning architectures based on recurrent neural networks have been proposed. Unlike traditional systems, Neural Machine Translation (NMT) systems learn the parameters of the model and require only minimal preprocessing. Memory and time constraints allow to take only a fixed number of words into account, which leads to the out-of-vocabulary (OOV) problem. In this work, we analyze why the OOV problem arises and why it is considered a serious problem in German. We study the effectiveness of compound word splitters for alleviating the OOV problem, resulting in a 2.5+ BLEU points improvement over a baseline on the WMTâ€™14 German-to-English translation task. For English-to-German translation, we use target-side compound splitting through a special syntax during training that allows the model to merge compound words and gain 0.2 BLEU points.
ä¼ ç»Ÿçš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿå¾€å¾€éœ€è¦å¤§é‡çš„ç‰¹å¾å·¥ç¨‹å’Œå¤šç§æŠ€æœ¯çš„ç»“åˆæ¥è§£å†³ä¸åŒçš„å­é—®é¢˜ã€‚ è¿‘å¹´æ¥ï¼Œæå‡ºäº†å‡ ç§åŸºäºé€’å½’ç¥ç»ç½‘ç»œçš„ç«¯åˆ°ç«¯å­¦ä¹ ç»“æ„ã€‚ ä¸ä¼ ç»Ÿç³»ç»Ÿä¸åŒï¼Œç¥ç»æœºå™¨ç¿»è¯‘(NMT)ç³»ç»Ÿå­¦ä¹ æ¨¡å‹çš„å‚æ•°ï¼Œåªéœ€æœ€å°çš„é¢„å¤„ç†ã€‚ å†…å­˜å’Œæ—¶é—´é™åˆ¶åªå…è®¸è€ƒè™‘å›ºå®šçš„å­—æ•°ï¼Œè¿™å¯¼è‡´äº†è¯æ±‡é‡ä¸è¶³(OOV)é—®é¢˜ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†OOVé—®é¢˜äº§ç”Ÿçš„åŸå› ï¼Œä»¥åŠä¸ºä»€ä¹ˆå®ƒåœ¨å¾·è¯­ä¸­è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªä¸¥é‡çš„é—®é¢˜ã€‚ æˆ‘ä»¬ç ”ç©¶äº†å¤åˆåˆ†è¯å™¨åœ¨ç¼“è§£OOVé—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œåœ¨WMTâ€™14å¾·è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œå¤åˆåˆ†è¯å™¨æ¯”åŸºçº¿æé«˜äº†2.5+BLEUç‚¹ã€‚ å¯¹äºè‹±å¾·ç¿»è¯‘ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç›®æ ‡ç«¯å¤åˆå¥æ³•æ‹†åˆ†çš„æ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆå¹¶å¤åˆè¯å¹¶è·å¾—0.2ä¸ªBLEUç‚¹ã€‚</p>

<h5 id="65-mi-haitao-wang-zhiguo-ittycheriah-avocabulary-manipulation-for-neural-machine-translationproceedings-of-the54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-2016124-129">[65] Mi Haitao, Wang Zhiguo, Ittycheriah A.Vocabulary manipulation for neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:124-129</h5>
<p>In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentencelevel or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordto-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).
ä¸ºäº†æ•æ‰ä¸°å¯Œçš„è¯­è¨€ç°è±¡ï¼Œç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹å¿…é¡»ä½¿ç”¨å¤§é‡çš„è¯æ±‡é‡ï¼Œè¿™å°±éœ€è¦è¾ƒé«˜çš„è®¡ç®—æ—¶é—´å’Œè¾ƒå¤§çš„å†…å­˜ä½¿ç”¨é‡ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å¥å­çº§æˆ–æ‰¹å¤„ç†çº§è¯æ±‡æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œè¿™åªæ˜¯æ•´ä¸ªè¾“å‡ºè¯æ±‡çš„ä¸€ä¸ªéå¸¸å°çš„å­é›†ã€‚ å¯¹äºæ¯ä¸ªå¥å­æˆ–æ‰¹æ¬¡ï¼Œæˆ‘ä»¬åªé¢„æµ‹å…¶å¥å­çº§æˆ–æ‰¹æ¬¡çº§è¯æ±‡ä¸­çš„ç›®æ ‡è¯ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬å‡å°‘äº†è®¡ç®—æ—¶é—´å’Œå†…å­˜ä½¿ç”¨ã€‚ è¯¥æ–¹æ³•ç®€å•åœ°è€ƒè™‘äº†æºå¥ä¸­æ¯ä¸ªå•è¯æˆ–çŸ­è¯­çš„ç¿»è¯‘é€‰é¡¹ï¼Œå¹¶åŸºäºå•è¯åˆ°å•è¯çš„ç¿»è¯‘æ¨¡å‹æˆ–ä»ä¼ ç»Ÿæœºå™¨ç¿»è¯‘æ¨¡å‹ä¸­å­¦ä¹ çš„åŒè¯­çŸ­è¯­åº“ä¸ºæ¯ä¸ªå¥å­é€‰æ‹©éå¸¸å°çš„ç›®æ ‡è¯æ±‡ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œä¸Jeanç­‰äººçš„å¤§è¯æ±‡é‡ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„ç¿»è¯‘æ€§èƒ½æé«˜äº†1ä¸ªbleuç‚¹ã€‚ ï¼ˆ2015å¹´ï¼‰ã€‚</p>

<h5 id="66-luong-m-t-manning-c-dachieving-open-vocabulary-neural-machine-translation-with-hybrid-word-character-modelsproceedings-of-the-54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-20161054-1063">[66] Luong M-T, Manning C D.Achieving open vocabulary neural machine translation with hybrid word-character models//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1054-1063</h5>
<p>Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel wordcharacter solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMTâ€™15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1âˆ’11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.
å‡ ä¹æ‰€æœ‰å…³äºç¥ç»æœºå™¨ç¿»è¯‘(NMT)çš„å‰æœŸå·¥ä½œéƒ½ä½¿ç”¨äº†éå¸¸æœ‰é™çš„è¯æ±‡ï¼Œå¯èƒ½è¿˜æœ‰ä¸€ç§åç»­çš„æ–¹æ³•æ¥ä¿®è¡¥æœªçŸ¥çš„å•è¯ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å®ç°å¼€æ”¾å¼è¯æ±‡NMTçš„WordCharacterè§£å†³æ–¹æ¡ˆã€‚ æˆ‘ä»¬æ„å»ºäº†æ··åˆç³»ç»Ÿï¼Œä¸»è¦åœ¨å•è¯çº§ç¿»è¯‘ï¼Œå¹¶å‚è€ƒç¨€æœ‰å•è¯çš„å­—ç¬¦æˆåˆ†ã€‚ æˆ‘ä»¬çš„å­—ç¬¦çº§é€’å½’ç¥ç»ç½‘ç»œè®¡ç®—æºè¯è¡¨ç¤ºï¼Œå¹¶åœ¨éœ€è¦æ—¶æ¢å¤æœªçŸ¥çš„ç›®æ ‡è¯ã€‚ è¿™ç§æ··åˆæ–¹æ³•çš„åŒé‡ä¼˜ç‚¹æ˜¯ï¼Œå®ƒæ¯”åŸºäºå­—ç¬¦çš„æ–¹æ³•è®­ç»ƒå¾—å¿«å¾—å¤šï¼Œä¹Ÿå®¹æ˜“å¾—å¤šï¼› åŒæ—¶ï¼Œå®ƒä»ä¸åƒåŸºäºå•è¯çš„æ¨¡å‹é‚£æ ·ç”ŸæˆæœªçŸ¥å•è¯ã€‚ åœ¨WMTâ€™15è‹±è¯­åˆ°æ·å…‹çš„ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè¿™ç§æ··åˆæ–¹æ³•æ¯”å·²ç»å¤„ç†æœªçŸ¥å•è¯çš„æ¨¡å‹å¢åŠ äº†+2.1-11.4ä¸ªBLEUç‚¹ã€‚ æˆ‘ä»¬æœ€å¥½çš„ç³»ç»Ÿè¾¾åˆ°äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„ç»“æœï¼Œ20.7 BLEUåˆ†æ•°ã€‚ æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„å­—ç¬¦æ¨¡å‹ä¸ä»…å¯ä»¥æˆåŠŸåœ°ä¸ºæ·å…‹è¯­ç”Ÿæˆç»“æ„è‰¯å¥½çš„å•è¯ï¼Œè€Œä¸”å¯ä»¥ä¸ºè‹±è¯­æºè¯å»ºç«‹æ­£ç¡®çš„è¡¨ç¤ºã€‚æ·å…‹è¯­æ˜¯ä¸€ç§é«˜åº¦å±ˆæŠ˜å˜åŒ–çš„è¯­è¨€ï¼Œè¯æ±‡éå¸¸å¤æ‚ã€‚</p>

<h5 id="67-chitnis-r-denero-jvariable-length-word-encodings-for-neural-translation-modelsproceedings-of-the-2015conference-on-empirical-methods-in-natural-language-processing-emnlp-2015-lisbon-portugal-20152088-2093">[67] Chitnis R, DeNero J.Variable-length word encodings for neural translation models//Proceedings of the 2015Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) .Lisbon, Portugal, 2015:2088-2093</h5>
<p>Recent work in neural machine translation has shown promising performance, but the most effective architectures do not scale naturally to large vocabulary sizes. We propose and compare three variable-length encoding schemes that represent a large vocabulary corpus using a much smaller vocabulary with no loss in information. Common words are unaffected by our encoding, but rare words are encoded using a sequence of two pseudo-words. Our method is simple and effective: it requires no complete dictionaries, learning procedures, increased training time, changes to the model, or new parameters. Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU.
æœ€è¿‘åœ¨ç¥ç»æœºå™¨ç¿»è¯‘æ–¹é¢çš„å·¥ä½œæ˜¾ç¤ºäº†å¾ˆå¥½çš„æ€§èƒ½ï¼Œä½†æ˜¯æœ€æœ‰æ•ˆçš„ä½“ç³»ç»“æ„å¹¶ä¸èƒ½è‡ªç„¶åœ°æ‰©å±•åˆ°å¤§çš„è¯æ±‡é‡ã€‚ æˆ‘ä»¬æå‡ºå¹¶æ¯”è¾ƒäº†ä¸‰ç§å˜é•¿ç¼–ç æ–¹æ¡ˆï¼Œå®ƒä»¬ä½¿ç”¨æ›´å°çš„è¯æ±‡é‡æ¥è¡¨ç¤ºå¤§è¯æ±‡é‡çš„è¯­æ–™åº“ï¼Œå¹¶ä¸”æ²¡æœ‰ä¿¡æ¯æŸå¤±ã€‚ æ™®é€šå•è¯ä¸å—æˆ‘ä»¬ç¼–ç çš„å½±å“ï¼Œä½†ç¨€æœ‰å•è¯ä½¿ç”¨ä¸¤ä¸ªä¼ªå•è¯çš„åºåˆ—è¿›è¡Œç¼–ç ã€‚ æˆ‘ä»¬çš„æ–¹æ³•ç®€å•æœ‰æ•ˆ:å®ƒä¸éœ€è¦å®Œæ•´çš„å­—å…¸ã€å­¦ä¹ è¿‡ç¨‹ã€å¢åŠ çš„è®­ç»ƒæ—¶é—´ã€å¯¹æ¨¡å‹çš„æ›´æ”¹æˆ–æ–°çš„å‚æ•°ã€‚ ä¸ç”¨æœªçŸ¥å•è¯ç¬¦å·æ›¿æ¢æ‰€æœ‰ç¨€æœ‰å•è¯çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æœ€ä½³å¯å˜é•¿åº¦ç¼–ç ç­–ç•¥å°†WMTè‹±æ³•ç¿»è¯‘æ€§èƒ½æé«˜äº†1.7bleuã€‚</p>

<h5 id="68-pouget-abadie-j-bahdanau-d-van-merrienboer-b-et-alovercoming-the-curse-of-sentence-length-for-neural-machine-translation-using-automatic-segmentationproceedings-of-the-ssst-8-eighth-workshop-on-syntax-semantics-and-structure-in-statistical-translation-ssst-8-doha-qatar-201478-85">[68] Pouget-Abadie J, Bahdanau D, van Merrienboer B, et al.Overcoming the curse of sentence length for neural machine translation using automatic segmentation//Proceedings of the SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8) .Doha, Qatar, 2014:78-85</h5>
<p>The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.
ï¼ˆChoç­‰äººï¼Œ2014a)çš„ä½œè€…å·²ç»è¡¨æ˜ï¼Œä¸ç°æœ‰çš„åŸºäºçŸ­è¯­çš„ç¿»è¯‘ç³»ç»Ÿä¸åŒï¼Œæœ€è¿‘å¼•å…¥çš„ç¥ç»ç½‘ç»œç¿»è¯‘ç³»ç»Ÿåœ¨ç¿»è¯‘é•¿å¥å­æ—¶çš„ç¿»è¯‘è´¨é‡æ˜¾è‘—ä¸‹é™ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†è¾“å…¥å¥å­è‡ªåŠ¨åˆ†å‰²æˆæ˜“äºç¥ç»ç½‘ç»œç¿»è¯‘æ¨¡å‹ç¿»è¯‘çš„çŸ­è¯­çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ ä¸€æ—¦ç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹ç‹¬ç«‹åœ°ç¿»è¯‘äº†æ¯ä¸ªç‰‡æ®µï¼Œç¿»è¯‘çš„å­å¥å°±è¢«è¿æ¥èµ·æ¥å½¢æˆæœ€ç»ˆçš„ç¿»è¯‘ã€‚ å®è¯ç»“æœè¡¨æ˜ï¼Œé•¿å¥çš„ç¿»è¯‘è´¨é‡æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚</p>

<h5 id="69-he-wei-he-zhongjun-wu-hua-et-alimproved-neural-machine-translation-with-smt-featuresproceedings-of-the30th-aaai-conference-on-artificial-intelligence-aaai2016-phoenix-usa-2016151-157">[69] He Wei, He Zhongjun, Wu Hua, et al.Improved neural machine translation with SMT features//Proceedings of the30th AAAI Conference on Artificial Intelligence (AAAI2016) .Phoenix, USA, 2016:151-157</h5>
<p>Neural machine translation (NMT) conducts end-to-end translation with a source language encoder and a target language decoder, making promising translation performance. However, as a newly emerged approach, the method has some limitations. An NMT system usually has to apply a vocabulary of certain size to avoid the time-consuming training and decoding, thus it causes a serious out-of-vocabulary problem. Furthermore, the decoder lacks a mechanism to guarantee all the source words to be translated and usually favors short translations, resulting in fluent but inadequate translations. In order to solve the above problems, we incorporate statistical machine translation (SMT) features, such as a translation model and an n-gram language model, with the NMT model under the log-linear framework. Our experiments show that the proposed method significantly improves the translation quality of the state-of-the-art NMT system on Chinese-toEnglish translation tasks. Our method produces a gain of up to 2.33 BLEU score on NIST open test sets.
ç¥ç»æœºå™¨ç¿»è¯‘(NMT)åˆ©ç”¨æºè¯­è¨€ç¼–ç å™¨å’Œç›®æ ‡è¯­è¨€è§£ç å™¨è¿›è¡Œç«¯åˆ°ç«¯ç¿»è¯‘ï¼Œå…·æœ‰è‰¯å¥½çš„ç¿»è¯‘æ€§èƒ½ã€‚ ç„¶è€Œï¼Œä½œä¸ºä¸€ç§æ–°å…´çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚ NMTç³»ç»Ÿé€šå¸¸éœ€è¦ä½¿ç”¨ä¸€å®šæ•°é‡çš„è¯æ±‡æ¥é¿å…è€—æ—¶çš„è®­ç»ƒå’Œè§£ç ï¼Œä»è€Œé€ æˆä¸¥é‡çš„è¯æ±‡é‡ä¸è¶³é—®é¢˜ã€‚ æ­¤å¤–ï¼Œè§£ç å™¨ç¼ºä¹ä¿è¯æ‰€æœ‰æºè¯è¢«ç¿»è¯‘çš„æœºåˆ¶ï¼Œå¹¶ä¸”é€šå¸¸åçˆ±çŸ­ç¿»è¯‘ï¼Œä»è€Œå¯¼è‡´æµç•…ä½†ä¸å……åˆ†çš„ç¿»è¯‘ã€‚ ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å¯¹æ•°çº¿æ€§æ¡†æ¶ä¸‹ï¼Œå°†ç»Ÿè®¡æœºå™¨ç¿»è¯‘(SMT)ç‰¹å¾ï¼ˆå¦‚ç¿»è¯‘æ¨¡å‹å’ŒN-gramè¯­è¨€æ¨¡å‹ï¼‰ä¸NMTæ¨¡å‹ç›¸ç»“åˆã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç›®å‰æœ€å…ˆè¿›çš„NMTç³»ç»Ÿå¯¹æ±‰è‹±ç¿»è¯‘ä»»åŠ¡çš„ç¿»è¯‘è´¨é‡ã€‚ æˆ‘ä»¬çš„æ–¹æ³•åœ¨NISTå¼€æ”¾æµ‹è¯•é›†ä¸Šäº§ç”Ÿé«˜è¾¾2.33BLEUåˆ†æ•°çš„å¢ç›Šã€‚</p>

<h5 id="70-wang-xing-lu-zhengdong-tu-zhaopeng-et-alneural-machine-translation-advised-by-statistical-machine-translationproceedings-of-the-31st-aaai-conference-on-artificial-intelligence-aaai-2017-san-francisco-usa-20173330-3336">[70] Wang Xing, Lu Zhengdong, Tu Zhaopeng, et al.Neural machine translation advised by statistical machine translation//Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI 2017) .San Francisco, USA, 2017:3330-3336</h5>
<p>Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu et al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰æ˜¯è¿‘å¹´æ¥å‘å±•èµ·æ¥çš„ä¸€ç§æ–°çš„æœºå™¨ç¿»è¯‘æ–¹æ³•ã€‚ ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒNMTé€šå¸¸äº§ç”Ÿæµåˆ©ä½†ä¸å……åˆ†çš„ç¿»è¯‘ï¼ˆTuç­‰äººï¼Œ2016bï¼›Tuç­‰äººï¼Œ2016aï¼›Heç­‰äººï¼Œ2016ï¼›Tuç­‰äººï¼Œ2017ï¼‰ã€‚ è¿™ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘(SMT)å½¢æˆå¯¹æ¯”ï¼Œåè€…é€šå¸¸äº§ç”Ÿè¶³å¤Ÿä½†ä¸æµåˆ©çš„ç¿»è¯‘ã€‚ å› æ­¤ï¼Œåˆ©ç”¨è¿™ä¸¤ç§æ¨¡å‹çš„ä¼˜ç‚¹è¿›è¡Œæ›´å¥½çš„ç¿»è¯‘æ˜¯å¾ˆè‡ªç„¶çš„ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®å°†SMTæ¨¡å‹åˆå¹¶åˆ°NMTæ¡†æ¶ä¸­ã€‚ æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨æ¯ä¸ªè§£ç æ­¥éª¤ï¼ŒSMTåŸºäºæ¥è‡ªNMTçš„è§£ç ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œç”Ÿæˆçš„éƒ¨åˆ†ç¿»è¯‘å’Œæ³¨æ„å†å²ï¼‰æä¾›ç”Ÿæˆå­—çš„é™„åŠ æ¨èã€‚ ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¾…åŠ©åˆ†ç±»å™¨å¯¹SMTæ¨èè¿›è¡Œè¯„åˆ†ï¼Œå¹¶ä½¿ç”¨é€‰é€šå‡½æ•°å°†SMTæ¨èä¸NMTç”Ÿæˆç›¸ç»“åˆï¼Œä¸¤è€…éƒ½ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼åœ¨NMTä½“ç³»ç»“æ„ä¸­è”åˆè®­ç»ƒã€‚ æ±‰è‹±ç¿»è¯‘å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªNISTæµ‹è¯•é›†ä¸Šæ¯”ç°æœ‰çš„NMTå’ŒSMTç³»ç»Ÿå–å¾—äº†æ˜¾è‘—å’Œä¸€è‡´çš„æ”¹è¿›ã€‚</p>

<h5 id="71-zhou-long-hu-wenpeng-zhang-jiajun-et-alneural-system-combination-for-machine-translationproceedings-of-the55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-2017378-384">[71] Zhou Long, Hu Wenpeng, Zhang Jiajun, et al.Neural system combination for machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:378-384</h5>
<p>Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰æ˜¯ä¸€ç§æ–°çš„æœºå™¨ç¿»è¯‘æ–¹æ³•ï¼Œä¸ç»Ÿè®¡æœºå™¨ç¿»è¯‘ï¼ˆStatistical Machine Translationï¼ŒSMTï¼‰ç›¸æ¯”ï¼Œå®ƒèƒ½äº§ç”Ÿæ›´æµç•…çš„ç¿»è¯‘ç»“æœã€‚ ç„¶è€Œï¼ŒSMTåœ¨ç¿»è¯‘å……åˆ†æ€§æ–¹é¢é€šå¸¸ä¼˜äºNMTã€‚ å› æ­¤ï¼Œå°†NMTå’ŒSMTçš„ä¼˜ç‚¹ç»“åˆèµ·æ¥æ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰é€”çš„æ–¹å‘ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæºNMTçš„ç¥ç»ç³»ç»Ÿç»„åˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥NMTå’ŒSMTç³»ç»Ÿçš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œäº§ç”Ÿæœ€ç»ˆçš„è½¬æ¢ç»“æœã€‚ å¤§é‡çš„æ±‰è‹±ç¿»è¯‘å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ¯”æœ€å¥½çš„å•ä¸ªç³»ç»Ÿè¾“å‡ºæé«˜äº†5.3ä¸ªBLEUç‚¹ï¼Œæ¯”æœ€å…ˆè¿›çš„ä¼ ç»Ÿç³»ç»Ÿç»„åˆæ–¹æ³•æé«˜äº†3.4ä¸ªBLEUç‚¹ã€‚</p>

<h5 id="72-stahlberg-f-de-gispert-a-hasler-e-et-alneural-machine-translation-by-minimising-the-bayes-risk-with-respect-to-syntactic-translation-latticesproceedings-of-the-15th-conference-of-the-european-chapter-of-the-association-for-computational-linguistics-eacl-2017-valencia-spain-2017362-368">[72] Stahlberg F, de Gispert A, Hasler E, et al.Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices//Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017) .Valencia, Spain, 2017:362-368</h5>
<p>We present a novel scheme to combine neural machine translation (NMT) with traditional statistical machine translation (SMT). Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is combined with the Bayes-risk of the translation according the SMT lattice. This makes our approach much more flexible than n-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on EnglishGerman and Japanese-English and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.
æå‡ºäº†ä¸€ç§å°†ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘(SMT)ç›¸ç»“åˆçš„æ–°æ–¹æ¡ˆã€‚ æˆ‘ä»¬çš„æ–¹æ³•å€Ÿé‰´äº†çº¿æ€§æ ¼å­æœ€å°è´å¶æ–¯é£é™©è¯‘ç çš„æ€æƒ³ã€‚ æ ¹æ®SMTæ ¼å°†NMTå¾—åˆ†ä¸è½¬æ¢çš„Bayesé£é™©ç›¸ç»“åˆã€‚ è¿™ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•æ¯”N-æœ€ä½³åˆ—è¡¨æˆ–æ ¼å­é‡è¯„åˆ†çµæ´»å¾—å¤šï¼Œå› ä¸ºç¥ç»è§£ç å™¨ä¸é™äºSMTæœç´¢ç©ºé—´ã€‚ æˆ‘ä»¬ç»™å‡ºäº†ä¸€ç§å°†é£é™©ä¼°è®¡é›†æˆåˆ°é€‚åˆäºå­—çº§å’Œå­å­—å•å…ƒçº§NMTçš„NMTè§£ç å™¨ä¸­çš„ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ã€‚ æˆ‘ä»¬åœ¨è‹±è¯­ã€å¾·è¯­å’Œæ—¥è¯­-è‹±è¯­ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶æŠ¥å‘Šäº†åœ¨å•ä¸ªå’Œé›†æˆNMTçš„å‡ ä¸ªæ•°æ®é›†ä¸Šï¼Œä¸ç‚¹é˜µå¾—åˆ†ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚ MBRè§£ç å™¨äº§ç”Ÿäº†å…¨æ–°çš„å‡è®¾ï¼Œè¿œè¿œè¶…å‡ºäº†ç®€å•åœ°é‡æ–°è¯„åˆ†SMTæœç´¢ç©ºé—´æˆ–åœ¨NMTè¾“å‡ºä¸­å›ºå®šUNKçš„èŒƒå›´ã€‚</p>

<h5 id="73-tang-yaohua-meng-fandong-lu-zhengdong-et-alneural-machine-translation-with-external-phrase-memoryarxiv-preprint160601792v1-2016">[73] Tang Yaohua, Meng Fandong, Lu Zhengdong, et al.Neural machine translation with external phrase memory.arXiv preprint/1606.01792v1, 2016</h5>
<p>In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.
æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰çŸ­è¯­è®°å¿†åŠŸèƒ½çš„ç¥ç»æœºå™¨ç¿»è¯‘å™¨PhraseNetï¼Œå®ƒä»¥ç¬¦å·å½¢å¼å­˜å‚¨ä»è¯­æ–™åº“ä¸­æŒ–æ˜å‡ºæ¥çš„çŸ­è¯­å¯¹æˆ–ç”±äººç±»ä¸“å®¶æŒ‡å®šçš„çŸ­è¯­å¯¹ã€‚ å¯¹äºä»»ä½•ç»™å®šçš„æºå¥ï¼ŒçŸ­è¯­ç½‘æ‰«æçŸ­è¯­å­˜å‚¨å™¨ä»¥ç¡®å®šå€™é€‰çŸ­è¯­å¯¹ï¼Œå¹¶ç›¸åº”åœ°å°†æ ‡è®°ä¿¡æ¯é›†æˆåˆ°æºå¥çš„è¡¨ç¤ºä¸­ã€‚ è¯¥è§£ç å™¨åˆ©ç”¨å•è¯ç”Ÿæˆç»„ä»¶å’ŒçŸ­è¯­ç”Ÿæˆç»„ä»¶çš„æ··åˆï¼Œä½¿ç”¨ç‰¹å®šè®¾è®¡çš„ç­–ç•¥æ¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªå•è¯çš„åºåˆ—ã€‚ è¯¥çŸ­è¯­ç½‘ä¸ä»…ä¸ºå°†å¤–éƒ¨çŸ¥è¯†èå…¥åˆ°ç¥ç»æœºå™¨ç¿»è¯‘ä¸­è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œè€Œä¸”è¿˜åŠªåŠ›æ‰©å±•äº†é€’å½’ç¥ç»ç½‘ç»œçš„é€å­—ç”Ÿæˆæœºåˆ¶ã€‚ æˆ‘ä»¬å¯¹æ±‰è‹±ç¿»è¯‘çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒé€‰æ‹©çš„çŸ­è¯­è¡¨ï¼ŒçŸ­è¯­ç½‘æ¯”ä¸€èˆ¬çš„ç¥ç»æœºå™¨ç¿»è¯‘å™¨æé«˜äº†3.45bleuã€‚</p>

<h5 id="74-feng-yang-zhang-shiyue-zhang-andi-et-almemoryaugmented-neural-machine-translationproceedings-of-the2017conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-20171401-1410">[74] Feng Yang, Zhang Shiyue, Zhang Andi, et al.Memoryaugmented neural machine translation//Proceedings of the2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1401-1410</h5>
<p>Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memoryaugmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods
è¿‘å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘(NMT)å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†äººä»¬ä¹Ÿæ™®éè®¤è¯†åˆ°ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤„ç†éé¢‘ç¹è¯å’Œè¯å¯¹æ—¶å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®°å¿†å¢å¼ºNMTï¼ˆMemoryAugmented NMTï¼ŒM-NMTï¼‰ç»“æ„ï¼Œè¯¥ç»“æ„å°†å…³äºå•è¯ï¼ˆé€šå¸¸å¾ˆå°‘é‡åˆ°çš„å•è¯ï¼‰åº”è¯¥å¦‚ä½•ç¿»è¯‘çš„çŸ¥è¯†å­˜å‚¨åœ¨å­˜å‚¨å™¨ä¸­ï¼Œç„¶ååˆ©ç”¨å®ƒä»¬æ¥è¾…åŠ©ç¥ç»æ¨¡å‹ã€‚ åˆ©ç”¨è¯¥å­˜å‚¨æœºåˆ¶ï¼Œå°†ä¼ ç»Ÿç»Ÿè®¡æœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„çŸ¥è¯†ä¸NMTç³»ç»Ÿçš„è§„åˆ™ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¯¥æ¡†æ¶çš„è¯æ±‡è¡¨å¤–(OOV)è¯è§£å†³æ–¹æ¡ˆã€‚ æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ±‰è‹±ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM-NMTæ¶æ„åœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šåˆ†åˆ«æ¯”NMTåŸºçº¿é«˜9.0å’Œ2.7ä¸ªBLEUç‚¹ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸ç«äº‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§ä½“ç³»ç»“æ„å¯¼è‡´äº†æ›´æœ‰æ•ˆçš„OOVå¤„ç†</p>

<h5 id="75-wang-xing-tu-zhaopeng-xiong-deyi-et-altranslating-phrases-in-neural-machine-translationproceedings-of-the2017conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-20171432-1442">[75] Wang Xing, Tu Zhaopeng, Xiong Deyi, et al.Translating phrases in neural machine translation//Proceedings of the2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1432-1442</h5>

<h5 id="76-arthur-p-neubig-g-nakamura-sincorporating-discrete-translation-lexicons-into-neural-machine-translationproceedings-of-the-2016-conference-on-empirical-methods-in-natural-language-processing-emnlp-2016-austin-usa-20161557-1567">[76] Arthur P, Neubig G, Nakamura S.Incorporating discrete translation lexicons into neural machine translation//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1557-1567</h5>
<p>Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.1
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆneural machine translationï¼ŒNMTï¼‰åœ¨ç¿»è¯‘å¯¹ç†è§£å¥å­æ„ä¹‰è‡³å…³é‡è¦çš„ä½é¢‘å®è¯æ—¶ç»å¸¸å‡ºé”™ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ä½¿ç”¨ç¦»æ•£ç¿»è¯‘è¯å…¸æ¥å¢å¼ºNMTç³»ç»Ÿï¼Œè¿™äº›è¯å…¸å¯ä»¥æœ‰æ•ˆåœ°å¯¹è¿™äº›ä½é¢‘è¯çš„ç¿»è¯‘è¿›è¡Œç¼–ç ã€‚ æˆ‘ä»¬æè¿°äº†ä¸€ç§è®¡ç®—å€™é€‰ç¿»è¯‘ä¸­ä¸‹ä¸€ä¸ªå•è¯çš„è¯å…¸æ¦‚ç‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨NMTæ¨¡å‹çš„æ³¨æ„å‘é‡æ¥é€‰æ‹©æ¨¡å‹åº”è¯¥å…³æ³¨çš„æºå•è¯çš„è¯æ±‡æ¦‚ç‡ã€‚ æˆ‘ä»¬æµ‹è¯•äº†ä¸¤ç§å°†è¯¥æ¦‚ç‡ä¸æ ‡å‡†NMTæ¦‚ç‡ç›¸ç»“åˆçš„æ–¹æ³•:ï¼ˆ1ï¼‰å°†å…¶ç”¨ä½œåç½®ï¼›ï¼ˆ2ï¼‰çº¿æ€§æ’å€¼ã€‚ åœ¨ä¸¤ä¸ªè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBLEUå’ŒNISTè¯„åˆ†åˆ†åˆ«æé«˜äº†2.0-2.3å’Œ0.13-0.44ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚</p>

<h5 id="77-sennrich-r-haddow-blinguistic-input-features-improve-neural-machine-translationproceedings-of-the-1st-conference-on-machine-translationberlin-germany-201683-91">[77] Sennrich R, Haddow B.Linguistic input features improve neural machine translation//Proceedings of the 1st Conference on Machine Translation.Berlin, Germany, 2016:83-91</h5>
<p>Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoderâ€“decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-ofspeech tags, and syntactic dependency labels as input features to Englishâ†”German and Englishâ†’Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An opensource implementation of our neural MT system is available1, as are sample files and configurations2.
è¿‘å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœï¼Œä½†å¯¹å¤–éƒ¨è¯­è¨€ä¿¡æ¯çš„åˆ©ç”¨å´å¾ˆå°‘ã€‚ æœ¬æ–‡è¯æ˜äº†ç¥ç»MTæ¨¡å‹å…·æœ‰è¾ƒå¼ºçš„å­¦ä¹ èƒ½åŠ›ï¼Œä¸ä¼šé€ æˆè¯­è¨€ç‰¹å¾çš„å†—é¦€ï¼› å®ƒä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ç»“åˆåœ¨ä¸€èµ·ï¼Œä»¥æä¾›æ€§èƒ½ä¸Šçš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚ æˆ‘ä»¬å°†ç¼–ç å™¨çš„åµŒå…¥å±‚æ¨å¹¿åˆ°æ³¨æ„ç¼–ç å™¨-è§£ç å™¨ä½“ç³»ç»“æ„ä¸­ï¼Œä»¥æ”¯æŒé™¤åŸºçº¿å­—ç‰¹å¾ä¹‹å¤–çš„ä»»æ„ç‰¹å¾çš„åŒ…å«ã€‚ æˆ‘ä»¬å°†è¯å½¢ç‰¹å¾ã€è¯æ€§æ ‡è®°å’Œå¥æ³•ä¾å­˜æ ‡è®°ä½œä¸ºè¾“å…¥ç‰¹å¾æ·»åŠ åˆ°è‹±è¯­â†’å¾·è¯­å’Œè‹±è¯­â†’ç½—é©¬å°¼äºšè¯­çš„ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¸­ã€‚ åœ¨å¯¹WMT16è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°è¯­è¨€è¾“å…¥ç‰¹å¾å¯ä»¥é€šè¿‡ä¸‰ä¸ªæŒ‡æ ‡æ¥æé«˜æ¨¡å‹è´¨é‡:å›°æƒ‘åº¦ã€BLEUå’ŒCHRF3ã€‚ æˆ‘ä»¬çš„Neural MTç³»ç»Ÿçš„ä¸€ä¸ªå¼€æ”¾æºç å®ç°æ˜¯å¯ç”¨çš„1ï¼Œç¤ºä¾‹æ–‡ä»¶å’Œé…ç½®ä¹Ÿæ˜¯å¯ç”¨çš„2ã€‚</p>

<h5 id="78-chen-kehai-wang-rui-utiyama-m-et-alneural-machine-translation-with-source-dependency-representationproceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-20172836-3842">[78] Chen Kehai, Wang Rui, Utiyama M, et al.Neural machine translation with source dependency representation//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:2836-3842</h5>
<p>Source dependency information has been successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system.
æºä¾èµ–ä¿¡æ¯å·²è¢«æˆåŠŸåœ°å¼•å…¥åˆ°ç»Ÿè®¡æœºå™¨ç¿»è¯‘ä¸­ã€‚ ç„¶è€Œï¼Œå¯¹äºç¥ç»æœºå™¨ç¿»è¯‘(NMT)çš„åˆæ­¥å°è¯•è¿˜å¾ˆå°‘ï¼Œå¦‚å°†æºè¯åŠå…¶ä¾å­˜å…³ç³»æ ‡ç­¾çš„è¡¨ç¤ºçº§è”åœ¨ä¸€èµ·ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å…·æœ‰æºä¾å­˜å…³ç³»è¡¨ç¤ºçš„æ³¨æ„NMTï¼Œä»¥æé«˜NMTçš„ç¿»è¯‘æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¯¹é•¿å¥çš„ç¿»è¯‘æ€§èƒ½ã€‚ åœ¨NISTæ±‰è¯‘è‹±ä»»åŠ¡ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸€ä¸ªå¼ºå¤§çš„NMTç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¹³å‡æé«˜äº†1.6ä¸ªBLEUã€‚</p>

<h5 id="79-li-junhui-xiong-deyi-tu-zhaopeng-et-almodeling-source-syntax-for-neural-machine-translationproceedings-of-the55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-2017688-697">[79] Li Junhui, Xiong Deyi, Tu Zhaopeng, et al.Modeling source syntax for neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:688-697</h5>
<p>Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.
å°½ç®¡ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ä¸­çš„éè¯­è¨€å­¦åºåˆ—åˆ°åºåˆ—æ¨¡å‹å…·æœ‰ä¸€å®šçš„éšå¼å­¦ä¹ æºå¥å¥æ³•ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä½†æœ¬æ–‡è¡¨æ˜ï¼Œæºå¥æ³•å¯ä»¥æœ‰æ•ˆåœ°æ˜¾å¼åœ°èå…¥NMTä¸­ï¼Œä»¥æä¾›è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚ å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çº¿æ€§åŒ–æºè¯­å¥çš„è§£ææ ‘ä»¥è·å¾—ç»“æ„æ ‡è®°åºåˆ—ã€‚ åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§ä¸åŒçš„ç¼–ç å™¨å°†æºå¥æ³•å¹¶å…¥NMT:1ï¼‰å¹¶è¡ŒRNNç¼–ç å™¨ï¼Œå¹¶è¡Œå­¦ä¹ å•è¯å’Œæ ‡æ³¨å‘é‡ï¼› 2ï¼‰åˆ†å±‚RNNç¼–ç å™¨ï¼Œå­¦ä¹ ä¸¤çº§å±‚æ¬¡ç»“æ„ä¸­çš„å•è¯å’Œæ ‡ç­¾æ³¨é‡Šå‘é‡ï¼›3ï¼‰æ··åˆRNNç¼–ç å™¨ï¼Œåœ¨å•è¯å’Œæ ‡ç­¾æ··åˆçš„åºåˆ—ä¸Šæ‹¼æ¥åœ°å­¦ä¹ å•è¯å’Œæ ‡ç­¾æ³¨é‡Šå‘é‡ã€‚æ±‰è‹±ç¿»è¯‘å®éªŒè¡¨æ˜ï¼Œæ‰€æœ‰ä¸‰ç§å»ºè®®çš„å¥æ³•ç¼–ç å™¨éƒ½èƒ½å¤Ÿæé«˜ç¿»è¯‘ç²¾åº¦ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæœ€ç®€å•çš„RNNç¼–ç å™¨äº§ç”Ÿ1.4bleuç‚¹çš„æ˜¾è‘—æ”¹è¿›çš„æ€§èƒ½ã€‚</p>

<h5 id="80-bastings-j-titov-i-aziz-w-et-algraph-convolutional-encoders-for-syntax-aware-neural-machine-translationproceedings-of-the-2017conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-20171947-1957">[80] Bastings J, Titov I, Aziz W, et al.Graph convolutional encoders for syntax-aware neural machine translation//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1947-1957</h5>
<p>We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.
æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¥æ³•ç»“æ„ä¸åŸºäºç¥ç»æ³¨æ„çš„æœºå™¨ç¿»è¯‘ç¼–è§£ç å™¨æ¨¡å‹ç›¸ç»“åˆçš„ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚ æˆ‘ä»¬ä¾èµ–äºå›¾å·ç§¯ç½‘ç»œï¼ˆGCNsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ€è¿‘å‘å±•èµ·æ¥çš„ç”¨äºå¯¹å›¾ç»“æ„æ•°æ®å»ºæ¨¡çš„ç¥ç»ç½‘ç»œã€‚ æˆ‘ä»¬çš„GCNä½¿ç”¨æºå¥çš„é¢„æµ‹å¥æ³•ä¾èµ–æ ‘æ¥äº§ç”Ÿå¯¹å®ƒä»¬çš„å¥æ³•é‚»åŸŸæ•æ„Ÿçš„å•è¯ï¼ˆå³ç¼–ç å™¨çš„éšè—çŠ¶æ€ï¼‰çš„è¡¨ç¤ºã€‚ GCNä»¥å­—è¡¨ç¤ºä¸ºè¾“å…¥ï¼Œä»¥å­—è¡¨ç¤ºä¸ºè¾“å‡ºï¼Œå› æ­¤å®ƒä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ä½œä¸ºå±‚åˆå¹¶åˆ°æ ‡å‡†ç¼–ç å™¨ä¸­ï¼ˆä¾‹å¦‚ï¼Œåœ¨åŒå‘RNNæˆ–å·ç§¯ç¥ç»ç½‘ç»œä¹‹ä¸Šï¼‰ã€‚ æˆ‘ä»¬é€šè¿‡ä¸åŒç±»å‹ç¼–ç å™¨çš„è‹±å¾·å’Œè‹±æ·å…‹ç¿»è¯‘å®éªŒæ¥è¯„ä¼°å®ƒä»¬çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è§‚å¯Ÿåˆ°åœ¨æ‰€æœ‰è€ƒè™‘çš„è®¾ç½®ä¸­ï¼Œå®ƒä»¬çš„è¯­æ³•ä¸å¯çŸ¥ç‰ˆæœ¬éƒ½æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>

<h5 id="81-wu-shuangzhi-zhou-ming-zhang-dongdongimproved-neural-machine-translation-with-source-syntaxproceedings-of-the-26th-international-joint-conference-on-artificial-intelligence-ijcai-2017-melbourne-australia-20174179-4185">[81] Wu Shuangzhi, Zhou Ming, Zhang Dongdong.Improved neural machine translation with source syntax//Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017) .Melbourne, Australia, 2017:4179-4185</h5>
<p>Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently achieved the state-of-the-art performance. Researchers have proven that extending word level attention to phrase level attention by incorporating source-side phrase structure can enhance the attention model and achieve promising improvement. However, word dependencies that can be crucial to correctly understand a source sentence are not always in a consecutive fashion (i.e. phrase structure), sometimes they can be in long distance. Phrase structures are not the best way to explicitly model long distance dependencies. In this paper we propose a simple but effective method to incorporate source-side long distance dependencies into NMT. Our method based on dependency trees enriches each source state with global dependency structures, which can better capture the inherent syntactic structure of source sentences. Experiments on Chinese-English and English-Japanese translation tasks show that our proposed method outperforms state-of-the-art SMT and NMT baselines.
åŸºäºç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æœ€è¿‘å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ ç ”ç©¶äººå‘˜å·²ç»è¯æ˜ï¼Œé€šè¿‡ç»“åˆè¯æºä¾§è¯ç»„ç»“æ„ï¼Œå°†è¯çº§æ³¨æ„æ‰©å±•åˆ°è¯ç»„çº§æ³¨æ„ï¼Œå¯ä»¥å¢å¼ºæ³¨æ„æ¨¡å‹ï¼Œå¹¶å–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚ ç„¶è€Œï¼Œå¯¹æ­£ç¡®ç†è§£æºå¥è‡³å…³é‡è¦çš„è¯çš„ä¾å­˜å…³ç³»å¹¶ä¸æ€»æ˜¯ä»¥è¿ç»­çš„æ–¹å¼ï¼ˆå³çŸ­è¯­ç»“æ„ï¼‰å­˜åœ¨ï¼Œæœ‰æ—¶å®ƒä»¬å¯èƒ½ç›¸éš”å¾ˆè¿œã€‚ çŸ­è¯­ç»“æ„ä¸æ˜¯æ˜¾å¼å»ºæ¨¡è¿œç¨‹ä¾èµ–å…³ç³»çš„æœ€ä½³æ–¹æ³•ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†ä¿¡æºç«¯çš„é•¿è·ç¦»ç›¸å…³æ€§å¼•å…¥åˆ°NMTä¸­ã€‚ æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¾èµ–æ ‘ï¼Œç”¨å…¨å±€ä¾èµ–ç»“æ„ä¸°å¯Œäº†æ¯ä¸€ä¸ªæºçŠ¶æ€ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æºå¥çš„å›ºæœ‰å¥æ³•ç»“æ„ã€‚ åœ¨æ±‰è‹±å’Œè‹±æ—¥ç¿»è¯‘ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„SMTå’ŒNMTåŸºçº¿ã€‚</p>

<h5 id="82-chen-huadong-huang-shujian-chiang-d-et-alimproved-neural-machine-translation-with-a-syntax-aware-encoder-and-decoderproceedings-of-the-55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-20171936-1945">[82] Chen Huadong, Huang Shujian, Chiang D, et al.Improved neural machine translation with a syntax-aware encoder and decoder//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1936-1945</h5>
<h5 id="83-niehues-j-cho-eexploiting-linguistic-resources-for-neural-machine-translation-using-multi-task-learningproceedings-of-the-conference-on-machine-translation-wmt-2017-copenhagen-denmark-201780-89">[83] Niehues J, Cho E.Exploiting linguistic resources for neural machine translation using multi-task learning//Proceedings of the Conference on Machine Translation (WMT 2017) .Copenhagen, Denmark, 2017:80-89</h5>
<p>Linguistic resources such as part-ofspeech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances. However, usage of such linguistic annotations in neural machine translation (NMT) systems has been left under-explored.
In this work, we show that multi-task learning is a successful and a easy approach to introduce an additional knowledge into an end-to-end neural attentional model. By jointly training several natural language processing (NLP) tasks in one system, we are able to leverage common
information and improve the performance of the individual task.
We analyze the impact of three design decisions in multi-task learning: the tasks used in training, the training schedule, and the degree of parameter sharing across the tasks, which is defined by the network architecture. The experiments are conducted for an German to English translation task. As additional linguistic resources, we exploit POS information and named-entities (NE). Experiments show that the translation quality can be improved by up to 1.5 BLEU points under the low-resource condition. The performance of the POS tagger is also improved using the multi-task learning scheme.
è¯æ€§æ ‡æ³¨(POS)ç­‰è¯­è¨€èµ„æºåœ¨ç»Ÿè®¡æœºå™¨ç¿»è¯‘(SMT)æ¡†æ¶ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œå¹¶å–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ã€‚ ç„¶è€Œï¼Œè¿™ç±»è¯­è¨€æ³¨é‡Šåœ¨ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¸­çš„åº”ç”¨ä¸€ç›´æ²¡æœ‰å¾—åˆ°å……åˆ†çš„ç ”ç©¶ã€‚ 
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å¤šä»»åŠ¡å­¦ä¹ æ˜¯ä¸€ä¸ªæˆåŠŸçš„å’Œå®¹æ˜“çš„æ–¹æ³•æ¥å¼•å…¥é¢å¤–çš„çŸ¥è¯†åˆ°ç«¯åˆ°ç«¯çš„ç¥ç»æ³¨æ„æ¨¡å‹ã€‚ é€šè¿‡åœ¨ä¸€ä¸ªç³»ç»Ÿä¸­è”åˆè®­ç»ƒå¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å…¬å…± 
ä¿¡æ¯å’Œæé«˜å•ä¸ªä»»åŠ¡çš„æ€§èƒ½ã€‚ 
æˆ‘ä»¬åˆ†æäº†ä¸‰ä¸ªè®¾è®¡å†³ç­–å¯¹å¤šä»»åŠ¡å­¦ä¹ çš„å½±å“:è®­ç»ƒä¸­ä½¿ç”¨çš„ä»»åŠ¡ã€è®­ç»ƒè®¡åˆ’å’Œè·¨ä»»åŠ¡å‚æ•°å…±äº«çš„ç¨‹åº¦ï¼Œè¿™ä¸‰ä¸ªè®¾è®¡å†³ç­–æ˜¯ç”±ç½‘ç»œä½“ç³»ç»“æ„å®šä¹‰çš„ã€‚ å®éªŒæ˜¯ä¸ºä¸€ä¸ªå¾·è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ä»»åŠ¡è€Œè¿›è¡Œçš„ã€‚ ä½œä¸ºé¢å¤–çš„è¯­è¨€èµ„æºï¼Œæˆ‘ä»¬åˆ©ç”¨äº†POSä¿¡æ¯å’Œå‘½åå®ä½“(NE)ã€‚ å®éªŒè¡¨æ˜ï¼Œåœ¨ä½èµ„æºæ¡ä»¶ä¸‹ï¼Œè¯¥ç®—æ³•çš„ç¿»è¯‘è´¨é‡å¯æé«˜1.5ä¸ªbleuç‚¹ã€‚ ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ æ–¹æ¡ˆï¼Œä¹Ÿæé«˜äº†è¯æ€§æ ‡æ³¨å™¨çš„æ€§èƒ½ã€‚</p>

<h5 id="84-zhang-jiacheng-liu-yang-luan-huanbo-et-alprior-knowledge-integration-for-neural-machine-translation-using-posterior-regularizationproceedings-of-the-55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-20171514-1523">[84] Zhang Jiacheng, Liu Yang, Luan Huanbo, et al.Prior knowledge integration for neural machine translation using posterior regularization//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1514-1523</h5>
<p>Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning process of the neural translation model. Experiments on ChineseEnglish translation show that our approach leads to significant improvements.
å°½ç®¡è¿‘å¹´æ¥ç¥ç»æœºå™¨ç¿»è¯‘å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ï¼Œä½†å¦‚ä½•é›†æˆå¤šä¸ªé‡å çš„ã€ä»»æ„çš„å…ˆéªŒçŸ¥è¯†æºä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨åéªŒæ­£åˆ™åŒ–æ¥æä¾›ä¸€ä¸ªå°†å…ˆéªŒçŸ¥è¯†é›†æˆåˆ°ç¥ç»æœºå™¨ç¿»è¯‘ä¸­çš„ä¸€èˆ¬æ¡†æ¶ã€‚ å°†å…ˆéªŒçŸ¥è¯†è¡¨ç¤ºä¸ºå¯¹æ•°çº¿æ€§æ¨¡å‹çš„ç‰¹å¾ï¼ŒæŒ‡å¯¼ç¥ç»ç¿»è¯‘æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚ åœ¨æ±‰è‹±ç¿»è¯‘ä¸­çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰äº†å¾ˆå¤§çš„æ”¹è¿›ã€‚</p>

<h5 id="85-eriguchi-a-hashimoto-k-tsuruoka-ytree-to-sequence-attentional-neural-machine-translationproceedings-of-the54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-2016823-833">[85] Eriguchi A, Hashimoto K, Tsuruoka Y.Tree-to-Sequence attentional neural machine translation//Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:823-833</h5>
<p>Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WATâ€™15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system
ç°æœ‰çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹å¤§å¤šé›†ä¸­äºåºåˆ—æ•°æ®çš„è½¬æ¢ï¼Œæ²¡æœ‰ç›´æ¥åˆ©ç”¨å¥æ³•ä¿¡æ¯ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯å¥æ³•NMTæ¨¡å‹ï¼Œæ‰©å±•äº†æºç«¯çŸ­è¯­ç»“æ„çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ã€‚ æˆ‘ä»¬çš„æ¨¡å‹æœ‰ä¸€ä¸ªæ³¨æ„æœºåˆ¶ï¼Œä½¿è§£ç å™¨èƒ½å¤Ÿç”Ÿæˆä¸€ä¸ªå·²ç¿»è¯‘çš„å•è¯ï¼ŒåŒæ—¶å°†å…¶ä¸æºå¥å­çš„çŸ­è¯­å’Œå•è¯è½»æŸ”åœ°å¯¹é½ã€‚ åœ¨WATâ€™15è‹±æ—¥æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹æ¯”åºåˆ—å¯¹åºåˆ—çš„æ³¨æ„NMTæ¨¡å‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸ç›®å‰æœ€å…ˆè¿›çš„æ ‘å¯¹ä¸²SMTç³»ç»Ÿè¿›è¡Œäº†æ¯”è¾ƒ</p>

<h5 id="86-aharoni-r-goldberg-ytowards-string-to-tree-neural-machine-translationproceedings-of-the-55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-2017132-140">[86] Aharoni R, Goldberg Y.Towards string-to-tree neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:132-140</h5>
<p>We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A smallscale human evaluation also showed an advantage to the syntax-aware system.
æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œé€šè¿‡ç¿»è¯‘æˆçº¿æ€§åŒ–ã€è¯æ±‡åŒ–çš„æˆåˆ†æ ‘ï¼Œå°†ç›®æ ‡è¯­è¨€çš„å¥æ³•ä¿¡æ¯åˆå¹¶åˆ°ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¸­ã€‚ åœ¨WMT16å¾·è‹±æ–°é—»ç¿»è¯‘ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼Œä¸åœ¨ç›¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒçš„è¯­æ³•ä¸å¯çŸ¥çš„NMTåŸºçº¿ç›¸æ¯”ï¼ŒBLEUå¾—åˆ†æœ‰æ‰€æé«˜ã€‚ å¯¹å¥æ³•æ„ŸçŸ¥ç³»ç»Ÿç¿»è¯‘çš„åˆ†æè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œå®ƒåœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­æ‰§è¡Œäº†æ›´å¤šçš„é‡æ–°æ’åºã€‚ å°è§„æ¨¡çš„äººå·¥è¯„ä¼°ä¹Ÿæ˜¾ç¤ºäº†è¯­æ³•æ„ŸçŸ¥ç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚</p>

<h5 id="87-wu-shuangzhi-zhang-dongdong-yang-nan-et-alsequenceto-dependency-neural-machine-translationproceedings-of-the55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-2017698-707">[87] Wu Shuangzhi, Zhang Dongdong, Yang Nan, et al.Sequenceto-Dependency neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:698-707</h5>
<p>Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks.
ç›®å‰ï¼Œå…¸å‹çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹ä»¥çº¿æ€§åºåˆ—çš„å½¢å¼äº§ç”Ÿä»å·¦åˆ°å³çš„ç¿»è¯‘ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç›®æ ‡å¥å­çš„æ½œåœ¨å¥æ³•ç»“æ„å¹¶æ²¡æœ‰å¾—åˆ°æ˜ç¡®çš„å…³æ³¨ã€‚ å—åˆ©ç”¨ç›®æ ‡è¯­è¨€å¥æ³•çŸ¥è¯†æ”¹è¿›ç»Ÿè®¡æœºå™¨ç¿»è¯‘çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åºåˆ—ä¾èµ–ç¥ç»æœºå™¨ç¿»è¯‘æ–¹æ³•ï¼ˆSD-NMTï¼‰ï¼Œè¯¥æ–¹æ³•å°†ç›®æ ‡è¯åºåˆ—åŠå…¶å¯¹åº”çš„ä¾èµ–ç»“æ„è”åˆæ„å»ºå’Œå»ºæ¨¡ï¼Œå¹¶å°†è¯¥ç»“æ„ä½œä¸ºä¸Šä¸‹æ–‡æ¥ä¿ƒè¿›è¯çš„ç”Ÿæˆã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ±‰è‹±å’Œæ—¥è¯­ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜æ˜¾ä¼˜äºç°æœ‰çš„åŸºçº¿ã€‚</p>

<h5 id="88-gulcehre-c-firat-o-xu-k-et-alon-using-monolingual-corpora-in-neural-machine-translationarxiv-preprint150303535v2-2015">[88] Gulcehre C, Firat O, Xu K, et al.On using monolingual corpora in neural machine translation.arXiv preprint/1503.03535v2, 2015</h5>
<p>Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to 1.96 BLEU improvement on the lowresource language pair Turkish-English, and 1.59 BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of 0.39 and 0.47 BLEU scores over the neural machine translation baselines, respectively.
è¿‘å¹´æ¥ï¼ŒåŸºäºç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œçš„æœºå™¨ç¿»è¯‘ä½“ç³»ç»“æ„åœ¨EN-FRå’ŒEN-DEç¿»è¯‘ä¸­å¾—åˆ°äº†å¾ˆå¥½çš„åº”ç”¨ã€‚ å¯ä»¥è¯´ï¼Œè¿™ä¸€æˆåŠŸèƒŒåçš„ä¸€ä¸ªä¸»è¦å› ç´ æ˜¯é«˜è´¨é‡çš„å¹³è¡Œè¯­æ–™åº“çš„å¯ç”¨æ€§ã€‚ æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨ä¸°å¯Œçš„å•è¯­è¯­æ–™åº“è¿›è¡Œç¥ç»æœºå™¨ç¿»è¯‘ã€‚ ä¸åŸºäºçŸ­è¯­å’Œåˆ†å±‚çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨ä½èµ„æºè¯­è¨€å¯¹åœŸè€³å…¶è¯­-è‹±è¯­ä¸Šè·å¾—äº†1.96bleuçš„æ”¹è¿›ï¼Œåœ¨ä¸­è‹±æ–‡èŠå¤©æ¶ˆæ¯çš„ç„¦ç‚¹åŸŸä»»åŠ¡ä¸Šè·å¾—äº†1.59bleuçš„æ”¹è¿›ã€‚ è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•æœ€åˆæ˜¯é’ˆå¯¹å¹¶è¡Œæ•°æ®è¾ƒå°‘çš„ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒä¹Ÿæ‰©å±•åˆ°é«˜èµ„æºè¯­è¨€ï¼Œå¦‚CS-ENå’ŒDE-ENï¼Œåœ¨è¿™ä¸¤ç§è¯­è¨€ä¸­ï¼Œæˆ‘ä»¬åˆ†åˆ«æ¯”ç¥ç»æœºå™¨ç¿»è¯‘åŸºçº¿æé«˜äº†0.39å’Œ0.47ä¸ªBLEUåˆ†æ•°ã€‚</p>

<h5 id="89-domhan-t-hieber-fusing-target-side-monolingual-data-for-neural-machine-translation-through-multi-task-learningproceedings-of-the-2017conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-20171501-1506">[89] Domhan T, Hieber F.Using target-side monolingual data for neural machine translation through multi-task learning//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1501-1506</h5>
<p>The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰æ¨¡å‹çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè¶³å¤Ÿå¤šçš„å¹¶è¡Œæ•°æ®çš„å¯ç”¨æ€§ï¼Œè€Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤§é‡çš„å•è¯­æ•°æ®è¿˜æ²¡æœ‰æ‰¾åˆ°ã€‚ æˆ‘ä»¬å»ºè®®åœ¨ä¸€ä¸ªç¥ç»åºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹ä¸­ä¿®æ”¹è§£ç å™¨ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¯¹ä¸¤ä¸ªå¼ºç›¸å…³çš„ä»»åŠ¡è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ :ç›®æ ‡ç«¯è¯­è¨€å»ºæ¨¡å’Œç¿»è¯‘ã€‚ è§£ç å™¨é€šè¿‡ä¸¤ä¸ªé€šé“é¢„æµ‹ä¸‹ä¸€ä¸ªç›®æ ‡å•è¯ï¼Œä¸€ä¸ªæ˜¯æœ€ä½å±‚çš„ç›®æ ‡ç«¯è¯­è¨€æ¨¡å‹ï¼Œå¦ä¸€ä¸ªæ˜¯ä»¥æºè¡¨ç¤ºä¸ºæ¡ä»¶çš„æ³¨æ„é€’å½’æ¨¡å‹ã€‚ è¿™ç§ä½“ç³»ç»“æ„å…è®¸å¯¹å¤§é‡çš„å•è¯­å’Œä¸­ç­‰æ•°é‡çš„åŒè¯­æ•°æ®è¿›è¡Œè”åˆåŸ¹è®­ï¼Œä»¥æé«˜NMTæ€§èƒ½ã€‚ ä¸‰ç§è¯­è¨€å¯¹çš„æ–°é—»é¢†åŸŸçš„åˆæ­¥ç»“æœæ˜¾ç¤ºï¼Œä¸ä»…å¯¹åŒè¯­æ•°æ®è¿›è¡ŒåŸ¹è®­çš„åŸºçº¿ç›¸æ¯”ï¼Œæœ‰é€‚åº¦ä½†ä¸€è‡´çš„æ”¹è¿›ã€‚</p>

<h5 id="90-sennrich-r-haddow-b-birch-aimproving-neural-machine-translation-models-with-monolingual-dataproceedings-of-the-54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-201686-96">[90] Sennrich R, Haddow B, Birch A.Improving neural machine translation models with monolingual data//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:86-96</h5>
<p>Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic backtranslation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task Englishâ†”German (+2.8â€“3.7 BLEU), and for the low-resourced IWSLT 14 task Turkishâ†’English (+2.1â€“3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task Englishâ†’German.
ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNeural Machine Translationï¼ŒNMTï¼‰åœ¨ä»…ä½¿ç”¨å¹¶è¡Œæ•°æ®è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¯¹å¤šä¸ªè¯­è¨€å¯¹è·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ Targetsideå•è¯­æ•°æ®åœ¨æé«˜åŸºäºçŸ­è¯­çš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘æµåˆ©æ€§æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å•è¯­æ•°æ®åœ¨NMTä¸­çš„åº”ç”¨ã€‚ ä¸ä»¥å¾€å°†NMTæ¨¡å‹ä¸å•ç‹¬è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç›¸ç»“åˆçš„å·¥ä½œç›¸åï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ç¼–ç å™¨-è§£ç å™¨NMTä½“ç³»ç»“æ„å·²ç»å…·æœ‰å­¦ä¹ ä¸è¯­è¨€æ¨¡å‹ç›¸åŒçš„ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æˆ‘ä»¬æ¢ç´¢äº†åœ¨ä¸æ”¹å˜ç¥ç»ç½‘ç»œä½“ç³»ç»“æ„çš„æƒ…å†µä¸‹ä½¿ç”¨å•è¯­è¨€æ•°æ®è¿›è¡Œè®­ç»ƒçš„ç­–ç•¥ã€‚ é€šè¿‡å°†å•è¯­è®­ç»ƒæ•°æ®ä¸è‡ªåŠ¨å›è¯‘æ•°æ®é…å¯¹ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä½œä¸ºé¢å¤–çš„å¹¶è¡Œè®­ç»ƒæ•°æ®å¤„ç†ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯¹WMT15ä»»åŠ¡è‹±è¯­Ã—å¾·è¯­(+2.8-3.7bleu)å’Œèµ„æºä¸è¶³çš„IWSLT14ä»»åŠ¡åœŸè€³å…¶è¯­â†’è‹±è¯­(+2.1-3.4bleu)è·å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè·å¾—äº†æ–°çš„æœ€æ–°ç»“æœã€‚ æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¯¹åŸŸå†…å•è¯­å’Œå¹¶è¡Œæ•°æ®çš„å¾®è°ƒå¯¹IWSLT15ä»»åŠ¡è‹±è¯­â†’å¾·è¯­æœ‰å¾ˆå¤§çš„æ”¹è¿›ã€‚</p>

<h5 id="91-zhang-jiajun-zong-chengqingexploiting-source-side-monolingual-data-in-neural-machine-translationproceedings-of-the-2016-conference-on-empirical-methods-in-natural-language-processing-emnlp-2016-austin-usa-20161535-1545">[91] Zhang Jiajun, Zong Chengqing.Exploiting source-side monolingual data in neural machine translation//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1535-1545</h5>
<p>Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of
NMT, especially when the parallel corpus is far from sufficient. In this paper, we propose two approaches to make full use of the sourceside monolingual data in NMT. The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT.
åŸºäºç¼–è§£ç å™¨ç»“æ„çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ˜¯è¿‘å¹´æ¥å‡ºç°çš„ä¸€ä¸ªæ–°çš„ç ”ç©¶èŒƒå¼ã€‚ ç ”ç©¶äººå‘˜å·²ç»è¯æ˜ï¼Œç›®æ ‡ç«¯çš„å•è¯­æ•°æ®å¯ä»¥å¤§å¤§å¢å¼ºNMTçš„è§£ç å™¨æ¨¡å‹ã€‚ ç„¶è€Œï¼Œæºç«¯å•è¯­è¨€æ•°æ®å¹¶æ²¡æœ‰å¾—åˆ°å……åˆ†çš„æ¢è®¨ï¼Œå°½ç®¡åŠ å¼ºç¼–ç å™¨æ¨¡å‹åº”è¯¥æ˜¯æœ‰ç”¨çš„ã€‚ 
NMTï¼Œç‰¹åˆ«æ˜¯å½“å¹³è¡Œè¯­æ–™åº“è¿œè¿œä¸å¤Ÿæ—¶ã€‚ æœ¬æ–‡æå‡ºäº†ä¸¤ç§å……åˆ†åˆ©ç”¨NMTæºç«¯å•è¯­æ•°æ®çš„æ–¹æ³•ã€‚ ç¬¬ä¸€ç§æ–¹æ³•é‡‡ç”¨è‡ªå­¦ä¹ ç®—æ³•ç”Ÿæˆç”¨äºNMTè®­ç»ƒçš„åˆæˆå¤§è§„æ¨¡å¹¶è¡Œæ•°æ®ã€‚ ç¬¬äºŒç§æ–¹æ³•é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨ä¸¤ä¸ªNMTåŒæ—¶é¢„æµ‹æºç«¯å•è¯­å¥å­çš„ç¿»è¯‘å’Œé‡æ’ã€‚ å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºå¼ºæ³¨æ„çš„NMTæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å¾—åˆ°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>

<h5 id="92-ramachandran-p-liu-p-j-le-q-vunsupervised-pretraining-for-sequence-to-sequence-learningproceedings-of-the-2017conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-2017383-391">[92] Ramachandran P, Liu P J, Le Q V.Unsupervised pretraining for sequence to sequence learning//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:383-391</h5>
<p>This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-theart results on the WMT Englishâ†’German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMTâ€™14 and WMTâ€™15 Englishâ†’German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.
ä¸ºäº†æé«˜åºåˆ—å¯¹åºåˆ—(SEQ2SEQ)æ¨¡å‹çš„ç²¾åº¦ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚ åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼ŒSEQ2SEQæ¨¡å‹çš„ç¼–ç å™¨å’Œè§£ç å™¨çš„æƒé‡è¢«åˆå§‹åŒ–ä¸ºä¸¤ä¸ªè¯­è¨€æ¨¡å‹çš„é¢„å…ˆè®­ç»ƒçš„æƒé‡ï¼Œç„¶åç”¨æ ‡è®°çš„æ•°æ®è¿›è¡Œå¾®è°ƒã€‚ æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºæœºå™¨ç¿»è¯‘å’ŒæŠ½è±¡æ‘˜è¦ä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒæ˜¾è‘—åœ°æ”¹è¿›äº†åç»­çš„æœ‰ç›‘ç£æ¨¡å‹ã€‚ æˆ‘ä»¬çš„ä¸»è¦ç»“æœæ˜¯ï¼Œé¢„è®­ç»ƒæé«˜äº†SEQ2SEQæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ æˆ‘ä»¬åœ¨WMTè‹±è¯­â†’å¾·è¯­ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°çš„æˆæœï¼Œè¶…è¿‡äº†åŸºäºçŸ­è¯­çš„æœºå™¨ç¿»è¯‘å’Œç¥ç»æœºå™¨ç¿»è¯‘çš„ä¸€ç³»åˆ—æ–¹æ³•ã€‚ æˆ‘ä»¬çš„æ–¹æ³•åœ¨WMTâ€™14å’ŒWMTâ€™15è‹±è¯­â†’å¾·è¯­ä¸¤ç§è¯­è¨€ä¸Šéƒ½æ¯”ä»¥å‰çš„æœ€ä½³æ¨¡å‹æœ‰äº†1.3BLEUçš„æ˜¾è‘—æ”¹è¿›ã€‚ æˆ‘ä»¬è¿˜å¯¹æŠ½è±¡æ‘˜è¦è¿›è¡Œäº†äººç±»è¯„ä»·ï¼Œå‘ç°æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»Ÿè®¡ä¸Šæ˜¾è‘—ä¼˜äºçº¯ç›‘ç£å­¦ä¹ åŸºçº¿ã€‚</p>

<h5 id="93-currey-a-barone-a-heafield-kcopied-monolingual-data-improves-low-resource-neural-machine-translationproceedings-of-the-conference-on-machine-translation-wmt-2017-copenhagen-denmark-2017148-156">[93] Currey A, Barone A, Heafield K.Copied monolingual data improves low-resource neural machine translation//Proceedings of the Conference on Machine Translation (WMT 2017) .Copenhagen, Denmark, 2017:148-156</h5>
<p>We train a neural machine translation (NMT) system to both translate sourcelanguage text and copy target-language text, thereby exploiting monolingual corpora in the target language. Specifically, we create a bitext from the monolingual text in the target language so that each source sentence is identical to the target sentence. This copied data is then mixed with the parallel corpus and the NMT system is trained like normal, with no metadata to distinguish the two input languages.
Our proposed method proves to be an effective way of incorporating monolingual data into low-resource NMT. On Turkishâ†”English and Romanianâ†”English translation tasks, we see gains of up to 1.2 BLEU over a strong baseline with back-translation. Further analysis shows that the linguistic phenomena behind these gains are different from and largely orthogonal to back-translation, with our copied corpus method improving accuracy on named entities and other words that should remain identical between the source and target languages.
æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»æœºå™¨ç¿»è¯‘(NMT)ç³»ç»Ÿæ¥ç¿»è¯‘æºè¯­è¨€æ–‡æœ¬å’Œå¤åˆ¶ç›®æ ‡è¯­è¨€æ–‡æœ¬ï¼Œä»è€Œåˆ©ç”¨ç›®æ ‡è¯­è¨€ä¸­çš„å•è¯­è¯­æ–™åº“ã€‚ å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬ä»ç›®æ ‡è¯­è¨€ä¸­çš„å•è¯­æ–‡æœ¬åˆ›å»ºä¸€ä¸ªä½æ–‡æœ¬ï¼Œä»¥ä¾¿æ¯ä¸ªæºå¥ä¸ç›®æ ‡å¥ç›¸åŒã€‚ ç„¶åå°†å¤åˆ¶çš„æ•°æ®ä¸å¹¶è¡Œè¯­æ–™åº“æ··åˆï¼ŒNMTç³»ç»Ÿåƒæ­£å¸¸æƒ…å†µä¸€æ ·è¿›è¡Œè®­ç»ƒï¼Œæ²¡æœ‰å…ƒæ•°æ®æ¥åŒºåˆ†ä¸¤ç§è¾“å…¥è¯­è¨€ã€‚ 
æˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¢«è¯æ˜æ˜¯ä¸€ç§å°†å•è¯­æ•°æ®åˆå¹¶åˆ°ä½èµ„æºNMTä¸­çš„æœ‰æ•ˆæ–¹æ³•ã€‚ åœ¨åœŸè€³å…¶è¯­è‹±è¯­å’Œç½—é©¬å°¼äºšè¯­è‹±è¯­ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°åœ¨æœ‰å›è¯‘çš„å¼ºå¤§åŸºçº¿ä¸Šå¢åŠ äº†1.2ä¸ªbleuã€‚ è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œè¿™äº›æ”¶è·èƒŒåçš„è¯­è¨€ç°è±¡ä¸åŒäºå›è¯‘ï¼Œå¹¶ä¸”åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ­£äº¤äºå›è¯‘ï¼Œæˆ‘ä»¬çš„å¤åˆ¶è¯­æ–™åº“æ–¹æ³•æé«˜äº†å‘½åå®ä½“å’Œå…¶ä»–åº”è¯¥åœ¨æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä¹‹é—´ä¿æŒç›¸åŒçš„å•è¯çš„å‡†ç¡®æ€§ã€‚</p>

<h5 id="94-fadaee-m-bisazza-a-monz-cdata-augmentation-for-low-resource-neural-machine-translationproceedings-of-the55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-2017567-573">[94] Fadaee M, Bisazza A, Monz C.Data augmentation for low-resource neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:567-573</h5>
<p>The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.
ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå¤§è§„æ¨¡å¹¶è¡Œè¯­æ–™åº“çš„å¯ç”¨æ€§ã€‚ å¯¹äºèµ„æºè¾ƒå°‘çš„è¯­è¨€å¯¹ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ï¼Œå¯¼è‡´ç¿»è¯‘è´¨é‡è¾ƒå·®ã€‚ å—è®¡ç®—æœºè§†è§‰ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨æ–°çš„ã€ç»¼åˆåˆ›å»ºçš„ä¸Šä¸‹æ–‡ä¸­ç”ŸæˆåŒ…å«ç¨€æœ‰è¯çš„æ–°å¥å­å¯¹æ¥é’ˆå¯¹ä½é¢‘è¯ã€‚ åœ¨æ¨¡æ‹Ÿçš„ä½èµ„æºè®¾ç½®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºçº¿ä¸Šæé«˜äº†2.9ä¸ªBLEUç‚¹ï¼Œåœ¨å›è¯‘ä¸Šæé«˜äº†3.2ä¸ªBLEUç‚¹ã€‚</p>

<h5 id="95-zoph-b-yuret-d-may-j-et-altransfer-learning-for-low-resource-neural-machine-translationproceedings-of-the2016conference-on-empirical-methods-in-natural-language-processing-emnlp-2016-austin-usa-20161568-1575">[95] Zoph B, Yuret D, May J, et al.Transfer learning for low-resource neural machine translation//Proceedings of the2016Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1568-1575</h5>
<p>The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.
ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘(NMT)çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å·²è¢«è¯æ˜åœ¨å¤§æ•°æ®åœºæ™¯ä¸­æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å¯¹äºä½èµ„æºè¯­è¨€çš„æ•ˆæœè¦å·®å¾—å¤šã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ˜¾è‘—æé«˜ä¸€ç³»åˆ—ä½èµ„æºè¯­è¨€çš„BLEUåˆ†æ•°ã€‚ æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯é¦–å…ˆè®­ç»ƒä¸€ä¸ªé«˜èµ„æºè¯­è¨€å¯¹ï¼ˆçˆ¶æ¨¡å‹ï¼‰ï¼Œç„¶åå°†ä¸€äº›å­¦ä¹ åˆ°çš„å‚æ•°ä¼ é€’ç»™ä½èµ„æºè¯­è¨€å¯¹ï¼ˆå­æ¨¡å‹ï¼‰æ¥åˆå§‹åŒ–å’Œçº¦æŸè®­ç»ƒã€‚ ä½¿ç”¨æˆ‘ä»¬çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å››ä¸ªä½èµ„æºè¯­è¨€å¯¹ä¸Šå¹³å‡æ”¹è¿›äº†åŸºçº¿NMTæ¨¡å‹5.6BLEUã€‚ é›†æˆå’ŒæœªçŸ¥è¯æ›¿æ¢å¢åŠ äº†2ä¸ªBLEUï¼Œä½¿å¾—ä½èµ„æºæœºå™¨ç¿»è¯‘çš„NMTæ€§èƒ½æ¥è¿‘äºä¸€ä¸ªå¼ºçš„åŸºäºè¯­æ³•çš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿ(SBMT)ï¼Œè¶…è¿‡äº†å®ƒåœ¨ä¸€ä¸ªè¯­è¨€å¯¹ä¸Šçš„æ€§èƒ½ã€‚ å¦å¤–ï¼Œåˆ©ç”¨è¿ç§»å­¦ä¹ æ¨¡å‹å¯¹SBMTç³»ç»Ÿè¿›è¡Œé‡æ–°è¯„åˆ†ï¼Œå¹³å‡æé«˜äº†1.3BLEUï¼Œæ”¹å–„äº†ä½èµ„æºæœºå™¨ç¿»è¯‘çš„ç°çŠ¶ã€‚</p>

<h5 id="96-chen-yun-liu-yang-cheng-yong-et-ala-teacherstudent-framework-for-zero-resource-neural-machine-translationproceedings-of-the-55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-20171925-1935">[96] Chen Yun, Liu Yang, Cheng Yong, et al.A teacherstudent framework for zero-resource neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1925-1935</h5>
<p>While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model (â€œstudentâ€) without parallel corpora available, guided by an existing pivot-to-target NMT model (â€œteacherâ€) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.
ç«¯åˆ°ç«¯ç¥ç»æœºå™¨ç¿»è¯‘(NMT)è¿‘å¹´æ¥å–å¾—äº†é•¿è¶³çš„è¿›æ­¥ï¼Œä½†ä»ç„¶é¢ä¸´ç€ä½èµ„æºè¯­è¨€å¯¹å’Œé¢†åŸŸçš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶èµ„æºNMTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å‡è®¾å¹¶è¡Œå¥å­åœ¨ç¬¬ä¸‰è¯­è¨€ä¸­ç”Ÿæˆå¥å­çš„æ¦‚ç‡å¾ˆå°ã€‚ åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç°æœ‰çš„æ¢è½´åˆ°ç›®æ ‡NMTæ¨¡å‹ï¼ˆâ€œæ•™å¸ˆâ€ï¼‰çš„æŒ‡å¯¼ä¸‹ï¼Œåœ¨æ²¡æœ‰å¹³è¡Œè¯­æ–™åº“çš„æƒ…å†µä¸‹ï¼Œåœ¨ä¸€ä¸ªæºæ¢è½´å¹³è¡Œè¯­æ–™åº“ä¸Šè®­ç»ƒæºåˆ°ç›®æ ‡NMTæ¨¡å‹ï¼ˆâ€œå­¦ç”Ÿâ€ï¼‰ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”åŸºäºåŸºçº¿æ¢è½´çš„æ¨¡å‹åœ¨ä¸åŒçš„è¯­è¨€å¯¹ä¸Šæé«˜äº†+3.0ä¸ªBLEUç‚¹ã€‚</p>

<h5 id="97-zheng-hao-cheng-yong-liu-yangmaximum-expected-likelihood-estimation-for-zero-resource-neural-machine-translationproceedings-of-the-26th-international-joint-conference-on-artificial-intelligence-ijcai-2017-melbourne-australia-20174251-4257">[97] Zheng Hao, Cheng Yong, Liu Yang.Maximum expected likelihood estimation for zero-resource neural machine translation//Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017) .Melbourne, Australia, 2017:4251-4257</h5>
<p>While neural machine translation (NMT) has made remarkable progress in translating a handful of resource-rich language pairs recently, parallel corpora are not always readily available for most language pairs. To deal with this problem, we propose an approach to zero-resource NMT via maximum expected likelihood estimation. The basic idea is to maximize the expectation with respect to a pivot-to-source translation model for the intended source-to-target model on a pivot-target parallel corpus. To approximate the expectation, we propose two methods to connect the pivot-to-source and source-to-target models. Experiments on two zero-resource language pairs show that the proposed approach yields substantial gains over baseline methods. We also observe that when trained jointly with the source-to-target model, the pivotto-source translation model also obtains improvements over independent training.
è¿‘å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘(NMT)åœ¨ç¿»è¯‘å°‘æ•°èµ„æºä¸°å¯Œçš„è¯­è¨€å¯¹æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å¹¶è¡Œè¯­æ–™åº“å¹¶ä¸æ€»æ˜¯å¯¹å¤§å¤šæ•°è¯­è¨€å¯¹æœ‰æ•ˆã€‚ é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæœ€å¤§æœŸæœ›ä¼¼ç„¶ä¼°è®¡çš„é›¶èµ„æºNMTç®—æ³•ã€‚ å…¶åŸºæœ¬æ€æƒ³æ˜¯åœ¨æ¢è½´-ç›®æ ‡å¹¶è¡Œè¯­æ–™åº“ä¸Šï¼Œæœ€å¤§é™åº¦åœ°æé«˜ç›®æ ‡æº-ç›®æ ‡æ¨¡å‹å¯¹æ¢è½´-æºç¿»è¯‘æ¨¡å‹çš„æœŸæœ›ã€‚ ä¸ºäº†é€¼è¿‘æœŸæœ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥è¿æ¥æ¢è½´åˆ°æºæ¨¡å‹å’Œæºåˆ°ç›®æ ‡æ¨¡å‹ã€‚ åœ¨ä¸¤ä¸ªé›¶èµ„æºè¯­è¨€å¯¹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ¯”åŸºçº¿æ–¹æ³•äº§ç”Ÿäº†æ˜¾è‘—çš„å¢ç›Šã€‚ æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œå½“ä¸æºè¯­åˆ°ç›®æ ‡è¯­æ¨¡å‹è”åˆè®­ç»ƒæ—¶ï¼Œæ¢è½´åˆ°æºè¯­ç¿»è¯‘æ¨¡å‹ä¹Ÿæ¯”ç‹¬ç«‹è®­ç»ƒå¾—åˆ°äº†æ”¹è¿›ã€‚</p>

<h5 id="98-cheng-yong-yang-qian-liu-yang-et-aljoint-training-for-pivot-based-neural-machine-translationproceedings-of-the-26th-international-joint-conference-on-artificial-intelligence-ijcai-2017-melbourne-australia-20173974">[98] Cheng Yong, Yang Qian, Liu Yang, et al.Joint training for pivot-based neural machine translation//Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017) .Melbourne, Australia, 2017:3974</h5>
<p>While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.
è™½ç„¶æœ€è¿‘çš„ç¥ç»æœºå™¨ç¿»è¯‘æ–¹æ³•å·²ç»ä¸ºèµ„æºä¸°å¯Œçš„è¯­è¨€å¯¹æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†æ˜¯å®ƒä»¬å—åˆ°èµ„æºç¨€ç¼ºçš„è¯­è¨€å¯¹çš„æ•°æ®ç¨€ç¼ºé—®é¢˜çš„å›°æ‰°ã€‚ è™½ç„¶å¯ä»¥é€šè¿‡ä½¿ç”¨é€è§†è¯­è¨€æ¥æ¡¥æ¥æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯æºåˆ°é€è§†å’Œé€è§†åˆ°ç›®æ ‡ç¿»è¯‘æ¨¡å‹é€šå¸¸æ˜¯ç‹¬ç«‹è®­ç»ƒçš„ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¢è½´çš„ç¥ç»æœºå™¨ç¿»è¯‘è”åˆè®­ç»ƒç®—æ³•ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸‰ç§æ–¹æ³•æ¥è¿æ¥è¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’äº¤äº’ã€‚ åœ¨Europarlå’ŒWMTè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæºåˆ°æ¢å’Œæ¢åˆ°ç›®æ ‡æ¨¡å‹çš„è”åˆè®­ç»ƒæ¯”è·¨è¯­è¨€çš„ç‹¬ç«‹è®­ç»ƒæœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>

<h5 id="99-ranzato-m-a-chopra-s-auli-m-et-alsequence-level-training-with-recurrent-neural-networksarxiv-preprint151106732v7-2015">[99] Ranzato M A, Chopra S, Auli M, et al.Sequence level training with recurrent neural networks.arXiv preprint/1511.06732v7, 2015</h5>
<p>Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.
è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ç¨‹åºä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬ã€‚ è¿™äº›æ¨¡å‹é€šå¸¸è¢«è®­ç»ƒæ¥é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ï¼Œç»™å‡ºå‰ä¸€ä¸ªå•è¯å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚å›¾åƒã€‚ ä½†æ˜¯ï¼Œåœ¨æµ‹è¯•æ—¶ï¼Œé¢„æœŸæ¨¡å‹å°†ä»å¤´å¼€å§‹ç”Ÿæˆæ•´ä¸ªåºåˆ—ã€‚ è¿™ç§å·®å¼‚ä½¿ç”Ÿæˆå˜å¾—è„†å¼±ï¼Œå› ä¸ºé”™è¯¯å¯èƒ½ä¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç´¯ç§¯ã€‚ æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ–°çš„åºåˆ—çº§è®­ç»ƒç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç®—æ³•ç›´æ¥ä¼˜åŒ–æµ‹è¯•æ—¶ä½¿ç”¨çš„åº¦é‡ï¼Œä¾‹å¦‚BLEUæˆ–Rougeã€‚ åœ¨ä¸‰ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºè´ªå©ªç”Ÿæˆçš„å‡ ä¸ªå¼ºåŸºçº¿ã€‚ å½“è¿™äº›åŸºçº¿é‡‡ç”¨æ³¢æŸæœç´¢æ—¶ï¼Œè¯¥æ–¹æ³•ä¹Ÿæ˜¯æœ‰ç«äº‰åŠ›çš„ï¼ŒåŒæ—¶é€Ÿåº¦æ˜¯æ³¢æŸæœç´¢çš„å‡ å€ã€‚</p>

<h5 id="100-bengio-s-vinyals-o-jaitly-n-et-alscheduled-sampling-for-sequence-prediction-with-recurrent-neural-networksproceedings-of-the-neural-information-processing-systems-nips-2015-montreal-canada-20151-9">[100] Bengio S, Vinyals O, Jaitly N, et al.Scheduled sampling for sequence prediction with recurrent neural networks//Proceedings of the Neural Information Processing Systems (NIPS 2015) .Montreal, Canada, 2015:1-9</h5>
<p>Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.
é€’å½’ç¥ç»ç½‘ç»œå¯ä»¥è¢«è®­ç»ƒä»¥åœ¨ç»™å®šä¸€äº›è¾“å…¥çš„æƒ…å†µä¸‹äº§ç”Ÿä»¤ç‰Œåºåˆ—ï¼Œä¾‹å¦‚æœ€è¿‘åœ¨æœºå™¨ç¿»è¯‘å’Œå›¾åƒå­—å¹•æ–¹é¢çš„ç»“æœã€‚ å½“å‰è®­ç»ƒå®ƒä»¬çš„æ–¹æ³•åŒ…æ‹¬åœ¨ç»™å®šå½“å‰ï¼ˆé‡å¤ï¼‰çŠ¶æ€å’Œå‰ä¸€ä¸ªä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼Œæœ€å¤§åŒ–åºåˆ—ä¸­æ¯ä¸ªä»¤ç‰Œçš„å¯èƒ½æ€§ã€‚ åœ¨æ¨ç†æ—¶ï¼ŒæœªçŸ¥çš„å‰ä¸€ä¸ªä»¤ç‰Œç„¶åè¢«æ¨¡å‹æœ¬èº«ç”Ÿæˆçš„ä»¤ç‰Œæ›¿æ¢ã€‚ è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„è¿™ç§å·®å¼‚å¯ä»¥äº§ç”Ÿé”™è¯¯ï¼Œè¿™äº›é”™è¯¯å¯ä»¥æ²¿ç€ç”Ÿæˆçš„åºåˆ—å¿«é€Ÿç´¯ç§¯ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå°†è®­ç»ƒè¿‡ç¨‹ä»ä½¿ç”¨çœŸå®çš„å‰ä¸€ä¸ªä»¤ç‰Œçš„å®Œå…¨æŒ‡å¯¼æ–¹æ¡ˆè½»è½»åœ°æ”¹å˜ä¸ºä½¿ç”¨ç”Ÿæˆçš„ä»¤ç‰Œçš„è¾ƒå°‘æŒ‡å¯¼æ–¹æ¡ˆã€‚ åœ¨å¤šä¸ªåºåˆ—é¢„æµ‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ æ­¤å¤–ï¼Œå®ƒè¿˜æˆåŠŸåœ°ç”¨äºæˆ‘ä»¬èµ¢å¾—2015å¹´MSCOCOå›¾åƒå­—å¹•æŒ‘æˆ˜èµ›çš„å‚èµ›ä½œå“ã€‚</p>

<h5 id="101-shen-shiqi-cheng-yong-he-zhongjun-et-alminimum-risk-training-for-neural-machine-translationproceedings-of-the-54th-annual-meeting-of-the-association-for-computational-linguistics-acl-2016-berlin-germany-20161683-1692">[101] Shen Shiqi, Cheng Yong, He Zhongjun, et al.Minimum risk training for neural machine translation//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) .Berlin, Germany, 2016:1683-1692</h5>
<p>We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.
æå‡ºäº†ç«¯åˆ°ç«¯ç¥ç»æœºå™¨ç¿»è¯‘çš„æœ€å°é£é™©è®­ç»ƒæ–¹æ³•ã€‚ ä¸ä¼ ç»Ÿçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸åŒï¼Œæœ€å°é£é™©è®­ç»ƒèƒ½å¤Ÿç›´æ¥é’ˆå¯¹ä¸ä¸€å®šå¯åŒºåˆ†çš„ä»»æ„è¯„ä¼°åº¦é‡æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚ å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒè¯­è¨€å¯¹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸Šå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚ å¯¹äºä½“ç³»ç»“æ„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€æ˜çš„ï¼Œå¯ä»¥åº”ç”¨äºæ›´å¤šçš„ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸”æ½œåœ¨åœ°å—ç›Šäºæ›´å¤šçš„NLPä»»åŠ¡ã€‚</p>

<h5 id="102-bahdanau-d-brakel-p-lowe-r-et-alan-actor-critic-algorithm-for-sequence-predictionarxiv-preprint160707086v2-2016">[102] Bahdanau D, Brakel P, Lowe R, et al.An actor-critic algorithm for sequence prediction.arXiv preprint/1607.07086v2, 2016</h5>
<p>We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (RL)çš„ç¥ç»ç½‘ç»œè®­ç»ƒæ–¹æ³•ã€‚ å½“å‰çš„å¯¹æ•°ä¼¼ç„¶è®­ç»ƒæ–¹æ³•ç”±äºè®­ç»ƒå’Œæµ‹è¯•æ¨¡å¼ä¹‹é—´çš„å·®å¼‚è€Œå—åˆ°é™åˆ¶ï¼Œå› ä¸ºæ¨¡å‹å¿…é¡»ç”ŸæˆåŸºäºå…ˆå‰çŒœæµ‹çš„ä»¤ç‰Œï¼Œè€Œä¸æ˜¯åŸºäºå®é™…æƒ…å†µçš„ä»¤ç‰Œã€‚ æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªæ‰¹è¯„æ€§ç½‘ç»œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç½‘ç»œè¢«è®­ç»ƒæˆåœ¨ç»™å®šå‚ä¸è€…ç½‘ç»œç­–ç•¥çš„æƒ…å†µä¸‹é¢„æµ‹è¾“å‡ºä»¤ç‰Œçš„å€¼ã€‚ è¿™å¯¼è‡´äº†ä¸€ä¸ªæ›´æ¥è¿‘æµ‹è¯•é˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å…è®¸æˆ‘ä»¬ç›´æ¥ä¼˜åŒ–ç‰¹å®šäºä»»åŠ¡çš„å¾—åˆ†ï¼Œä¾‹å¦‚BLEUã€‚ è‡³å…³é‡è¦çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬åœ¨ç›‘ç£å­¦ä¹ ç¯å¢ƒä¸­è€Œä¸æ˜¯åœ¨ä¼ ç»Ÿçš„RLç¯å¢ƒä¸­åˆ©ç”¨è¿™äº›æŠ€æœ¯ï¼Œå› æ­¤æˆ‘ä»¬å°†æ‰¹è¯„æ€§ç½‘ç»œè®¾ç½®ä¸ºå®é™…çš„çœŸç†è¾“å‡ºã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆä»»åŠ¡å’Œå¾·è‹±æœºå™¨ç¿»è¯‘ä¸­éƒ½èƒ½æé«˜æ€§èƒ½ã€‚ æˆ‘ä»¬çš„åˆ†æä¸ºè¿™äº›æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€å­—å¹•ç”Ÿæˆå’Œå¯¹è¯å»ºæ¨¡ã€‚</p>

<h5 id="103-wiseman-s-rush-a-msequence-to-sequence-learning-as-beam-search-optimizationproceedings-of-the-2016conference-on-empirical-methods-in-natural-language-processing-emnlp-2016-austin-usa-20161296-1302">[103] Wiseman S, Rush A M.Sequence-to-Sequence learning as beam-search optimization//Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing (EMNLP 2016) .Austin, USA, 2016:1296-1302</h5>
<p>Sequence-to-Sequence (seq2seq) modeling has rapidly become an important generalpurpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beamsearch training scheme, based on the work of Daume III and Marcu (2005), that extends Â´ seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highlyoptimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.
åºåˆ—å¯¹åºåˆ—(SEQ2SEQ)å»ºæ¨¡å·²è¿…é€Ÿæˆä¸ºä¸€ç§é‡è¦çš„é€šç”¨è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆå’Œåºåˆ—æ ‡æ³¨ç­‰é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ SEQ2SEQå»ºç«‹åœ¨æ·±åº¦ç¥ç»è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œç»§æ‰¿äº†å®ƒåœ¨ä¼°è®¡å±€éƒ¨ã€ä¸‹ä¸€ä¸ªå•è¯åˆ†å¸ƒæ–¹é¢çš„æ˜¾è‘—ç²¾ç¡®åº¦ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨Daume IIIå’ŒMarcu(2005)çš„å·¥ä½œåŸºç¡€ä¸Šå¼•å…¥äº†ä¸€ç§æ¨¡å‹å’Œæ³¢æŸæœç´¢è®­ç»ƒæ–¹æ¡ˆï¼Œå°†SEQ2SEQæ‰©å±•åˆ°å­¦ä¹ å…¨å±€åºåˆ—åˆ†æ•°ã€‚ è¯¥ç»“æ„åŒ–æ–¹æ³•é¿å…äº†ä¸å±€éƒ¨è®­ç»ƒç›¸å…³çš„ç»å…¸åå·®ï¼Œå¹¶å°†è®­ç»ƒæŸå¤±ä¸æµ‹è¯•æ—¶é—´çš„ä½¿ç”¨ç»Ÿä¸€èµ·æ¥ï¼ŒåŒæ—¶ä¿ç•™äº†SEQ2SEQç»è¿‡éªŒè¯çš„æ¨¡å‹ä½“ç³»ç»“æ„åŠå…¶æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ã€‚ æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨æ’åºä»»åŠ¡çš„ä¸‰ä¸ªä¸åŒåºåˆ—ï¼ˆè¯åºã€è§£æå’Œæœºå™¨ç¿»è¯‘ï¼‰ä¸Šä¼˜äºé«˜åº¦ä¼˜åŒ–çš„åŸºäºæ³¨æ„çš„SEQ2SEQç³»ç»Ÿå’Œå…¶ä»–åŸºçº¿ã€‚</p>

<h5 id="104-norouzi-m-bengio-s-chen-zhifeng-et-alreward-augmented-maximum-likelihood-for-neural-structured-predictionproceedings-of-the-neural-information-processing-systems-nips-2016-barcelona-spain-20161723-1731">[104] Norouzi M, Bengio S, Chen Zhifeng, et al.Reward augmented maximum likelihood for neural structured prediction//Proceedings of the Neural Information Processing Systems (NIPS 2016) .Barcelona, Spain, 2016:1723-1731</h5>
<p>A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. By establishing a link between the log-likelihood and expected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated scaled rewards. Accordingly, we present a framework to smooth the predictive probability of the outputs using their corresponding rewards. We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards. Experiments on neural sequence to sequence models for speech recognition and machine translation show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.
ç»“æ„åŒ–è¾“å‡ºé¢„æµ‹ä¸­çš„ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯ç›´æ¥ä¼˜åŒ–æµ‹è¯•è¯„ä¼°æ‰€éœ€çš„ä»»åŠ¡å¥–åŠ±å‡½æ•°ã€‚ æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†ä»»åŠ¡æŠ¥é…¬çº³å…¥æœ€å¤§ä¼¼ç„¶æ¡†æ¶çš„ç®€å•ä¸”è®¡ç®—æ•ˆç‡é«˜çš„æ–¹æ³•ã€‚ é€šè¿‡å»ºç«‹å¯¹æ•°ä¼¼ç„¶ä¸æœŸæœ›æŠ¥é…¬ç›®æ ‡ä¹‹é—´çš„è”ç³»ï¼Œæˆ‘ä»¬è¯æ˜äº†å½“ç»™å®šè¾“å…¥çš„è¾“å‡ºçš„æ¡ä»¶åˆ†å¸ƒä¸å…¶æŒ‡æ•°æ¯”ä¾‹æŠ¥é…¬æˆæ­£æ¯”æ—¶ï¼Œè·å¾—æœ€ä¼˜æ­£åˆ™åŒ–æœŸæœ›æŠ¥é…¬ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œä»¥å¹³æ»‘çš„é¢„æµ‹æ¦‚ç‡çš„è¾“å‡ºä½¿ç”¨å…¶ç›¸åº”çš„å¥–åŠ±ã€‚ æˆ‘ä»¬ä¼˜åŒ–äº†å¢åŠ è¾“å‡ºçš„æ¡ä»¶å¯¹æ•°æ¦‚ç‡ï¼Œè¿™äº›è¾“å‡ºæ˜¯æŒ‰æŒ‡æ•°æ¯”ä¾‹é‡‡æ ·çš„ã€‚ åœ¨è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘çš„ç¥ç»åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„å®éªŒä¸­ï¼Œä½¿ç”¨å¥–åŠ±å¢å¼ºçš„æœ€å¤§ä¼¼ç„¶(RML)å¯¹æœ€å¤§ä¼¼ç„¶åŸºçº¿è¿›è¡Œäº†æ˜¾è‘—æ”¹è¿›ï¼Œå…¶ä¸­å¥–åŠ±è¢«å®šä¹‰ä¸ºè¾“å‡ºå’Œåœ°é¢çœŸå®æ ‡ç­¾ä¹‹é—´çš„è´Ÿç¼–è¾‘è·ç¦»ã€‚</p>

<h5 id="105-calixto-i-liu-q-campbell-ndoubly-attentive-decoder-for-multi-modal-neural-machine-translationproceedings-of-the55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-20171913-1924">[105] Calixto I, Liu Q, Campbell N.Doubly-attentive decoder for multi-modal neural machine translation//Proceedings of the55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1913-1924</h5>
<p>We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ç¥ç»æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸­çš„åŒæ³¨æ„è¯‘ç å™¨è‡ªç„¶åœ°èåˆäº†åˆ©ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œè·å¾—çš„ç©ºé—´è§†è§‰ç‰¹å¾ï¼Œå¼¥è¡¥äº†å›¾åƒæè¿°ä¸ç¿»è¯‘ä¹‹é—´çš„å·®è·ã€‚ æˆ‘ä»¬çš„è§£ç å™¨åœ¨ç›®æ ‡è¯­è¨€ä¸­ç”Ÿæˆå•è¯æ—¶ï¼Œé€šè¿‡ä¸¤ä¸ªç‹¬ç«‹çš„æ³¨æ„æœºåˆ¶ç‹¬ç«‹åœ°æ³¨æ„æºè¯­è¨€å•è¯å’Œå›¾åƒçš„éƒ¨åˆ†ã€‚ æˆ‘ä»¬å‘ç°ï¼Œè¯¥æ¨¡å‹ä¸ä»…èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨é¢†åŸŸå†…çš„å¤šæ¨¡æ€æ•°æ®ï¼Œè€Œä¸”èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨å¤§å‹é€šç”¨é¢†åŸŸçš„çº¯æ–‡æœ¬å¤šæ¨¡æ€è¯­æ–™åº“ã€‚ æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†å¤š30Kæ•°æ®é›†çš„æœ€æ–°ç»“æœã€‚</p>

<h5 id="106-delbrouck-j-b-dupont-san-empirical-study-on-the-effectiveness-of-images-in-multimodal-neural-machine-translationproceedings-of-the-2017conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-2017921-930">[106] Delbrouck J-B, Dupont S.An empirical study on the effectiveness of images in multimodal neural machine translation//Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:921-930</h5>
<p>In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image â†’ German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.
åœ¨æœ€å…ˆè¿›çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ä¸­ï¼Œåœ¨è§£ç æœŸé—´ä½¿ç”¨æ³¨æ„æœºåˆ¶æ¥å¢å¼ºç¿»è¯‘ã€‚ åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œè§£ç å™¨ä½¿ç”¨è¿™ç§æœºåˆ¶æ¥å…³æ³¨æºå¥çš„ä¸åŒéƒ¨åˆ†ï¼Œä»¥åœ¨è¾“å‡ºå…¶ç›®æ ‡å­—ä¹‹å‰æ”¶é›†æœ€æœ‰ç”¨çš„ä¿¡æ¯ã€‚ è¿‘å¹´æ¥ï¼Œæ³¨æ„æœºåˆ¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä¹Ÿå¾—åˆ°äº†ç ”ç©¶ï¼Œåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œäººä»¬å¯ä»¥åŒæ—¶å…³æ³¨å¥å­éƒ¨åˆ†å’Œå›¾åƒåŒºåŸŸã€‚ æœ¬æ–‡æ¯”è¾ƒäº†å¤šæ¨¡æ€ç¿»è¯‘ä»»åŠ¡ï¼ˆè‹±è¯­ã€å›¾åƒâ†’å¾·è¯­ï¼‰ä¸­çš„å‡ ç§æ³¨æ„æœºåˆ¶ï¼Œå¹¶è¯„ä»·äº†è¯¥æ¨¡å‹åˆ©ç”¨å›¾åƒæé«˜ç¿»è¯‘è´¨é‡çš„èƒ½åŠ›ã€‚ æˆ‘ä»¬åœ¨å¤šä¸ª30Kæ•°æ®é›†ä¸Šçš„å¾—åˆ†è¶…è¿‡äº†æœ€å…ˆè¿›çš„å¾—åˆ†ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨ç¿»è¯‘æ—¶è¯†åˆ«å¹¶æŠ¥å‘Šäº†æœºå™¨çš„ä¸åŒé”™è¯¯è¡Œä¸ºã€‚</p>

<h5 id="107-calixto-i-liu-qunincorporating-global-visual-features-into-attention-based-neural-machine-translationproceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing-emnlp-2017-copenhagen-denmark-20171003-1014">[107] Calixto I, Liu Qun.Incorporating global visual features into attention-based neural machine translation//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) .Copenhagen, Denmark, 2017:1003-1014</h5>
<p>We introduce multi-modal, attentionbased Neural Machine Translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into English and German, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable Phrase-Based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set.
æˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€çš„ã€åŸºäºæ³¨æ„çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è§†è§‰ç‰¹å¾ç»“åˆåˆ°ç¼–ç å™¨å’Œè§£ç å™¨çš„ä¸åŒéƒ¨åˆ†ä¸­ã€‚ ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œæå–å…¨å±€å›¾åƒç‰¹å¾ï¼Œå¹¶ä¸”å°†å…¨å±€å›¾åƒç‰¹å¾å¹¶å…¥(i)ä½œä¸ºæºå¥å­ä¸­çš„å•è¯ï¼Œ(ii)ç”¨äºåˆå§‹åŒ–ç¼–ç å™¨éšè—çŠ¶æ€ï¼Œä»¥åŠ(iii)ä½œä¸ºç”¨äºåˆå§‹åŒ–è§£ç å™¨éšè—çŠ¶æ€çš„é™„åŠ æ•°æ®ã€‚ åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è‹±æ–‡å’Œå¾·æ–‡çš„ç¿»è¯‘ï¼Œä¸åŒçš„èå…¥å…¨çƒå›¾åƒç‰¹å¾çš„ç­–ç•¥å¦‚ä½•æ¯”è¾ƒï¼Œä»¥åŠå“ªäº›ç­–ç•¥è¡¨ç°æœ€å¥½ã€‚ æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ·»åŠ åˆæˆå¤šæ¨¡æ€ã€å¤šè¯­ç§æ•°æ®å¯¹å¤šæ¨¡æ€æ¨¡å‹çš„å½±å“ï¼Œå‘ç°æ·»åŠ æ•°æ®å¯¹å¤šæ¨¡æ€æ¨¡å‹æœ‰æ­£å‘å½±å“ã€‚ æˆ‘ä»¬æŠ¥å‘Šäº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹ä¹Ÿæ˜¾è‘—æ”¹è¿›äº†åŸºäºçŸ­è¯­çš„ç»Ÿè®¡MT(PBSMT)æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®è¯„ä¼°çš„æ‰€æœ‰åº¦é‡åœ¨å¤š30Kæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡çº¯ç²¹çš„ç¥ç»æ¨¡å‹æ¯”PBSMTæ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šè¯„ä¼°çš„æ‰€æœ‰åº¦é‡ä¸Šéƒ½æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>

<h5 id="108-caglayan-o-aransa-w-wang-y-et-aldoes-multimodality-help-human-and-machine-for-translation-and-image-captioningproceedings-of-the-1st-conference-on-machine-translation-wmt-2016-berlin-germany-2016627-633">[108] Caglayan O, Aransa W, Wang Y, et al.Does multimodality help human and machine for translation and image captioning?//Proceedings of the 1st Conference on Machine Translation (WMT 2016) .Berlin, Germany, 2016:627-633</h5>
<p>This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.
æœ¬æ–‡ä»‹ç»äº†LIUMå…¬å¸å’ŒCVCå…¬å¸ä¸ºåº”å¯¹WMT16å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘æŒ‘æˆ˜è€Œå¼€å‘çš„ç³»ç»Ÿã€‚ æˆ‘ä»¬æ¢ç´¢äº†å„ç§æ¯”è¾ƒæ–¹æ³•ï¼Œå³åŸºäºçŸ­è¯­çš„ç³»ç»Ÿå’Œä½¿ç”¨å•æ¨¡æ€æˆ–å¤šæ¨¡æ€æ•°æ®è®­ç»ƒçš„æ³¨æ„é€’å½’ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ ä¸ºäº†è¯„ä¼°å¤šæ¨¡æ€æ•°æ®å¯¹æœºå™¨ç¿»è¯‘å’Œå›¾åƒæè¿°ç”Ÿæˆçš„æœ‰ç”¨æ€§ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†äººçš„è¯„ä¼°ã€‚ æ ¹æ®è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡BLEUå’ŒMeteorï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸­éƒ½è·å¾—äº†æœ€å¥½çš„ç»“æœã€‚</p>

<h5 id="109-gehring-j-auli-m-grangier-d-et-alconvolutional-sequence-to-sequence-learningarxiv-preprint170503122v3-2017">[109] Gehring J, Auli M, Grangier D, et al.Convolutional sequence to sequence learning.arXiv preprint/1705.03122v3, 2017</h5>
<p>The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMTâ€™14 English-German and WMTâ€™14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.?
åºåˆ—å­¦ä¹ çš„æ™®éæ–¹æ³•æ˜¯é€šè¿‡é€’å½’ç¥ç»ç½‘ç»œå°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°å¯å˜é•¿åº¦çš„è¾“å‡ºåºåˆ—ã€‚ ä»‹ç»äº†ä¸€ç§å®Œå…¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚ ä¸é€’å½’æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨è®­ç»ƒæœŸé—´å¯ä»¥å®Œå…¨å¹¶è¡ŒåŒ–æ‰€æœ‰å…ƒç´ çš„è®¡ç®—ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨GPUç¡¬ä»¶ï¼Œå¹¶ä¸”ä¼˜åŒ–æ›´å®¹æ˜“ï¼Œå› ä¸ºéçº¿æ€§çš„æ•°ç›®æ˜¯å›ºå®šçš„å¹¶ä¸”ä¸è¾“å…¥é•¿åº¦æ— å…³ã€‚ é—¨æ§çº¿æ€§å•å…ƒçš„ä½¿ç”¨ç®€åŒ–äº†æ¢¯åº¦ä¼ æ’­ï¼Œå¹¶ä¸”æˆ‘ä»¬ä¸ºæ¯ä¸ªè§£ç å™¨å±‚é…å¤‡äº†å•ç‹¬çš„å…³æ³¨æ¨¡å—ã€‚ æˆ‘ä»¬çš„æ€§èƒ½ä¼˜äºWuç­‰äººçš„æ·±åº¦LSTMè®¾ç½®çš„ç²¾åº¦ã€‚ ï¼ˆ2016ï¼‰åœ¨WMTâ€™14è‹±å¾·ç¿»è¯‘å’ŒWMTâ€™14è‹±æ³•ç¿»è¯‘ä¸Šï¼ŒGPUå’ŒCPUä¸Šçš„é€Ÿåº¦éƒ½å¿«äº†ä¸€ä¸ªæ•°é‡çº§ã€‚</p>

<h5 id="110-vaswani-a-shazeer-n-parmar-n-et-alattention-is-all-you-needarxiv-preprint170603762v4-2017">[110] Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need.arXiv preprint/1706.03762v4, 2017</h5>
<p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiï¬cantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.
æ˜¾æ€§åºåˆ—è½¬å¯¼æ¨¡å‹åŸºäºåŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨çš„å¤æ‚çš„é€’å½’æˆ–å·ç§¯ç¥ç»ç½‘ç»œã€‚ æ€§èƒ½æœ€å¥½çš„å‹å·è¿˜é€šè¿‡ä¸€ä¸ªæ³¨æ„æœºåˆ¶è¿æ¥ç¼–ç å™¨å’Œè§£ç å™¨ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®€å•çš„ç½‘ç»œç»“æ„&amp;å˜å‹å™¨ï¼Œå®ƒå®Œå…¨åŸºäºæ³¨æ„æœºåˆ¶ï¼Œå®Œå…¨é¿å…äº†é‡å¤å’Œå·ç§¯ã€‚ åœ¨ä¸¤ä¸ªæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹å…·æœ‰æ›´é«˜çš„è´¨é‡ï¼ŒåŒæ—¶å…·æœ‰æ›´å¼ºçš„å¹¶è¡Œæ€§ï¼Œå¹¶ä¸”è®­ç»ƒæ—¶é—´æ˜æ˜¾å‡å°‘ã€‚ æˆ‘ä»¬çš„æ¨¡å‹åœ¨2014å¹´WMTè‹±å¾·ç¿»è¯‘ä»»åŠ¡ä¸­å®ç°äº†28.4BLEUï¼Œæ¯”ç°æœ‰çš„æœ€ä½³ç»“æœï¼ˆåŒ…æ‹¬é›†æˆï¼‰æé«˜äº†2BLEUä»¥ä¸Šã€‚ åœ¨WMT2014è‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨8ä¸ªGPUä¸Šè¿›è¡Œ3.5å¤©çš„åŸ¹è®­åï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„å•æ¨¡å‹æœ€æ–°BLEUè¯„åˆ†ä¸º41.0ï¼Œè¿™åªæ˜¯æ–‡çŒ®ä¸­æœ€ä½³æ¨¡å‹åŸ¹è®­æˆæœ¬çš„ä¸€å°éƒ¨åˆ†ã€‚</p>

<h5 id="111-he-di-xia-yingce-qin-tao-et-aldual-learning-for-machine-translationproceedings-of-the-30th-conference-on-neural-information-processing-systems-nips-2016-barcelona-spain-20161-9">[111] He Di, Xia Yingce, Qin Tao, et al.Dual learning for machine translation//Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS 2016) .Barcelona, Spain, 2016:1-9</h5>
<p>While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the languagemodel likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on Englishâ†”French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.
è¿‘ä¸¤å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘(NMT)å–å¾—äº†é•¿è¶³çš„å‘å±•ï¼Œä½†å…¶è®­ç»ƒéœ€è¦æ•°åƒä¸‡å¯¹åŒè¯­å¥å­ã€‚ ç„¶è€Œï¼Œäººçš„æ ‡ç­¾æ˜¯éå¸¸æ˜‚è´µçš„ã€‚ ä¸ºäº†è§£å†³è¿™ä¸€è®­ç»ƒæ•°æ®ç“¶é¢ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŒå­¦ä¹ æœºåˆ¶ï¼Œä½¿NMTç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡åŒå­¦ä¹ æ¸¸æˆä»æœªæ ‡è®°çš„æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ ã€‚ è¿™ä¸€æœºåˆ¶å—åˆ°ä»¥ä¸‹è§‚å¯Ÿçš„å¯å‘:ä»»ä½•æœºå™¨ç¿»è¯‘ä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªåŒé‡ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼Œè‹±è¯­-æ³•è¯­ç¿»è¯‘ï¼ˆåŸè¯­ï¼‰å’Œæ³•è¯­-è‹±è¯­ç¿»è¯‘ï¼ˆåŒè¯­ï¼‰ï¼› åŸå§‹å’ŒåŒé‡ä»»åŠ¡å¯ä»¥å½¢æˆä¸€ä¸ªé—­ç¯ï¼Œå¹¶äº§ç”Ÿä¿¡æ¯åé¦ˆä¿¡å·æ¥è®­ç»ƒç¿»è¯‘æ¨¡å‹ï¼Œå³ä½¿æ²¡æœ‰äººç±»æ ‡è®°å™¨çš„å‚ä¸ã€‚ åœ¨åŒå­¦ä¹ æœºåˆ¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªAgentæ¥è¡¨ç¤ºåŸå§‹ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦ä¸€ä¸ªAgentæ¥è¡¨ç¤ºåŒä»»åŠ¡çš„æ¨¡å‹ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹è¦æ±‚å®ƒä»¬ç›¸äº’å­¦ä¹ ã€‚ åŸºäºåœ¨æ­¤è¿‡ç¨‹ä¸­äº§ç”Ÿçš„åé¦ˆä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹è¾“å‡ºçš„è¯­è¨€æ¨¡å‹ä¼¼ç„¶æ€§ï¼Œä»¥åŠåŸå§‹å’Œå¯¹å¶ç¿»è¯‘ååŸå§‹å¥å­çš„é‡æ„è¯¯å·®ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥è¿­ä»£åœ°æ›´æ–°è¿™ä¸¤ä¸ªæ¨¡å‹ç›´åˆ°æ”¶æ•›ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼‰ã€‚ æˆ‘ä»¬æŠŠç›¸åº”çš„ç¥ç»æœºå™¨ç¿»è¯‘æ–¹æ³•ç§°ä¸ºåŒNMTã€‚ å®éªŒè¡¨æ˜ï¼ŒåŒNMTåœ¨è‹±æ³•ç¿»è¯‘ä¸­æœ‰å¾ˆå¥½çš„æ•ˆæœï¼› ç‰¹åˆ«æ˜¯ï¼Œé€šè¿‡å­¦ä¹ å•è¯­æ•°æ®ï¼ˆ10%çš„åŒè¯­æ•°æ®ä½œä¸ºWARM STARTï¼‰ï¼Œå®ƒè¾¾åˆ°äº†ä¸å…¨åŒè¯­æ•°æ®è®­ç»ƒçš„NMTç›¸å½“çš„å‡†ç¡®ç‡ï¼Œç”¨äºæ³•è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ä»»åŠ¡ã€‚</p>

<h5 id="112-nguyen-k-daume-iii-h-boyd-graber-jreinforcement-learning-for-bandit-neural-machine-translation-with-simulated-human-feedbackproceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing-emnlp2017-copenhagen-denmark-20171465-1475">[112] Nguyen K, Daume III H, Boyd-Graber J.Reinforcement learning for bandit neural machine translation with simulated human feedback//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP2017) .Copenhagen, Denmark, 2017:1465-1475</h5>
<p>Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoderdecoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.
æœºå™¨ç¿»è¯‘æ˜¯ä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªè‡ªç„¶çš„å€™é€‰é—®é¢˜:ç”¨æˆ·å¯¹å€™é€‰ç¿»è¯‘æä¾›å¿«é€Ÿã€è‚®è„çš„è¯„çº§ï¼Œä»¥æŒ‡å¯¼ç³»ç»Ÿæ”¹è¿›ã€‚ ç„¶è€Œï¼Œç›®å‰çš„ç¥ç»æœºå™¨ç¿»è¯‘è®­ç»ƒä¸»è¦é›†ä¸­åœ¨æ˜‚è´µçš„äººå·¥ç”Ÿæˆçš„å‚è€ƒè¯‘æ–‡ä¸Šã€‚ æˆ‘ä»¬æè¿°äº†ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•ä»æ¨¡æ‹Ÿäººçš„åé¦ˆä¸­æ”¹è¿›ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿã€‚ æˆ‘ä»¬çš„ç®—æ³•ç»“åˆäº†Advantage Actor-Criticç®—æ³•ï¼ˆMNIHç­‰äººï¼Œ2016ï¼‰å’ŒåŸºäºæ³¨æ„åŠ›çš„ç¥ç»ç¼–ç å™¨è§£ç å™¨æ¶æ„ï¼ˆLuongç­‰äººï¼Œ2015ï¼‰ã€‚ è¯¥ç®—æ³•èƒ½æœ‰æ•ˆåœ°ä¼˜åŒ–ä¼ ç»Ÿçš„è¯­æ–™åº“çº§æœºå™¨ç¿»è¯‘åº¦é‡ï¼Œå¯¹æ¨¡æ‹Ÿäººç±»å®é™…è¡Œä¸ºçš„åæ–œã€é«˜æ–¹å·®ã€ç²’åº¦åé¦ˆå…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ã€‚</p>

<h5 id="113-yang-zhen-chen-wei-wang-feng-et-alimproving-neural-machine-translation-with-conditional-sequence-generative-adversarial-netsarxiv-preprint170304887v2-2017">[113] Yang Zhen, Chen Wei, Wang Feng, et al.Improving neural machine translation with conditional sequence generative adversarial nets.arXiv preprint/1703.04887v2, 2017</h5>
<p>This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from humantranslated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.
æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†GANSåº”ç”¨äºNMTçš„æ–¹æ³•ã€‚ æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ¡ä»¶åºåˆ—ç”Ÿæˆå¯¹æŠ—æ€§ç½‘ç»œï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå­æ¨¡å‹ï¼Œä¸€ä¸ªç”Ÿæˆå™¨å’Œä¸€ä¸ªé‰´åˆ«å™¨ã€‚ ç”Ÿæˆå™¨çš„ç›®çš„æ˜¯ç”Ÿæˆéš¾ä»¥åŒºåˆ«äºäººå·¥ç¿»è¯‘çš„å¥å­ï¼ˆå³é‡‘è‰²ç›®æ ‡å¥ï¼‰ï¼› é‰´åˆ«è€…åŠªåŠ›åŒºåˆ†æœºå™¨ç”Ÿæˆçš„å¥å­å’Œäººå·¥ç¿»è¯‘çš„å¥å­ã€‚ è¿™ä¸¤ä¸ªå­æ¨¡å‹åœ¨è¾¾åˆ°çº³ä»€å‡è¡¡æ—¶è¿›è¡Œæœ€å°-æœ€å¤§åšå¼ˆï¼Œè¾¾åˆ°åŒèµ¢ã€‚ æ­¤å¤–ï¼Œé™æ€å¥å­çº§BLEUè¢«ç”¨ä½œç”Ÿæˆå™¨çš„å¢å¼ºç›®æ ‡ï¼Œè¿™å°†ç”Ÿæˆå™¨åå‘äºé«˜BLEUç‚¹ã€‚ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨åŠ¨æ€é‰´åˆ«å™¨å’Œé™æ€BLEUç›®æ ‡å¯¹ç”Ÿæˆçš„å¥å­è¿›è¡Œè¯„ä»·ï¼Œå¹¶å¯¹è¯„ä»·ç»“æœè¿›è¡Œåé¦ˆï¼Œä»¥æŒ‡å¯¼ç”Ÿæˆå™¨çš„å­¦ä¹ ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è‹±å¾·å’Œæ±‰è‹±ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ€§èƒ½å‡ä¼˜äºä¼ ç»Ÿçš„RNNæœç´¢å’Œæœ€æ–°å‡ºç°çš„æœ€å…ˆè¿›çš„è½¬æ¢ç®—æ³•ã€‚</p>

<h5 id="114-wu-lijun-xia-yingce-zhao-li-et-aladversarial-neural-machine-translationarxiv-preprint170406933v3-2017">[114] Wu Lijun, Xia Yingce, Zhao Li, et al.Adversarial neural machine translation.arXiv preprint/1704.06933v3, 2017</h5>
<p>In this paper, we study a new learning paradigm for neural machine translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by an NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as AdversarialNMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed 2D convolutional neural network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on Englishâ†’French and Germanâ†’English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.
æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ–°çš„ç¥ç»æœºå™¨ç¿»è¯‘(NMT)å­¦ä¹ èŒƒå¼ã€‚ æˆ‘ä»¬ä¸æ˜¯åƒå‰äººé‚£æ ·æœ€å¤§åŒ–äººå·¥ç¿»è¯‘çš„å¯èƒ½æ€§ï¼Œè€Œæ˜¯æœ€å°åŒ–äººå·¥ç¿»è¯‘å’ŒNMTæ¨¡å‹ç»™å‡ºçš„ç¿»è¯‘ä¹‹é—´çš„åŒºåˆ«ã€‚ ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œåœ¨æœ€è¿‘ç”Ÿæˆæ€§å¯¹æŠ—ç½‘ç»œ(GANS)æˆåŠŸçš„å¯å‘ä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¯¹æŠ—è®­ç»ƒä½“ç³»ç»“æ„ï¼Œå¹¶å°†å…¶å‘½åä¸ºå¯¹æŠ—NMTã€‚ åœ¨å¯¹æŠ—-NMTä¸­ï¼ŒNMTæ¨¡å‹çš„è®­ç»ƒç”±ä¸€ä¸ªå¯¹æ‰‹è¾…åŠ©ï¼Œè¯¥å¯¹æ‰‹æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„2Då·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€‚ å¯¹æ‰‹çš„ç›®æ ‡æ˜¯å°†NMTæ¨¡å‹äº§ç”Ÿçš„ç¿»è¯‘ç»“æœä¸äººç±»äº§ç”Ÿçš„ç¿»è¯‘ç»“æœåŒºåˆ†å¼€æ¥ã€‚ NMTæ¨¡å¼çš„ç›®æ ‡æ˜¯äº§ç”Ÿé«˜è´¨é‡çš„ç¿»è¯‘ï¼Œä»è€Œæ¬ºéª—å¯¹æ‰‹ã€‚ åˆ©ç”¨ç­–ç•¥æ¢¯åº¦æ³•å¯¹NMTæ¨¡å‹å’Œæ•Œæ‰‹è¿›è¡Œè”åˆè®­ç»ƒã€‚ åœ¨è‹±è¯­â†’æ³•è¯­å’Œå¾·è¯­â†’è‹±è¯­ç¿»è¯‘ä»»åŠ¡ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåä¹‰NMTæ¯”å‡ ç§å¼ºåŸºçº¿èƒ½è·å¾—æ›´å¥½çš„ç¿»è¯‘è´¨é‡ã€‚</p>

<h5 id="115-shah-k-logacheva-v-paetzold-g-hshef-nntranslation-quality-estimation-with-neural-networksproceedings-of-the10th-workshop-on-statistical-machine-translationlisbon-portugal-2015342-347">[115] Shah K, Logacheva V, Paetzold G H.SHEF-NN:Translation quality estimation with neural networks//Proceedings of the10th Workshop on Statistical Machine Translation.Lisbon, Portugal, 2015:342-347</h5>
<p>We describe our systems for Tasks 1 and 2 of the WMT15 Shared Task on Quality Estimation. Our submissions use (i) a continuous space language model to extract additional features for Task 1 (SHEFGP, SHEF-SVM), (ii) a continuous bagof-words model to produce word embeddings as features for Task 2 (SHEF-W2V) and (iii) a combination of features produced by QuEst++ and a feature produced with word embedding models (SHEFQuEst++). Our systems outperform the baseline as well as many other submissions. The results are especially encouraging for Task 2, where our best performing system (SHEF-W2V) only uses features learned in an unsupervised fashion.
æˆ‘ä»¬æè¿°äº†WMT15è´¨é‡è¯„ä¼°å…±äº«ä»»åŠ¡çš„ä»»åŠ¡1å’Œä»»åŠ¡2çš„ç³»ç»Ÿã€‚ æˆ‘ä»¬æäº¤çš„ææ–™ä½¿ç”¨(i)è¿ç»­ç©ºé—´è¯­è¨€æ¨¡å‹æ¥æå–ä»»åŠ¡1çš„é™„åŠ ç‰¹å¾(SHEFGPï¼ŒSHEF-SVMï¼‰ï¼Œ(ii)è¿ç»­Bagof-Wordsæ¨¡å‹æ¥ç”Ÿæˆä½œä¸ºä»»åŠ¡2ç‰¹å¾çš„å•è¯åµŒå…¥(SHEF-W2V)ï¼Œä»¥åŠ(iii)ç”±Quest++ç”Ÿæˆçš„ç‰¹å¾å’Œç”±å•è¯åµŒå…¥æ¨¡å‹ç”Ÿæˆçš„ç‰¹å¾çš„ç»„åˆ(SHEFQuest+++)ã€‚ æˆ‘ä»¬çš„ç³»ç»Ÿæ€§èƒ½ä¼˜äºåŸºçº¿å’Œè®¸å¤šå…¶ä»–æäº¤çš„æ•°æ®ã€‚ ä»»åŠ¡2çš„ç»“æœå°¤å…¶ä»¤äººé¼“èˆï¼Œåœ¨ä»»åŠ¡2ä¸­ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ€§èƒ½ç³»ç»Ÿ(SHEF-W2V)åªä½¿ç”¨ä»¥æ— äººç›‘ç£çš„æ–¹å¼å­¦ä¹ çš„ç‰¹æ€§ã€‚</p>

<h5 id="116-guzman-f-joty-s-marquez-l-et-alpairwise-neural-machine-translation-evaluationproceedings-of-the-53rd-annual-meeting-of-the-association-for-computational-linguistics-and-the-7th-international-joint-conference-on-natural-language-processing-acl-2015-beijing-china-2015805-814">[116] Guzman F, Joty S, Marquez L, et al.Pairwise neural machine translation evaluation//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL 2015) .Beijing, China, 2015:805-814</h5>
<p>We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºç¥ç»ç½‘ç»œçš„æœºå™¨ç¿»è¯‘è¯„ä»·æ¡†æ¶ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨ç»™å®šå‚è€ƒè¯‘æ–‡çš„æƒ…å†µä¸‹ï¼Œä»ä¸€å¯¹å‡è®¾ä¸­é€‰æ‹©æ›´å¥½çš„è¯‘æ–‡ã€‚ åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ¥è‡ªæŒ‡ç§°å’Œä¸¤ä¸ªå‡è®¾çš„è¯æ±‡ã€å¥æ³•å’Œè¯­ä¹‰ä¿¡æ¯è¢«å‹ç¼©æˆç›¸å¯¹è¾ƒå°çš„åˆ†å¸ƒå¼å‘é‡è¡¨ç¤ºï¼Œå¹¶è¢«é¦ˆé€åˆ°å¤šå±‚ç¥ç»ç½‘ç»œä¸­ï¼Œè¯¥å¤šå±‚ç¥ç»ç½‘ç»œå¯¹æ¯ä¸ªå‡è®¾å’ŒæŒ‡ç§°ä¹‹é—´ä»¥åŠä¸¤ä¸ªå‡è®¾ä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚ è¿™äº›ç´§å‡‘çš„è¡¨ç¤ºä¾æ¬¡åŸºäºå•è¯å’Œå¥å­çš„åµŒå…¥ï¼Œè¿™äº›åµŒå…¥æ˜¯ä½¿ç”¨ç¥ç»ç½‘ç»œå­¦ä¹ çš„ã€‚ è¯¥æ¡†æ¶æ˜¯çµæ´»çš„ï¼Œå…è®¸æœ‰æ•ˆçš„å­¦ä¹ å’Œåˆ†ç±»ï¼Œå¹¶äº§ç”Ÿä¸äººç±»çš„ç›¸å…³æ€§ï¼Œä¸ç°æœ‰æŠ€æœ¯çš„ç«äº‰ã€‚</p>

<h5 id="117-gupta-r-orasan-c-van-genabith-jrevala-simple-and-effective-machine-translation-evaluation-metric-based-on-recurrent-neural-networksproceedings-of-the-2015conference-on-empirical-methods-in-natural-language-processing-emnlp-2015-lisbon-portugal-20151066-1072">[117] Gupta R, Orasan C, van Genabith J.ReVal:A simple and effective machine translation evaluation metric based on recurrent neural networks//Proceedings of the 2015Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) .Lisbon, Portugal, 2015:1066-1072</h5>
<p>Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data.
è®¸å¤šæœ€å…ˆè¿›çš„æœºå™¨ç¿»è¯‘(MT)è¯„ä¼°åº¦é‡æ˜¯å¤æ‚çš„ï¼Œæ¶‰åŠå¤§é‡çš„å¤–éƒ¨èµ„æºï¼ˆä¾‹å¦‚ç”¨äºé‡Šä¹‰ï¼‰ï¼Œå¹¶ä¸”éœ€è¦è¿›è¡Œè°ƒä¼˜ä»¥è·å¾—æœ€ä½³ç»“æœã€‚ æå‡ºäº†ä¸€ç§åŸºäºç¨ å¯†å‘é‡ç©ºé—´å’Œé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰ï¼Œç‰¹åˆ«æ˜¯é•¿çŸ­æ—¶è®°å¿†(LSTM)ç½‘ç»œçš„ç®€å•æ›¿ä»£æ–¹æ³•ã€‚ å¯¹äºWMT-14ï¼Œæˆ‘ä»¬çš„æ–°æŒ‡æ ‡åœ¨äº”ä¸ªè¯­è¨€å¯¹ä¸­çš„ä¸¤ä¸ªè¯­è¨€å¯¹ä¸­å¾—åˆ†æœ€é«˜ï¼Œåœ¨æ‰€æœ‰è¯­è¨€å¯¹ä¸­çš„æ€»ä½“æœ€ä½³å’Œæ¬¡ä¼˜ï¼Œåˆ†åˆ«ä½¿ç”¨Spearmanå’ŒPearsonç›¸å…³ã€‚ æˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•ä»WMTæ’åæ•°æ®ä¸­è‡ªåŠ¨è®¡ç®—è®­ç»ƒæ•°æ®ã€‚</p>

<h5 id="118-shi-x-padhi-i-knight-kdoes-string-based-neural-mtlearn-source-syntaxproceedings-of-the-2016conference-on-empirical-methods-in-natural-language-processing-emnlp2016-austin-usa-20161526-1534">[118] Shi X, Padhi I, Knight K.Does string-based neural MTlearn source syntax//Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing (EMNLP2016) .Austin, USA, 2016:1526-1534</h5>
<p>We investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing.
æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªç¥ç»ç¼–ç å™¨è¯‘ç å™¨ç¿»è¯‘ç³»ç»Ÿæ˜¯å¦ä½œä¸ºè®­ç»ƒçš„å‰¯äº§å“æ¥å­¦ä¹ æºç«¯çš„å¥æ³•ä¿¡æ¯ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥æ£€æµ‹ç¼–ç å™¨æ˜¯å¦å·²ç»å­¦ä¹ äº†æœ¬åœ°å’Œå…¨å±€æºè¯­æ³•ã€‚ å¯¹ç¼–ç å™¨å­¦ä¹ åˆ°çš„è¯­æ³•ç»“æ„è¿›è¡Œç»†ç²’åº¦åˆ†æï¼Œå¯ä»¥å‘ç°å­¦ä¹ åˆ°çš„è¯­æ³•ç±»å‹å’Œç¼ºå°‘çš„è¯­æ³•ç±»å‹ã€‚</p>

<h5 id="119-ding-yanzhuo-liu-yang-luan-huanbo-et-alvisualizing-and-understanding-neural-machine-translationproceedings-of-the-55th-annual-meeting-of-the-association-for-computational-linguistics-acl-2017-vancouver-canada-20171150-1159">[119] Ding Yanzhuo, Liu Yang, Luan Huanbo, et al.Visualizing and understanding neural machine translation//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017) .Vancouver, Canada, 2017:1150-1159</h5>
<p>While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoderdecoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.
è¿‘å¹´æ¥ï¼Œç¥ç»æœºå™¨ç¿»è¯‘(NMT)å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ç”±äºç¥ç»ç½‘ç»œçš„è¿ç»­è¡¨ç¤ºå’Œéçº¿æ€§ç‰¹æ€§ï¼Œå¾ˆéš¾è§£é‡Šå…¶å†…åœ¨çš„å·¥ä½œåŸç†ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåœ¨åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨æ¡†æ¶ä¸­ï¼Œä½¿ç”¨å±‚ç›¸å…³ä¼ æ’­(LRP)æ¥è®¡ç®—æ¯ä¸ªä¸Šä¸‹æ–‡å•è¯å¯¹ä»»æ„éšè—çŠ¶æ€çš„è´¡çŒ®ã€‚ å®éªŒç»“æœè¡¨æ˜ï¼ŒLRPå¯è§†åŒ–æœ‰åŠ©äºè§£é‡ŠNMTçš„å†…éƒ¨å·¥ä½œæœºåˆ¶å’Œåˆ†æç¿»è¯‘é”™è¯¯ã€‚</p>

<h5 id="120-reed-s-akata-z-yan-x-et-algenerative-adversarial-text-to-image-synthesisproceedings-of-the-33rd-international-conference-on-machine-learning-icml-2016-new-york-usa-20161060-1069">[120] Reed S, Akata Z, Yan X, et al.Generative adversarial text to image synthesis//Proceedings of the 33rd International Conference on Machine Learning (ICML 2016) .New York, USA, 2016:1060-1069</h5>
<p>Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.
ä»æ–‡æœ¬ä¸­è‡ªåŠ¨åˆæˆé€¼çœŸçš„å›¾åƒå°†æ˜¯æœ‰è¶£å’Œæœ‰ç”¨çš„ï¼Œä½†ç›®å‰çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿä»è¿œæœªè¾¾åˆ°è¿™ä¸€ç›®æ ‡ã€‚ ç„¶è€Œï¼Œè¿‘å¹´æ¥å‘å±•äº†é€šç”¨çš„ã€åŠŸèƒ½å¼ºå¤§çš„é€’å½’ç¥ç»ç½‘ç»œç»“æ„æ¥å­¦ä¹ åŒºåˆ†æ–‡æœ¬ç‰¹å¾è¡¨ç¤ºã€‚ åŒæ—¶ï¼Œæ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—æ€§ç½‘ç»œï¼ˆGANSï¼‰å·²ç»å¼€å§‹ç”Ÿæˆç‰¹å®šç±»åˆ«çš„éå¸¸å¼•äººæ³¨ç›®çš„å›¾åƒï¼Œä¾‹å¦‚äººè„¸ã€ä¸“è¾‘å°é¢å’Œæˆ¿é—´å†…éƒ¨ã€‚ åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ·±å±‚ç»“æ„å’ŒGaNå…¬å¼ï¼Œä»¥æœ‰æ•ˆåœ°è¿æ¥æ–‡æœ¬å’Œå›¾åƒå»ºæ¨¡çš„è¿™äº›è¿›å±•ï¼Œå°†è§†è§‰æ¦‚å¿µä»å­—ç¬¦è½¬æ¢ä¸ºåƒç´ ã€‚ æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹ä»è¯¦ç»†çš„æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆé¸Ÿå’ŒèŠ±çš„å¯ä¿¡å›¾åƒçš„èƒ½åŠ›ã€‚</p>
:ET