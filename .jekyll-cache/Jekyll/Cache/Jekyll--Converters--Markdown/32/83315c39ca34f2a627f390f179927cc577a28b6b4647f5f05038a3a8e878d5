I"#<h1 id="2018-sockeye--a-toolkit-for-neural-machine-translation">2018 SOCKEYE- A Toolkit for Neural Machine Translation</h1>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720170019.jpg" alt="image-20200516152528492" /></p>

<h1 id="摘要">摘要</h1>

<p>​		我们描述了S OCKEYE，1一个用于神经机器翻译(NMT)的开源序列到序列工具包。 S OCKEYE是一个可用于培训和应用模型的生产准备框架，也是一个研究人员的实验平台。 该工具包用Python编写并构建在MXN ET上，为三种最突出的编解码器架构提供可扩展的训练和推理:注意递归神经网络，自注意转换器和全卷积网络。 S OCKEYE还支持广泛的优化器，规范化和正则化技术，以及当前NMT文献中的推理改进。 用户可以轻松运行标准训练食谱，探索不同的模型设置，并融入新的想法。 本文着重介绍了S Ockeye的特点，并以2017年机器翻译会议(WMT)上的两个语言弧（英-德和拉脱维亚语-英）为例，对其与其他NMT工具包进行了比较。 我们报告了所有三种架构的BLEU竞争力得分，包括Sockeye的变压器实现的总体最佳得分。 为了便于进一步的比较，我们发布我们实验中使用的所有系统输出和训练脚本。 S OCKEYE toolkit是在Apache2.0许可下发布的自由软件。</p>

<h1 id="1-导言">1 导言</h1>

<p>​		在过去的两年里，一场深度学习革命给机器翻译领域带来了迅速而戏剧性的变化。 对于用户来说，新的基于神经网络的模型始终提供比上一代基于短语的系统更好的质量翻译。 对于研究人员来说，神经机器翻译(NMT)提供了一个激动人心的新领域，训练管道简化，可以直接从数据中训练统一模型。 超越统计机器翻译(SMT)限制的承诺给社区注入了活力，导致最近的工作几乎完全集中在NMT上，并且似乎每隔几个月就会推进最新的技术。</p>

<p>​		尽管取得了成功，NMT也提出了一系列新的挑战。 虽然流行的编码器-解码器模型非常简单，但最近的文献和共享评估任务的结果表明，要在翻译质量和计算效率方面实现“生产就绪”的性能，需要大量的工程。 在SMT带来的趋势中，最强的NMT系统得益于微妙的架构调整，超参数调优和经验上有效的启发式。 与SMT不同的是，没有一个“事实上”的工具包吸引了社区的大部分注意力，从而包含了最近文献中所有最好的想法。 2相反，许多独立工具包3的存在为技术带来了多样性，但也使得在不同工具包中实现的架构和算法改进难以比较。
为了应对这些挑战，我们介绍了S OCKEYE，这是一个用Python编写并构建在Apache MXN ET 4[Chen ET al.，2015]基础上的神经序列到序列工具包。 据我们所知，S OCKEYE是唯一一个包含所有三种主要神经翻译架构实现的工具包:注意力递归神经网络[Schwenk，2012，Kalchbrenner和Blunsom，2013，Sutskever et al.，2014，Bahdanau et al.，2014，Luong et al.，2015]，自注意力转换器[Vaswani et al.，2017]和全卷积网络[Gehring et al.，2017]。 这些实现由一系列不断更新的特性支持，这些特性反映了最近文献中的最佳思想。 用户可以轻松地根据最新的研究训练模型，比较不同的架构，并通过添加自己的代码对其进行扩展。 S OCKEYE正在积极开发中，它遵循研究和生产软件的最佳实践，包括清晰的编码和文档指南，全面的自动测试以及对代码贡献的同行评审。</p>

<p>​		在下面的章节中，我们提供了S Ockeye的三种编码器-解码器架构的概述(§2)。 然后，我们将重点介绍关键模型，训练和推理特性(§3)和基准测试的OCKEYE，并对照两个公共数据集WMT 2017 English-Derman和Latvian-English（Bojar et al.，2017)(§4)上的许多其他开源NMT工具包。 根据BLEU评分[Papineni等人，2002年]，S Ockeye的递归模型与性能最好的工具包的模型具有竞争力，卷积模型在类中表现最好，而transformer模型优于所有工具包中的所有其他模型。 我们以总结和合作邀请作为论文的结尾(§5)。</p>

<h1 id="2-nmt的编解码器模型">2 NMT的编解码器模型</h1>

<p>​		自从神经序列到序列建模的早期，编码器-解码器模型就一直很有吸引力。 其核心思想是将一个可变长度的令牌输入序列编码成一个向量表示序列，然后将那些表示解码成一个输出令牌序列。 这种译码既依赖于潜在输入向量编码的信息，也依赖于它自身不断更新的内部状态，这就激发了这样的想法:模型应该能够捕捉单词层次以外的意义和相互作用。</p>

<p>​		在形式上， 给定源句子X=x1， 。。。，x n和目标句子Y=y1，。。。，Y m，NMT系统建模p（Y x）作为目标语言序列模型，调节目标单词Y t在目标历史y1:t-1和源句子x上的概率。每个x i和Y t是由源和目标词汇映射V src和V trg给出的整数ID，从训练数据令牌建立，并表示为单热向量x i∈{0，1}V src和Y t∈{0，1}V trg。 利用嵌入矩阵R E×V src和Q E×V trg m E T∈R，将它们嵌入到E维向量表示E S x i和E T y T中。 目标序列分解为p（Y x；？）=t=1 p（Y t Y 1:t−1，x；？）。 该模型由？参数化，由编码器和解码器部分组成，这两个部分根据模型架构的不同而不同。 p（y t y 1:t−1，x；？）通过softmax输出层在某些解码器表示的t:</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720170040.jpg" alt="image-20200516152708006" /></p>

<p>其中W o缩放到目标词汇表V trg的维度。</p>

<p>为了参数化编码器和解码器，递归神经网络（RNNs）通过更新内部状态并返回输出序列，构成编码可变长度序列的自然方式，其中最后的输出可能总结编码序列[Sutskever et al.，2014]。 然而，对于较长的序列，固定大小的向量可能无法存储关于序列的有效信息，而注意机制通过动态调整给定状态的编码序列的表示，有助于减轻这一负担[Bahdanau等人，2014年]。 虽然在过去的几年中，具有注意力的递归体系结构已经成为NMT的事实上的标准，但它们表现出核心的限制，即在所有先前的表示都已知之前，不能计算序列中位于位置i的令牌的表示。 随着GPU等高度并行设备的兴起以及这些神经体系结构对大量训练数据的需求，编码器/解码器计算的并行化已经变得至关重要。</p>

<p>最近，已经提出了两个额外的体系结构来编码序列，它们除了改进并行化之外，还改进了NMT中的技术状态:自注意转换器[Vaswani et al.，2017]和全卷积模型[Gehring et al.，2017]。 我们给出了在S OCKEYE中实现的所有三种体系结构的简要描述，但请读者参阅参考文献以了解更多细节。</p>

<p>2.1带注意的堆叠RNN</p>

<p>我们首先定义了在S OCKEYE中实现的递归架构，之后是Bahdanau等人。 [2014]和Luong等人。 [2015]。</p>

<p>编码器第一个编码器层由一个双向RNN和一堆单向RNN组成。 具体地说，第一层产生隐藏状态h 1 0的前向序列。 。。h0通过RNN，使得:</p>

<p>其中h0 0=0∈R d，f enc是某种非线性函数，如门控循环单元(GRU)或长短时记忆单元(LSTM)。 反向RNN从右到左处理源语句:hi0=fenc(Esxi，h0)，两个方向的隐藏状态被级联:i+1 h0=[h0；h0]，其中括号表示向量级联。 因此，隐藏状态h0可以结合来自左侧和右侧两个令牌的信息。 双向RNN之后是单向层堆叠。 对于编码器层索引l&gt;1，位置i到f enc l处的输入是较低层的输出:h i l−1。 随着网络的深入，学习变得越来越困难[Hochreiter等人，2001年，Pascanu等人，2012年]，形式为h i l=h i l−1+f enc（h i l−1，h i−1 l）的剩余连接变得至关重要[He等人，2016年]。</p>

<p>解码器向量%s:</p>

<p>解码器由一个RNN组成，通过一个状态一次预测一个目标字</p>

<p>其中，dec是多层RNN，St-1是先前的状态向量，’St-1是源相关的</p>

<p>f注意向量。 提供注意向量作为第一解码器层的输入也称为输入馈电[Luong等人，2015]。 初始解码器隐藏状态是最后一个编码器隐藏状态的非线性变换:S0=tanh（W init h n+b init）。 注意向量t将解码器状态与上下文向量c t组合在一起:</p>

<p>由于对前一时间步长的经常性依赖性，RNN隐状态的计算不能随时间并行化。 计算图中的依赖关系导致从左下角到右上角的三角形计算，因为每个RNN单元都需要等待同一层的前一个时间步长和解码器侧的前一层的前一个时间步长（另见图1a)。由于输入馈电机制，计算甚至更加串行化，因为只有在第一个时间步长完成后才能开始第二个时间步长。 因此，每次计算一列。</p>

<p>…</p>
:ET