I"ül<h1 id="2019-training-neural-machine-translation-to-apply-terminology-constraints">2019 Training Neural Machine Translation To Apply Terminology Constraints</h1>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720164847.jpg" alt="image-20200526095616272" /></p>

<h1 id="abstract">Abstract</h1>

<p>â€‹		æœ¬æ–‡æå‡ºäº†ä¸€ç§<strong>åœ¨è¿è¡Œæ—¶å°†è‡ªå®šä¹‰æœ¯è¯­æ³¨å…¥ç¥ç»æœºå™¨ç¿»è¯‘çš„æ–°é¢–æ–¹æ³•</strong>ã€‚This paper proposes a novel method to inject custom terminology into neural machine translation at run time. ä»¥å‰çš„å·¥ä½œä¸»è¦æ˜¯<strong>å¯¹è§£ç ç®—æ³•æå‡ºä¿®æ”¹ï¼Œä»¥çº¦æŸè¾“å‡ºï¼Œä½¿å…¶åŒ…å«è¿è¡Œæ—¶æä¾›çš„ç›®æ ‡é¡¹</strong>ã€‚Previous works have mainly proposed modiï¬cations to the decoding algorithm in order to constrain the output to include run-time-provided target terms. è¿™äº›å—é™è§£ç æ–¹æ³•è™½ç„¶å¾ˆæœ‰æ•ˆï¼Œä½†åœ¨æ¨ç†æ­¥éª¤ä¸­å¢åŠ äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œè€Œä¸”å¦‚æœ¬æ–‡æ‰€ç¤ºï¼Œåœ¨å®é™…æ¡ä»¶ä¸‹æµ‹è¯•æ—¶å¯èƒ½ä¼šå˜å¾—è„†å¼±ã€‚While being effective, these constrained decoding methods add, however, signiï¬cant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions.  åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒä¸€ä¸ªç¥ç»MTç³»ç»Ÿæ¥å­¦ä¹ å½“æä¾›è¾“å…¥æ—¶å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æœ¯è¯­æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æ¯”ä¸€ç§æœ€å…ˆè¿›çš„çº¦æŸè¯‘ç å®ç°æ›´æœ‰æ•ˆï¼Œè€Œä¸”ä¸æ— çº¦æŸè¯‘ç åŒæ ·å¿«ã€‚In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.</p>

<h1 id="1-introduction">1 Introduction</h1>

<p>â€‹		å°½ç®¡å¦‚ä»Šç¥ç»æœºå™¨ç¿»è¯‘(NMT)è¾¾åˆ°äº†å¾ˆé«˜çš„è´¨é‡ï¼Œä½†å®ƒçš„è¾“å‡ºä»ç„¶ä¸èƒ½æ»¡è¶³ç¿»è¯‘è¡Œä¸šæ—¥å¸¸å¤„ç†çš„è®¸å¤šç‰¹å®šé¢†åŸŸã€‚ <strong>è™½ç„¶NMTå·²ç»è¯æ˜ï¼Œå¯ä»¥ä»åŸŸå†…å¹¶è¡Œæˆ–å•è¯­æ•°æ®çš„å¯ç”¨æ€§ä¸­å—ç›Šäºå­¦ä¹ é¢†åŸŸè§„èŒƒæœ¯è¯­ï¼ˆFarajian et al.ï¼Œ2018ï¼‰ï¼Œä½†å®ƒå¹¶ä¸æ˜¯ä¸€ä¸ªæ™®éé€‚ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºä¸€ä¸ªé¢†åŸŸé€šå¸¸å¯èƒ½å¤ªçª„ï¼Œå¹¶ä¸”ç¼ºä¹æ•°æ®ï¼Œå› æ­¤è¿™ç§è‡ªä¸¾æŠ€æœ¯æ— æ³•å‘æŒ¥ä½œç”¨</strong>ã€‚While NMT has shown to beneï¬t from the availability of in-domain parallel or monolingual data to learn domain speciï¬c terms (Farajian et al., 2018), it is not a universally applicable solution as often a domain may be too narrow and lacking in data for such bootstrapping techniques to work. å› æ­¤ï¼Œå¤§å¤šæ•°å¤šè¯­è¨€å†…å®¹æä¾›å•†ä¸ºå…¶æ‰€æœ‰åŸŸç»´æŠ¤ç”±è¯­è¨€ä¸“å®¶åˆ›å»ºçš„æœ¯è¯­ã€‚For this reason, most multilingual content providers maintain terminologies for all their domains which are created by language specialists. ä¾‹å¦‚ï¼Œä¸ºäº†è¡¨ç¤ºè¾“å…¥çš„ã€Šå¤§ç™½é²¨ã€‹æ˜¯ä¸€éƒ¨ææ€–ç”µå½±ï¼Œä¼šæœ‰ä¸€ä¸ªæ¡ç›®ï¼Œå¦‚ã€Šå¤§ç™½é²¨(en)Lo Squalo(it)ã€‹ï¼Œåº”è¯¥ç¿»è¯‘ä¸ºLo Squalo e`unfilm Paurosoã€‚ è™½ç„¶ç¿»è¯‘å­˜å‚¨å™¨å¯ä»¥è¢«çœ‹ä½œæ˜¯NMTé¢†åŸŸé€‚åº”çš„ç°æˆçš„è®­ç»ƒæ•°æ®ï¼Œä½†æœ¯è¯­æ•°æ®åº“ï¼ˆçŸ­æœŸåŸºç¡€ï¼‰æ›´éš¾å¤„ç†ï¼Œè€Œä¸”åœ¨æå‡ºåœ¨è¿è¡Œæ—¶å°†é¢†åŸŸæœ¯è¯­é›†æˆåˆ°NMTçš„æ–¹æ³•æ–¹é¢å·²ç»æœ‰äº†é‡è¦çš„å·¥ä½œã€‚</p>

<p>â€‹		<strong>çº¦æŸè¯‘ç æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„ä¸»è¦æ–¹æ³•ã€‚ ç®€è€Œè¨€ä¹‹ï¼Œå®ƒä½¿ç”¨æºç«¯ä¸è¾“å…¥ç«¯åŒ¹é…çš„æœ¯è¯­æ¡ç›®çš„ç›®æ ‡ç«¯ä½œä¸ºè§£ç æ—¶é—´çº¦æŸã€‚ Chatterjeeç­‰äººæå‡ºäº†çº¦æŸè¯‘ç å’Œå„ç§æ”¹è¿›</strong>ã€‚ Constrained decoding is the main approach to this problem. In short, it uses the target side of terminology entries whose source side match the input as decoding-time constraints.ï¼ˆ2017ï¼‰ï¼ŒHaslerç­‰äººã€‚ ï¼ˆ2018ï¼‰ï¼ŒHokampå’ŒLiuï¼ˆ2017ï¼‰ç­‰ã€‚ Hokampå’ŒLiuï¼ˆ2017ï¼‰æœ€è¿‘ä»‹ç»äº†ç½‘æ ¼<strong>æ³¢æŸæœç´¢(GBS)ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯¹æ¯ä¸ªæä¾›çš„è¯æ³•çº¦æŸä½¿ç”¨å•ç‹¬çš„æ³¢æŸ</strong>ã€‚ the grid beam search (GBS) algorithm which uses a separate beam for each supplied lexical constraint.ç„¶è€Œï¼Œè¯¥è§£å†³æ–¹æ¡ˆåœ¨çº¦æŸçš„æ•°é‡ä¸ŠæŒ‡æ•°åœ°å¢åŠ è§£ç è¿‡ç¨‹çš„è¿è¡Œæ—¶é—´å¤æ‚åº¦ã€‚ Postå’ŒVilar(2018)æœ€è¿‘å»ºè®®ä½¿ç”¨<strong>åŠ¨æ€æ³¢æŸåˆ†é…(DBA)æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†è®¡ç®—å¼€é”€å‡å°‘åˆ°æ’å®šå› å­ï¼Œä¸çº¦æŸçš„æ•°é‡æ— å…³</strong>ã€‚dynamic beam allocation (DBA) technique that reduces the computational overhead to a constant factor, independent from the number of constraints. åœ¨å®è·µä¸­ï¼ŒPostå’ŒVilar(2018)ä¸­æŠ¥é“çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DBAçš„çº¦æŸè§£ç æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å½“ä½¿ç”¨æ³¢æŸå¤§å°ä¸º5æ—¶ï¼Œä»ç„¶ä¼šå¯¼è‡´ç¿»è¯‘æ—¶é—´å¢åŠ 3å€ã€‚</p>

<p>â€‹		<strong>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŠŠçº¦æŸè¯‘ç é—®é¢˜çœ‹ä½œæ˜¯åœ¨è®­ç»ƒæ—¶é—´å­¦ä¹ æœ¯è¯­çš„å¤åˆ¶è¡Œä¸ºçš„é—®é¢˜ã€‚ é€šè¿‡ä¿®æ”¹ç¥ç»MTçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå®Œå…¨æ¶ˆé™¤äº†æ¨ç†æ—¶çš„ä»»ä½•è®¡ç®—å¼€é”€</strong>ã€‚ In this paper we address the problem of constrained decoding as that of learning a copy behaviour of terminology at training time. By modifying the training procedure of neural MT we are completely eliminating any computational overhead at inference time. <strong>å…·ä½“åœ°è¯´ï¼ŒNMTæ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨ä½œä¸ºæºå¥å­é™„åŠ è¾“å…¥çš„æœ¯è¯­æ¡ç›®ã€‚ æœ¯è¯­ç¿»è¯‘ä½œä¸ºå†…è”æ³¨é‡Šæ’å…¥ï¼Œé™„åŠ çš„è¾“å…¥æµï¼ˆæ‰€è°“çš„æºå› å­ï¼‰ç”¨äºé€šçŸ¥è¿è¡Œæ–‡æœ¬å’Œç›®æ ‡æœ¯è¯­ä¹‹é—´çš„åˆ‡æ¢ã€‚ æˆ‘ä»¬ç»™å‡ºäº†ä»ä¸¤ä¸ªæœ¯è¯­è¯å…¸ä¸­æŠ½å–çš„æœ¯è¯­è¿›è¡Œè‹±å¾·ç¿»è¯‘çš„å®éªŒ</strong>ã€‚Speciï¬cally, the NMT model is trained to learn how to use terminology entries when they are provided as additional input to the source sentence. Term translations are inserted as inline annotations and additional input streams (so called source factors) are used to signal the switch between running text and target terms. ç”±äºæˆ‘ä»¬ä¸å‡è®¾æœ¯è¯­åœ¨è®­ç»ƒæ—¶é—´æ˜¯å¯ç”¨çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„æ‰€æœ‰æµ‹è¯•éƒ½æ˜¯åœ¨é›¶é•œå¤´è®¾ç½®ä¸‹æ‰§è¡Œçš„ï¼Œå³ä½¿ç”¨æœªè§çš„æœ¯è¯­æœ¯è¯­ã€‚ æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸Postå’ŒVilar(2018)æå‡ºçš„å¸¦DBAçš„çº¦æŸè§£ç çš„æœ‰æ•ˆå®ç°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>

<p>â€‹		è€Œæˆ‘ä»¬çš„ç›®æ ‡ä¸Guç­‰äººçš„ç›®æ ‡ç›¸ä¼¼ã€‚ ï¼ˆ2017ï¼‰ï¼ˆæ•™NMTä½¿ç”¨ç¿»è¯‘è®°å¿†ï¼‰å’ŒPhamç­‰äººçš„ã€‚ ï¼ˆ2018ï¼‰ï¼ˆæ¢ç´¢å®æ–½å¤åˆ¶è¡Œä¸ºçš„ç½‘ç»œæ¶æ„ï¼‰ï¼Œ<strong>æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸æ ‡å‡†transformer NMTæ¨¡å‹ï¼ˆVaswaniç­‰äººï¼Œ2017ï¼‰ä¸€èµ·å·¥ä½œï¼Œè¯¥æ¨¡å‹æä¾›äº†åŒ…å«è¿è¡Œæ–‡æœ¬å’Œå†…è”æ³¨é‡Šçš„æ··åˆè¾“å…¥ã€‚ è¿™å°†æœ¯è¯­åŠŸèƒ½ä¸NMTä½“ç³»ç»“æ„è§£è€¦ï¼Œå½“æœ€æ–°çš„ä½“ç³»ç»“æ„ä¸æ–­å˜åŒ–æ—¶ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦</strong>ã€‚the method we propose works with a standard transformer NMT model (Vaswani et al., 2017) which is fed a hybrid input containing running text and inline annotations. This decouples the terminology functionality from the NMT architecture, which is particularly important as the state-of-the-art architectures are continuously changing.</p>

<h1 id="2-model">2 Model</h1>

<p>â€‹		<strong>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é›†æˆçš„æ–¹æ³•ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œå½“è¾“å…¥ä¸­æä¾›äº†ç›®æ ‡æœ¯è¯­æ—¶ï¼ŒMTæ¨¡å‹åœ¨è®­ç»ƒæ—¶å­¦ä¹ å¦‚ä½•ä½¿ç”¨æœ¯è¯­ã€‚ ç‰¹åˆ«æ˜¯ï¼Œæ¨¡å‹åº”è¯¥å­¦ä¼šåå‘ç¿»è¯‘ä»¥åŒ…å«æ‰€æä¾›çš„æœ¯è¯­ï¼Œå³ä½¿å®ƒä»¬åœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰è¢«è§‚å¯Ÿåˆ°</strong>ã€‚ We propose an integrated approach in which the MT model learns, at training time, how to use terminology when target terms are provided in input. In particular, the model should learn to bias the translation to contain the provided terms, even if they were not observed in the training data.<strong>æˆ‘ä»¬å¯¹ä¼ ç»Ÿçš„MTè¾“å…¥è¿›è¡Œäº†æ‰©å……ï¼Œä»¥åŒ…å«ä¸€ä¸ªæºå¥å­ä»¥åŠä¸€ä¸ªé’ˆå¯¹è¯¥å¥å­è§¦å‘çš„æœ¯è¯­æ¡ç›®åˆ—è¡¨ï¼Œç‰¹åˆ«æ˜¯é‚£äº›æºè¾¹ä¸å¥å­åŒ¹é…çš„æœ¯è¯­æ¡ç›®</strong>ã€‚ We augment the traditional MT input to contain a source sentence as well as a list of terminology entries that are triggered for that sentences, speciï¬cally those whose source sides match the sentence.<strong>è™½ç„¶äººä»¬å·²ç»æ¢ç´¢äº†è®¸å¤šä¸åŒçš„æ–¹æ³•æ¥å¢åŠ ç¿»è¯‘è¾“å…¥çš„é¢å¤–ä¿¡æ¯ï¼Œä½†æ˜¯æˆ‘ä»¬é€‰æ‹©åœ¨æºå¥å­ä¸­é›†æˆæœ¯è¯­ä¿¡æ¯ä½œä¸ºå†…è”æ³¨é‡Šï¼Œæˆ–è€…å°†ç›®æ ‡æœ¯è¯­æ·»åŠ åˆ°æºå¥å­ä¸­ï¼Œæˆ–è€…ç›´æ¥ç”¨ç›®æ ‡æœ¯è¯­æ›¿æ¢åŸæœ¯è¯­</strong>ã€‚ While many different ways have been explored to augment MT input with additional information, we opt here for integrating terminology information as inline annotations in the source sentence, by either appending the target term to its source version, or by directly replacing the original term with the target one.<strong>æˆ‘ä»¬åœ¨æºå¥ä¸­æ·»åŠ ä¸€ä¸ªé¢å¤–çš„å¹¶è¡Œæµæ¥è¡¨ç¤ºè¿™ç§â€œè¯­ç è½¬æ¢â€ã€‚ å½“é™„åŠ ç¿»è¯‘æ—¶ï¼Œæ­¤æµæœ‰ä¸‰ä¸ªå¯èƒ½çš„å€¼:0è¡¨ç¤ºæºå•è¯ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œ1è¡¨ç¤ºæºæœ¯è¯­ï¼Œ2è¡¨ç¤ºç›®æ ‡æœ¯è¯­ã€‚ ä¸¤ä¸ªè¢«æµ‹è¯•çš„å˜ä½“ï¼Œä¸€ä¸ªä¿ç•™äº†æœ¯è¯­çš„æºç«¯ï¼Œå¦ä¸€ä¸ªä¸¢å¼ƒäº†æºç«¯ï¼Œç”¨è¡¨1ä¸­çš„ä¸€ä¸ªä¾‹å­è¯´æ˜äº†è¿™ä¸¤ä¸ªå˜ä½“</strong>ã€‚We add an additional parallel stream to signal this â€œcode-switchingâ€ in the source sentence. When the translation is appended this stream has three possible values: 0 for source words (default), 1 for source terms, and 2 for target terms. The two tested variants, one in which the source side of the terminology is retained and one in which it is discarded, are illustrated with an example in Table 1.</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720165017.jpg" alt="image-20200526102926158" /></p>

<p>è¡¨1:ç”¨äºç”Ÿæˆsourcetargetè®­ç»ƒæ•°æ®çš„ä¸¤ç§æ›¿ä»£æ–¹å¼ï¼ŒåŒ…æ‹¬æºä¸­çš„ç›®æ ‡æœ¯è¯­ä»¥åŠæŒ‡ç¤ºæºè¯ï¼ˆ0ï¼‰ï¼Œæºæœ¯è¯­ï¼ˆ1ï¼‰å’Œç›®æ ‡æœ¯è¯­ï¼ˆ2ï¼‰çš„å› ç´ ã€‚Table 1: The two alternative ways used to generate sourcetarget training data, including target terms in the source and factors indicating source words (0), source terms (1), and target terms (2).</p>

<h2 id="21-training-data-creationè®­ç»ƒæ•°æ®åˆ›å»º">2.1 Training data creationè®­ç»ƒæ•°æ®åˆ›å»º</h2>

<p>â€‹		ç”±äºæˆ‘ä»¬ä¸ä¿®æ”¹åŸå§‹çš„sequence-tosequence NMTä½“ç³»ç»“æ„ï¼Œç½‘ç»œå¯ä»¥ä»è®­ç»ƒæ•°æ®çš„æ‰©å……ä¸­å­¦ä¹ æœ¯è¯­çš„ä½¿ç”¨ã€‚ <strong>æˆ‘ä»¬å‡è®¾å½“ä¸€ä¸ªæœ¯è¯­æ¡ç›®ï¼ˆt sï¼Œt tï¼‰åœ¨æºä¸­è¢«æ³¨é‡Šæ—¶ï¼Œç›®æ ‡æ–¹t tå‡ºç°åœ¨å‚è€ƒæ–‡çŒ®ä¸­æ—¶ï¼Œæ¨¡å‹å°†åœ¨è®­ç»ƒæ—¶å­¦ä¼šä½¿ç”¨æ‰€æä¾›çš„æœ¯è¯­</strong>ã€‚We hypothesize that the model will learn to use the provided terminology at training time if it holds true that when a terminology entry (t s , t t ) is annotated in the source, the target side t t is present in the reference. <strong>å› æ­¤ï¼Œæˆ‘ä»¬ä»…å¯¹æ»¡è¶³æ­¤æ ‡å‡†çš„æœ¯è¯­å¯¹è¿›è¡Œæ ‡æ³¨</strong>ã€‚ For this reason we annotate only terminology pairs that ï¬t this criterion.<strong>å®éªŒä¸­ä½¿ç”¨çš„æœ¯è¯­åº“ç›¸å½“å¤§ï¼Œå¹¶ä¸”å¯¹æ‰€æœ‰åŒ¹é…é¡¹è¿›è¡Œæ³¨é‡Šå¯¼è‡´äº†å¤§å¤šæ•°åŒ…å«æœ¯è¯­æ³¨é‡Šçš„å¥å­ã€‚ ç”±äºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åœ¨åŸºçº¿ï¼Œæ— çº¦æŸæ¡ä»¶ä¸‹è¡¨ç°å¾—åŒæ ·å¥½ï¼Œæ‰€ä»¥æˆ‘ä»¬é€šè¿‡éšæœºå¿½ç•¥ä¸€äº›åŒ¹é…æ¥é™åˆ¶æ³¨é‡Šçš„æ•°é‡</strong>ã€‚The term bases used in the experiments are quite large and annotating all matches leads to most of the sentences containing term annotations. Since we want to model to perform equally well in a baseline, constraint-free condition, we limit the number of annotations by randomly ignoring some of the matches.</p>

<p>â€‹		<strong>ä¸€ä¸ªå¥å­så¯èƒ½åŒ…å«æ¥è‡ªä¸€ä¸ªæœ¯è¯­åº“çš„å¤šä¸ªåŒ¹é…é¡¹ï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨æºæœ¯è¯­é‡å çš„æƒ…å†µä¸‹ä¿ç•™æœ€é•¿çš„åŒ¹é…é¡¹</strong>ã€‚ A sentence s may contain multiple matches from a term base, but we keep the longest match in the case of overlapping source terms.<strong>æ­¤å¤–ï¼Œå½“æ£€æŸ¥å¥å­ä¸­çš„æœ¯è¯­åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬åº”ç”¨è¿‘ä¼¼åŒ¹é…æ¥å…è®¸æœ¯è¯­ä¸­çš„ä¸€äº›å½¢æ€å˜åŒ–</strong>ã€‚Moreover, when checking for matches of a term inside a sentence, we apply approximate matching to allow for some morphological variations in the term <strong>åœ¨æˆ‘ä»¬å½“å‰çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å­—ç¬¦åºåˆ—åŒ¹é…ï¼Œä¾‹å¦‚ï¼Œå…è®¸å°†åŸºæœ¬å•è¯å½¢å¼è§†ä¸ºåŒ¹é…ï¼Œå³ä½¿å®ƒä»¬åœ¨å½±å“ä¸­æˆ–ä½œä¸ºåŒ–åˆç‰©çš„ä¸€éƒ¨åˆ†</strong>ã€‚In our current implementation, we use a simple character sequence match, allowing for example for base word forms to be considered matches even if they are inï¬‚ected or as part of compounds.</p>

<h1 id="3-experiments">3 Experiments</h1>

<h2 id="31-evaluation-setting">3.1 Evaluation setting</h2>

<p>â€‹		<strong>å¹¶è¡Œæ•°æ®å’ŒNMTæ¶æ„</strong>   æˆ‘ä»¬åœ¨WMT 2018è‹±å¾·æ–°é—»ç¿»è¯‘ä»»åŠ¡1ä¸Šæµ‹è¯•æˆ‘ä»¬çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨Europarlå’Œæ–°é—»è¯„è®ºæ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œ<strong>æ€»è®¡220ä¸‡å¥ã€‚ åŸºçº¿æŒ‰åŸæ ·ä½¿ç”¨æ­¤è®­ç»ƒæ•°æ®</strong>ã€‚We test our approach on the WMT 2018 English-German news translation tasks 1 , by training models on Europarl and news commentary data, for a total 2.2 million sentences. The baselines use this train data as is. <strong>å¯¹äºå…¶ä»–æ¡ä»¶ï¼ŒåŒ…å«æœ¯è¯­æ³¨é‡Šçš„å¥å­è¢«æ·»åŠ ï¼Œå¤§çº¦ç›¸å½“äºåŸå§‹æ•°æ®çš„10%</strong>ã€‚For the other conditions sentences containing term annotations are added amounting to approximately 10% of the original data. <strong>æˆ‘ä»¬é™åˆ¶äº†æ·»åŠ çš„æ•°æ®é‡ï¼ˆé€šè¿‡éšæœºå¿½ç•¥ä¸€äº›åŒ¹é…çš„æœ¯è¯­ï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åœ¨æ²¡æœ‰æä¾›æœ¯è¯­ä½œä¸ºè¾“å…¥æ—¶åŒæ ·å·¥ä½œè‰¯å¥½ã€‚ æ³¨æ„ï¼Œè¿™äº›å¥å­æ¥è‡ªåŸå§‹æ•°æ®æ± ï¼Œå› æ­¤æ²¡æœ‰å¼•å…¥å®é™…çš„æ–°æ•°æ®</strong>ã€‚We limit the amount of data added (by randomly ignoring some of the matched terms) as we want the model to work equally well when there are no terms provided as input. Note that these sentences are from the original data pool and therefore no actual new data is introduced.</p>

<p>â€‹		æˆ‘ä»¬ä½¿ç”¨<strong>Moses</strong>ï¼ˆKoehn et al.ï¼Œ2007ï¼‰å¯¹è¯­æ–™åº“è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å¯¹<strong>32k</strong>ä¸ªæ ‡è®°è¿›è¡Œ<strong>è”åˆæºå’Œç›®æ ‡BPEç¼–ç </strong>ï¼ˆSennrich et al.ï¼Œ2016ï¼‰ã€‚ <strong>æˆ‘ä»¬ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­æè¿°çš„æºå› å­æµï¼Œè¿™äº›æºå› å­æµä»¥ç®€å•çš„æ–¹å¼ä»å­—æµå¹¿æ’­åˆ°BPEæµ</strong>ã€‚We use the source factor streams described in the previous section which are broadcast from word streams to BPE streams in a trivial way. <strong>æˆ‘ä»¬å°†è¿™ä¸ªé™„åŠ æµçš„ä¸‰ä¸ªå€¼åµŒå…¥åˆ°å¤§å°ä¸º16çš„å‘é‡ä¸­ï¼Œå¹¶å°†å®ƒä»¬ä¸²è”åˆ°ç›¸åº”çš„å­å­—åµŒå…¥ä¸­</strong>ã€‚We embed the three values of this additional stream into vectors of size 16 and concatenate them to the corresponding sub-word embeddings. æˆ‘ä»¬ä½¿ç”¨å…·æœ‰<strong>ä¸¤ä¸ªç¼–ç å±‚å’Œä¸¤ä¸ªè§£ç å±‚çš„å˜å‹å™¨ç½‘ç»œ</strong>ï¼ˆVaswaniç­‰äººï¼Œ2017å¹´ï¼‰ï¼Œå…±äº«æºå’Œç›®æ ‡åµŒå…¥ï¼Œå¹¶ä½¿ç”¨<strong>Sockeyeå·¥å…·åŒ…</strong>ï¼ˆHieberç­‰äººï¼Œ2018å¹´ï¼‰è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼ˆå‚è§é™„å½•ä¸­çš„å®Œæ•´è®­ç»ƒé…ç½®ï¼‰ã€‚ WMT newstest 2013å¼€å‘é›†ç”¨äºè®¡ç®—åœæ­¢æ ‡å‡†ï¼Œ<strong>æ‰€æœ‰æ¨¡å‹éƒ½ç»è¿‡è‡³å°‘50ä¸ªï¼Œæœ€å¤š100ä¸ªå†å…ƒçš„è®­ç»ƒ</strong>ã€‚ åœ¨ç›¸åŒçš„æ¡ä»¶ä¸‹ï¼Œ<strong>ä½¿ç”¨5çš„æ³¢æŸå¤§å°</strong>ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬æå‡ºçš„ä¸¤ç§æ–¹æ³•ï¼Œ<strong>å³é€é™„åŠ çš„è®­ç»ƒå’Œé€æ›¿æ¢çš„è®­ç»ƒtrain-by-appending and train-by-replace</strong>ï¼Œ<strong>ä¸Sockeyeä¸­å¯ç”¨çš„Postå’ŒVilar(2018)çš„çº¦æŸè¯‘ç ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒ</strong>ã€‚</p>

<p>â€‹		<strong>æœ¯è¯­æ•°æ®åº“</strong>      æˆ‘ä»¬æå–äº†ä¸¤ä¸ªå¯å…¬å¼€ä½¿ç”¨çš„æœ¯è¯­åº“ï¼ŒWiktionaryå’ŒIATEçš„è‹±è¯­-å¾·è¯­éƒ¨åˆ†ã€‚ 2ä¸ºäº†é¿å…è™šå‡åŒ¹é…ï¼Œæˆ‘ä»¬ç­›é€‰å‡ºå‡ºç°åœ¨å‰500ä¸ªæœ€å¸¸è§è‹±è¯­å•è¯ä¸­çš„æ¡ç›®ä»¥åŠå•ä¸ªå­—ç¬¦æ¡ç›®ã€‚ é€šè¿‡ç¡®ä¿åœ¨æºç«¯æ²¡æœ‰é‡å ï¼Œæˆ‘ä»¬å°†æœ¯è¯­åŸºåˆ†æˆè®­ç»ƒå’Œæµ‹è¯•åˆ—è¡¨ã€‚We extracted the English-German portions of two publicly available term bases, Wiktionary and IATE. 2 In order to avoid spurious matches, we ï¬ltered out entries occurring in the top 500 most frequent English words as well as single character entries. We split the term bases into train and test lists by making sure there is no overlap on the source side.</p>

<h2 id="32ç»“æœ">3.2ç»“æœ</h2>

<p>â€‹		æˆ‘ä»¬åˆ†åˆ«å¯¹WMT newstest 2013/2017 aså¼€å‘(dev)å’Œæµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä½¿ç”¨Wiktionaryå’ŒIATEçš„æµ‹è¯•éƒ¨åˆ†å¯¹æµ‹è¯•é›†è¿›è¡Œæ³¨é‡Šã€‚ 3æˆ‘ä»¬é€‰æ‹©å‚è€ƒæ–‡çŒ®ä¸­ä½¿ç”¨è¯¥æœ¯è¯­çš„å¥å­ï¼Œå› æ­¤å¤åˆ¶è¡Œä¸ºæ˜¯æ­£ç¡®çš„ã€‚ ç”¨ç»´åŸºè¯åº“æå–çš„<strong>æµ‹è¯•é›†åŒ…å«727ä¸ªå¥å­å’Œ884ä¸ªè¯æ¡</strong>ï¼Œç”¨ç»´åŸºè¯åº“æå–çš„æµ‹è¯•é›†åŒ…å«<strong>414ä¸ªå¥å­å’Œ452ä¸ªè¯æ¡</strong>ã€‚</p>

<p>â€‹		è¡¨2æ˜¾ç¤ºäº†ç»“æœã€‚ æˆ‘ä»¬<strong>æŠ¥å‘Šè§£ç é€Ÿåº¦ï¼ŒBLEUåˆ†æ•°ï¼Œä»¥åŠæœ¯è¯­ä½¿ç”¨ç‡</strong>ï¼Œè®¡ç®—ä¸ºæœ¯è¯­ç¿»è¯‘åœ¨è¾“å‡ºä¸­äº§ç”Ÿçš„æ¬¡æ•°å æœ¯è¯­æ³¨é‡Šæ€»æ•°çš„ç™¾åˆ†æ¯”ã€‚</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720165034.jpg" alt="image-20200526104644947" /></p>

<p>â€‹		è¡¨2:æä¾›äº†æ­£ç¡®çš„æœ¯è¯­æ¡ç›®çš„ç³»ç»Ÿçš„æœ¯è¯­ä½¿ç”¨ç‡ç™¾åˆ†æ¯”å’ŒBLEUåˆ†æ•°ï¼Œè¿™äº›ç³»ç»Ÿä¸æºå’Œç›®æ ‡å®Œå…¨åŒ¹é…ã€‚ æˆ‘ä»¬è¿˜æä¾›äº†P99å»¶è¿Ÿæ•°ï¼ˆå³99%çš„ç¿»è¯‘æ˜¯åœ¨ç»™å®šçš„ç§’æ•°å†…å®Œæˆçš„ï¼‰ã€‚ å¹¶ä¸”åœ¨på€¼å°äº0.05æ—¶è¡¨ç¤ºæ¯”åŸºçº¿ç³»ç»Ÿæ˜æ˜¾æ›´å¥½å’Œæ›´å·®çš„ç³»ç»Ÿã€‚</p>

<p>â€‹		<strong>æœ¯è¯­ä½¿ç”¨ç‡å’Œè§£ç é€Ÿåº¦</strong>    æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè§‚å¯Ÿç»“æœæ˜¯ï¼Œ<u>åŸºçº¿æ¨¡å‹å·²ç»ä»¥76%çš„é«˜æ¯”ç‡ä½¿ç”¨æœ¯è¯­ç¿»è¯‘ã€‚ æŒ‰è¿½åŠ è®­ç»ƒè®¾ç½®è¾¾åˆ°çº¦90%çš„æœ¯è¯­ä½¿ç”¨ç‡ï¼Œè€ŒæŒ‰æ›¿æ¢è®­ç»ƒè¾¾åˆ°æ›´é«˜çš„ä½¿ç”¨ç‡ï¼ˆ93%ï¼Œ94%ï¼‰ï¼Œè¡¨æ˜å®Œå…¨æ¶ˆé™¤æºæœ¯è¯­ä¼šæ›´åŠ å¼ºçƒˆåœ°å¼ºåˆ¶æ‰§è¡Œå¤åˆ¶è¡Œä¸º</u>ã€‚ <strong>æ‰€æœ‰è¿™äº›éƒ½ä¼˜äºçº¦æŸè¯‘ç ï¼Œçº¦æŸè¯‘ç åœ¨Wiktionaryä¸Šè¾¾åˆ°99%ï¼Œè€Œåœ¨IATEä¸Šä»…ä¸º82%</strong>ã€‚ 4</p>

<p>â€‹		<strong>ç¬¬äºŒï¼Œæˆ‘ä»¬çš„ä¸¤ä¸ªè®¾ç½®çš„è§£ç é€Ÿåº¦éƒ½ä¸åŸºçº¿çš„è§£ç é€Ÿåº¦ç›¸å½“ï¼Œå› æ­¤æ¯”çº¦æŸè§£ç (CD)çš„å¹³ç§»é€Ÿåº¦å¿«ä¸‰å€</strong>ã€‚ è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„åŒºåˆ«ï¼Œå› ä¸ºè§£ç æ—¶é—´å¢åŠ ä¸‰å€ä¼šé˜»ç¢æœ¯è¯­åœ¨latencycriticalåº”ç”¨ç¨‹åºä¸­çš„ä½¿ç”¨ã€‚ æ³¨æ„ï¼Œè§£ç æ—¶é—´æ˜¯é€šè¿‡åœ¨å•ä¸ªGPU P3 AWSå®ä¾‹ä¸Šè¿è¡Œæ‰¹å¤„ç†å¤§å°ä¸º1çš„å®éªŒæ¥æµ‹é‡çš„ã€‚ 5</p>

<p>â€‹		<strong>ç¿»è¯‘è´¨é‡</strong>    <strong>ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ˜¾è‘—çš„å·®å¼‚W.R.T BLEUè¯„åˆ†ã€‚ è¯·æ³¨æ„ï¼Œæœ¯è¯­åªå½±å“å¥å­çš„ä¸€å°éƒ¨åˆ†ï¼Œå¹¶ä¸”å¤§å¤šæ•°æ—¶å€™åŸºçº¿å·²ç»åŒ…å«äº†æ‰€éœ€çš„æœ¯è¯­ï¼Œå› æ­¤åœ¨æ­¤æµ‹è¯•é›†ä¸­ä¸å¯èƒ½å‡ºç°é«˜çš„BLEUå˜åŒ–ã€‚ çº¦æŸè¯‘ç ä¸ä¼šå¯¼è‡´BLEUçš„ä»»ä½•å˜åŒ–ï¼Œé™¤äº†åœ¨å…·æœ‰5çš„å°æ³¢æŸå°ºå¯¸çš„æƒ…å†µä¸‹åœ¨IATEä¸Šçš„å‡å°</strong>ã€‚ Surprisingly, we observe signiï¬cant variance w.r.t BLEU scores. Note that the terminologies affect only a small part of a sentence and most of the times the baseline already contains the desired term, therefore high BLEU variations are impossible on this test set. Constrained decoding does not lead to any changes in BLEU, other than a decrease on IATE with a small beam size of 5.ç„¶è€Œï¼Œæ‰€æœ‰é€æ¬¡è®­ç»ƒæ¨¡å‹éƒ½æ˜¾ç¤ºBLEUå¢åŠ (+0.2åˆ°+0.9)ï¼Œç‰¹åˆ«æ˜¯æœ¯è¯­ä½¿ç”¨ç‡è¾ƒä½çš„é€æ¬¡è®­ç»ƒæ¨¡å‹ã€‚ å½“æ£€æŸ¥è¿™äº›æ–¹æ³•çš„é”™è¯¯æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™æ ·çš„æƒ…å†µï¼Œå³çº¦æŸè§£ç æ”¹å˜ç¿»è¯‘ä»¥é€‚åº”ä¸€ä¸ªæœ¯è¯­ï¼Œå³ä½¿è¯¥æœ¯è¯­çš„å˜ä½“å·²ç»åœ¨ç¿»è¯‘ä¸­ï¼Œå¦‚è¡¨3çš„Festzunehmen/Festnahmeç¤ºä¾‹ï¼ˆå¹¶ä¸”æœ‰æ—¶å³ä½¿å·²ç»ä½¿ç”¨äº†ç›¸åŒçš„æœ¯è¯­ï¼‰ã€‚ ä»”ç»†è§‚å¯Ÿä»¥å‰çš„çº¦æŸè¯‘ç æ–‡çŒ®ï¼Œå¯ä»¥å‘ç°å¤§å¤šæ•°çš„è¯„ä»·éƒ½ä¸æœ¬æ–‡ä¸åŒ:æ•°æ®é›†åªåŒ…å«å‚è€ƒæ–‡çŒ®ä¸­åŒ…å«æœ¯è¯­çš„å¥å­ï¼Œè€Œä¸”åŸºçº¿ä¹Ÿä¸èƒ½äº§ç”Ÿæœ¯è¯­ã€‚ è¿™æ˜¯ä¸€ä¸ªç†æƒ³çš„è®¾ç½®ï¼Œæˆ‘ä»¬ç›¸ä¿¡å¾ˆå°‘ï¼Œå¦‚æœæœ‰ï¼Œæ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„åº”ç”¨ç¨‹åºã€‚</p>

<p>â€‹		<strong>åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†å¦ä¸€ä¸ªä»¤äººæƒŠè®¶çš„ç§¯æè¡Œä¸ºï¼Œè€Œçº¦æŸè§£ç ä¸èƒ½å¤„ç†:åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆæœ¯è¯­åº“æä¾›çš„æœ¯è¯­ç¿»è¯‘çš„å½¢æ€å˜ä½“ã€‚ åœ¨æ­¤ä¹‹åï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªé¢å¤–çš„å®éªŒï¼Œé€šè¿‡æ‰©å±•å‰ä¸€ä¸ªå®éªŒé›†æ¥åŒ…æ‹¬ç›®æ ‡ä¾§çš„è¿‘ä¼¼åŒ¹é…ï¼ˆä¸2.1èŠ‚ä¸­è§£é‡Šçš„è®­ç»ƒä¸­çš„è¿‘ä¼¼åŒ¹é…ç›¸åŒï¼‰</strong>ã€‚We observed an additional surprisingly positive behavior with our approach which constrained decoding does not handle: in some cases, our models generate morphological variants of terminology translations provided by the term base. Following up on this we set up an additional experiment by extending the previous set to also include approximate matches on the target side (identical to the approximate match in training explained in Section 2.1).</p>

<p><img src="https://cdn.jsdelivr.net/gh/tian-guo-guo/cdn@master/assets/picgoimg/20200720164928.jpg" alt="image-20200526110644580" /></p>

<p>è¡¨4:æä¾›æœ¯è¯­æ¡ç›®çš„ç³»ç»Ÿçš„æœºå™¨ç¿»è¯‘ç»“æœï¼Œæ˜¾ç¤ºç²¾ç¡®çš„æºåŒ¹é…å’Œè¿‘ä¼¼çš„å‚è€ƒåŒ¹é…ã€‚ å½“På€¼å°äº0.05æ—¶ï¼Œè¡¨ç¤ºç³»ç»Ÿæ˜æ˜¾æ¯”åŸºçº¿å·®ã€‚</p>

<p>â€‹		è¡¨4æ˜¾ç¤ºäº†è¿™äº›ç»“æœã€‚ æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œè¿™ä¸ªæµ‹è¯•ç”¨ä¾‹å¯¹äºçº¦æŸè§£ç å’ŒæŒ‰æ›¿æ¢è®­ç»ƒæ¥è¯´å·²ç»æ›´åŠ å›°éš¾ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯å› ä¸ºåˆ é™¤äº†åŸå§‹æºç«¯å†…å®¹ã€‚ å¦ä¸€æ–¹é¢ï¼Œtrainby-appendçš„è¡¨ç°ä»ä¼˜äºåŸºçº¿ï¼Œè€Œå—é™è§£ç æ˜¾ç¤ºBLEUåˆ†æ•°æ˜¾è‘—é™ä½0.9-1.3ä¸ªBLEUç‚¹ã€‚ è¡¨3ä¸­çš„Humanithumanitç¤ºä¾‹ä»£è¡¨äº†åœ¨æºåŒ¹é…é¡¹ï¼ˆå…¶ç›®æ ‡ä¾§éœ€è¦å—åˆ°å½±å“ï¼‰çš„æƒ…å†µä¸‹ï¼Œç”±çº¦æŸè§£ç å¼•å…¥çš„é”™è¯¯ã€‚</p>

<h1 id="4-conclusion">4 Conclusion</h1>

<p>â€‹		è™½ç„¶ä»¥å¾€çš„ç¥ç»MTç ”ç©¶å¤§å¤šè‡´åŠ›äºæœ¯è¯­ä¸çº¦æŸè¯‘ç çš„é›†æˆï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸€ç§é»‘ç®±æ–¹æ³•ï¼Œå³ç›´æ¥è®­ç»ƒä¸€ä¸ªé€šç”¨çš„ç¥ç»MTä½“ç³»ç»“æ„æ¥å­¦ä¹ å¦‚ä½•ä½¿ç”¨è¿è¡Œæ—¶æä¾›çš„å¤–éƒ¨æœ¯è¯­ã€‚ æˆ‘ä»¬åœ¨é›¶é•œå¤´è®¾ç½®ä¸‹è¿›è¡Œäº†å®éªŒï¼Œè¡¨æ˜å¤åˆ¶è¡Œä¸ºæ˜¯åœ¨æµ‹è¯•æ—¶è§¦å‘çš„ï¼Œè€Œè¿™äº›è¯åœ¨è®­ç»ƒä¸­ä»æœªè§è¿‡ã€‚ ä¸çº¦æŸè§£ç ç›¸åï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°è¯¥æ–¹æ³•å±•ç¤ºäº†æœ¯è¯­çš„çµæ´»ä½¿ç”¨ï¼Œå› ä¸ºåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæœ¯è¯­ä»¥å…¶æä¾›çš„å½¢å¼ä½¿ç”¨ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹åˆ™æ‰§è¡Œçµæ´»ã€‚</p>

<p>â€‹		æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œåœ¨ç¥ç»MTçš„çº¦æŸè¯‘ç ç®—æ³•ç©ºé—´ä¸­ï¼Œæ²¡æœ‰ä»»ä½•ç°æœ‰çš„å·¥ä½œæ¯”æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ›´å¥½çš„é€Ÿåº¦ä¸æ€§èƒ½æƒè¡¡ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™ä½¿å¾—å®ƒç‰¹åˆ«é€‚åˆäºç”Ÿäº§ç¯å¢ƒã€‚</p>

<h1 id="references">References</h1>

<p>Rajen Chatterjee, Matteo Negri, Marco Turchi, Marcello Federico, Lucia Specia, and FrÂ´edÂ´eric Blain. 2017. Guiding neural machine translation decoding with external knowledge. In Proceedings of the Second Conference on Machine Translation, pages 157â€“168, Copenhagen, Denmark. Association for Computational Linguistics.</p>

<p>Josep Maria Crego, Jungi Kim, Guillaume Klein, Anabel Rebollo, Kathy Yang, Jean Senellart, Egor Akhanov, Patrice Brunelle, Aurelien Coquard, Yongchao Deng, Satoshi Enoue, Chiyo Geiss, Joshua Johanson, Ardas Khalsa, Raoum Khiari, Byeongil Ko, Catherine Kobus, Jean Lorieux, Leidiana Martins, Dang-Chuan Nguyen, Alexandra Priori, Thomas Riccardi, Natalia Segal, Christophe Servan, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou, and Peter Zoldan. 2016. Systranâ€™s pure neural machine translation systems. CoRR, abs/1610.05540.</p>

<p>M. Amin Farajian, Nicola Bertoldi, Matteo Negri, Marco Turchi, and Marcello Federico. 2018. Evaluation of Terminology Translation in Instance-Based Neural MT Adaptation. In Proceedings of the 21st Annual Conference of the European Association for Machine Translation, pages 149â€“158, Alicante, Spain. European Association for Machine Translation.</p>

<p>Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. 2017. Search engine guided nonparametric neural machine translation. In Proceedings of the Thirty-Second AAAI Conference on Artiï¬cial Intelligence, pages 5133â€“5140, New Orleans, Louisiana, USA. Association for the Advancement of Artiï¬cial Intelligence.</p>

<p>Eva Hasler, Adri`a Gispert, Gonzalo Iglesias, and Bill Byrne. 2018. Neural machine translation decoding with terminology constraints. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 506â€“512. Association for Computational Linguistics.</p>

<p>Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, and Matt Post. 2018. The sockeye neural machine translation toolkit at AMTA 2018. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers), pages 200â€“207. Association for Machine Translation in the Americas.</p>

<p>Chris Hokamp and Qun Liu. 2017. Lexically constrained decoding for sequence generation using grid beam search. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 August 4, Volume 1: Long Papers, pages 1535â€“1546.</p>

<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177â€“180. Association for Computational Linguistics.</p>

<p>Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. CoRR, abs/1508.04025.</p>

<p>Ngoc-Quan Pham, Jan Niehues, and Alexander H.</p>

<p>Waibel. 2018. Towards one-shot learning for rareword translation with external experts. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, NMT@ACL 2018, Melbourne, Australia, July 20, 2018, pages 100â€“109.</p>

<p>Matt Post and David Vilar. 2018. Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1314â€“1324.</p>

<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.</p>

<ol>
  <li>Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725. Association for Computational Linguistics.</li>
</ol>

<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000â€“6010.</p>

<h1 id="nmt-sockeye-train-parameters">NMT Sockeye train parameters</h1>

<p>encoder-config:</p>

<p>â€‹		act_type: relu</p>

<p>â€‹		attention_heads: 8</p>

<p>â€‹		conv_config: null</p>

<p>â€‹		dropout_act: 0.1</p>

<p>â€‹		dropout_attention: 0.1</p>

<p>â€‹		dropout_prepost: 0.1</p>

<p>â€‹		dtype: float32</p>

<p>â€‹		feed_forward_num_hidden: 2048</p>

<p>â€‹		lhuc: false</p>

<p>â€‹		max_seq_len_source: 101</p>

<p>â€‹		max_seq_len_target: 101</p>

<p>â€‹		model_size: 512</p>

<p>â€‹		num_layers: 2</p>

<p>â€‹		positional_embedding_type: fixed</p>

<p>â€‹		postprocess_sequence: dr</p>

<p>â€‹		preprocess_sequence: n</p>

<p>â€‹		use_lhuc: false</p>

<p>decoder config:</p>

<p>â€‹		act_type: relu</p>

<p>â€‹		attention_heads: 8</p>

<p>â€‹		conv_config: null</p>

<p>â€‹		dropout_act: 0.1</p>

<p>â€‹		dropout_attention: 0.1</p>

<p>â€‹		dropout_prepost: 0.1</p>

<p>â€‹		dtype: float32</p>

<p>â€‹		feed_forward_num_hidden: 2048</p>

<p>â€‹		max_seq_len_source: 101</p>

<p>â€‹		max_seq_len_target: 101</p>

<p>â€‹		model_size: 512</p>

<p>â€‹		num_layers: 2</p>

<p>â€‹		positional_embedding_type: fixed</p>

<p>â€‹		postprocess_sequence: dr</p>

<p>â€‹		preprocess_sequence: n</p>

<p>config_loss: !LossConfig</p>

<p>â€‹		label_smoothing: 0.1</p>

<p>â€‹		name: cross-entropy</p>

<p>â€‹		normalization_type: valid</p>

<p>â€‹		vocab_size: 32302</p>

<p>config_embed_source: !</p>

<p>â€‹		EmbeddingConfig</p>

<p>â€‹		dropout: 0.0</p>

<p>â€‹		dtype: float32</p>

<p>â€‹		factor_configs: null</p>

<p>â€‹		num_embed: 512</p>

<p>â€‹		num_factors: 1</p>

<p>â€‹		vocab_size: 32302</p>

<p>config_embed_target: !</p>

<p>â€‹		EmbeddingConfig</p>

<p>â€‹		dropout: 0.0</p>

<p>â€‹		dtype: float32</p>

<p>â€‹		factor_configs: null</p>

<p>â€‹		num_embed: 512</p>

<p>â€‹		num_factors: 1</p>

<p>â€‹		vocab_size: 32302</p>
:ET